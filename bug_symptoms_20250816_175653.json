{
  "metadata": {
    "generated_at": "2025-08-16T17:56:53.488564",
    "analysis_method": "Gemini symptom classification",
    "provider": "gemini",
    "sample_size": 2000
  },
  "symptoms": [
    "Runtime Crash or Error",
    "Incorrect/Inaccuracy Output",
    "Build/Compilation Error",
    "Model Loading  Error",
    "Hardware/Backend Compatibility Issue",
    "Performance/Memory Issue",
    "Distributed Parallelism/Sharding Bug",
    "API/argument parsing Error"
  ],
  "statistics": {
    "total_bugs_analyzed": 2000,
    "repository_distribution": {
      "microsoft/DeepSpeed": {
        "total": 283,
        "issues": 139,
        "prs": 144
      },
      "ggerganov/llama.cpp": {
        "total": 521,
        "issues": 223,
        "prs": 298
      },
      "vllm-project/vllm": {
        "total": 744,
        "issues": 261,
        "prs": 483
      },
      "NVIDIA/TensorRT-LLM": {
        "total": 266,
        "issues": 141,
        "prs": 125
      },
      "mlc-ai/mlc-llm": {
        "total": 93,
        "issues": 50,
        "prs": 43
      },
      "huggingface/text-generation-inference": {
        "total": 93,
        "issues": 46,
        "prs": 47
      }
    },
    "type_distribution": {
      "issue": 860,
      "pr": 1140
    },
    "symptom_distribution": {
      "Runtime Crash or Error": 422,
      "Incorrect/Inaccuracy Output": 343,
      "Build/Compilation Error": 285,
      "Model Loading  Error": 147,
      "Hardware/Backend Compatibility Issue": 180,
      "Performance/Memory Issue": 183,
      "Distributed Parallelism/Sharding Bug": 144,
      "API/argument parsing Error": 124,
      "other": 172
    },
    "classification_summary": {
      "total_classified": 2000,
      "symptoms_used": 9,
      "other_count": 172,
      "other_percentage": 8.6
    }
  },
  "detailed_classifications": [
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix(pipe): make pipe module `load_state_dir` non-strict-mode work",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4020",
      "number": 4020,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "fix: add missing macro definitions",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8281",
      "number": 8281,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[KVConnector] Always call connector `clear_metadata()` at end of step",
      "url": "https://github.com/vllm-project/vllm/pull/20756",
      "number": 20756,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix Initial Error in \"WarmupCosineLR\" scheduler at steps 0 and 1",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5287",
      "number": 5287,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Loadams/fix lightning pytorch2",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3040",
      "number": 3040,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llava-cli : don't crash if --image flag is invalid",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4835",
      "number": 4835,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "server : fix crash when error handler dumps invalid utf-8 json",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9210",
      "number": 9210,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1][Core] using cached vocab_size for Structured Outputs",
      "url": "https://github.com/vllm-project/vllm/pull/14630",
      "number": 14630,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Only turn on FastIncrementalDetokenizer when tokenizers >= 0.21.1",
      "url": "https://github.com/vllm-project/vllm/pull/17158",
      "number": 17158,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "use get_global_rank if available",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2567",
      "number": 2567,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml : fix dot product for ARM",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4630",
      "number": 4630,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix: Resolve CVEs identified by SDLe CT7 scan",
      "url": "https://github.com/vllm-project/vllm/pull/16558",
      "number": 16558,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Kernel] Add torch custom op for all_reduce",
      "url": "https://github.com/vllm-project/vllm/pull/7755",
      "number": 7755,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix RuntimeError when using ZeRO Stage3 with mpu: #3564",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3565",
      "number": 3565,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Core] Pipe `worker_class_fn` argument in Executor",
      "url": "https://github.com/vllm-project/vllm/pull/7707",
      "number": 7707,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Update deprecation-warning.cpp",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10619",
      "number": 10619,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix issue with symint input",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7243",
      "number": 7243,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "hotfix for llava-1.6 image number",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5495",
      "number": 5495,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix the issue of token count in BlockTable",
      "url": "https://github.com/vllm-project/vllm/pull/20425",
      "number": 20425,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix nvbug 5302895",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4650",
      "number": 4650,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Re-enable Gemma3 for V1",
      "url": "https://github.com/vllm-project/vllm/pull/14980",
      "number": 14980,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Adjusting invalid k values in top-k selection beyond vocabulary limits",
      "url": "https://github.com/vllm-project/vllm/pull/14234",
      "number": 14234,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Fix] Prefix cache only enables sliding window on leaf sequence",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2615",
      "number": 2615,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fixing the inference build-path when pre-building the op",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1755",
      "number": 1755,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Add compile backend arg for test_set_compiler_fn",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5611",
      "number": 5611,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "metal : fix build errors and rope kernel sig after #2268",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3898",
      "number": 3898,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Update TensorRT-LLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/941",
      "number": 941,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix Memory Leak In AIO",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6630",
      "number": 6630,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "`json`: refine whitespace rules to avoid runaways",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7866",
      "number": 7866,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server : fix incorrect usage of llama_get_embeddings()",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14225",
      "number": 14225,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Set zero stage to 0 in autotuning when prescale gradients is used",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2523",
      "number": 2523,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix(server): fix cohere",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2249",
      "number": 2249,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix rope cache key error",
      "url": "https://github.com/vllm-project/vllm/pull/1867",
      "number": 1867,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[fix] Add MPI barrier for proper NVLS multicast team initialization",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4952",
      "number": 4952,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Windows XP: support MinGW 8.1.0",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3419",
      "number": 3419,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "webui: fix markdown table",
      "url": "https://github.com/ggml-org/llama.cpp/pull/15081",
      "number": 15081,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Avoid graph break due to unsupported frozenset",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7105",
      "number": 7105,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: Accommodate Phi3/4 to work with ModelOpt's FP8 ckpts in Torch",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6761",
      "number": 6761,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Added chat template support to llama-run",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11215",
      "number": 11215,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Swissai llm fix",
      "url": "https://github.com/vllm-project/vllm/pull/17467",
      "number": 17467,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Python] Add flag to disable logging config",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/3060",
      "number": 3060,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] fixed nvfp4_moe test failures due to invalid kwargs",
      "url": "https://github.com/vllm-project/vllm/pull/21246",
      "number": 21246,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Meta] Unshift eagle prefill and support draft kv sharing from base",
      "url": "https://github.com/vllm-project/vllm/pull/21008",
      "number": 21008,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Fix ScalarType float4 naming ",
      "url": "https://github.com/vllm-project/vllm/pull/17690",
      "number": 17690,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix build issues on Windows",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2428",
      "number": 2428,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Model][Bugfix]: correct Aria model output",
      "url": "https://github.com/vllm-project/vllm/pull/12309",
      "number": 12309,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Core] Enable IPv6 with vllm.utils.make_zmq_socket()",
      "url": "https://github.com/vllm-project/vllm/pull/16506",
      "number": 16506,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "Enhance Model Delivery",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1283",
      "number": 1283,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix building package without a GPU",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2049",
      "number": 2049,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix hybrid model tests",
      "url": "https://github.com/vllm-project/vllm/pull/17182",
      "number": 17182,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Fix] FuseDecodeTranspose pass with PrimFunc deep copy",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/535",
      "number": 535,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Model] Add support for Qwen2-VL video embeddings input & multiple image embeddings input with varied resolutions",
      "url": "https://github.com/vllm-project/vllm/pull/10221",
      "number": 10221,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Replace sleep with condition variables in server",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4673",
      "number": 4673,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix crash in torch2.6 if TP=1",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2885",
      "number": 2885,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix the half-precision version of rotary_pos_emb kernel",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1683",
      "number": 1683,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "add missing methods to MPS_Accelerator",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5134",
      "number": 5134,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Title: Fix setup_env_ranks to Properly Set Environment Variables Instead of Raising Error",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6979",
      "number": 6979,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Ignore ray reinit error when current platform is ROCm or XPU",
      "url": "https://github.com/vllm-project/vllm/pull/10375",
      "number": 10375,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml : define missing HWCAP flags",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9684",
      "number": 9684,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "grammar-parser: fix possible null-deref",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9004",
      "number": 9004,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "[Stage][Fix] Add additional conditions when checking types of output from the model",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1026",
      "number": 1026,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : fix defrag logic",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11707",
      "number": 11707,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix bug",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1557",
      "number": 1557,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Remove unused data and add fixes",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5154",
      "number": 5154,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Build] Make pypi install work on CPU platform",
      "url": "https://github.com/vllm-project/vllm/pull/12874",
      "number": 12874,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix Hybrid Engine for BLOOM",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3580",
      "number": 3580,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Pipe Engine Reduce High Dimension Output tensor fix",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3774",
      "number": 3774,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix bugs about non-contiguous tensor broadcasting",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1168",
      "number": 1168,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Enable V1 for Hybrid SSM/Attention Models",
      "url": "https://github.com/vllm-project/vllm/pull/20016",
      "number": 20016,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Add OpenCL add kernel",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5151",
      "number": 5151,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix missing ARG in Dockerfile for arm64 platforms",
      "url": "https://github.com/vllm-project/vllm/pull/17261",
      "number": 17261,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Add endpoint load metrics",
      "url": "https://github.com/vllm-project/vllm/pull/20088",
      "number": 20088,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix logit soft cap in flash-attn backend",
      "url": "https://github.com/vllm-project/vllm/pull/7425",
      "number": 7425,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix: avoid setting use_sgmv if no kernels present",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2796",
      "number": 2796,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Revert \"[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of U…",
      "url": "https://github.com/vllm-project/vllm/pull/14848",
      "number": 14848,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Handle `None` as param value in api_like_OAI.py",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2746",
      "number": 2746,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Mistral tool calling when content is list",
      "url": "https://github.com/vllm-project/vllm/pull/18729",
      "number": 18729,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix Unable to parse import deepspeed.accelerator",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3913",
      "number": 3913,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "export-lora : throw error if lora is quantized",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9002",
      "number": 9002,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Server: use llama_chat_apply_template",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5593",
      "number": 5593,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI][BugFix] Flip is_quant_method_supported condition",
      "url": "https://github.com/vllm-project/vllm/pull/5577",
      "number": 5577,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Minor: logger import in attention backend",
      "url": "https://github.com/vllm-project/vllm/pull/13706",
      "number": 13706,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server : (tests) don't use thread for capturing stdout/stderr, bump openai client library",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10568",
      "number": 10568,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Model] Support Nemotron models (Nemotron-3, Nemotron-4, Minitron)",
      "url": "https://github.com/vllm-project/vllm/pull/6611",
      "number": 6611,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : fix llama_split_mode enum values in main_gpu document",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9057",
      "number": 9057,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[Issue 5927][fix] Avoid memory calls during broadcast for single GPU",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6010",
      "number": 6010,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Align `max_tokens` behavior with openai",
      "url": "https://github.com/vllm-project/vllm/pull/852",
      "number": 852,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[SLIM] Support out dtype for QuantizedLinear",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1411",
      "number": 1411,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix CPU builds",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4633",
      "number": 4633,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Fix][Tokenizer] Fix failure in decoding tokens for ByteLevel BPE",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2649",
      "number": 2649,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][PP] Add check for `PPMissingLayer` in logits_processor.py when using pp",
      "url": "https://github.com/vllm-project/vllm/pull/20121",
      "number": 20121,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "tests: fix 5250460",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4751",
      "number": 4751,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Revert QKVCrossParallelLinear usage in Mllama to keep BNB quantization work",
      "url": "https://github.com/vllm-project/vllm/pull/14498",
      "number": 14498,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Add weight normalization for Baichuan 2",
      "url": "https://github.com/vllm-project/vllm/pull/1876",
      "number": 1876,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Update barrier and reduce_scatter_base to conform to PyTorch signatures",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2570",
      "number": 2570,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix, bf16 optimizer remove dup loop",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7054",
      "number": 7054,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Assert ZeRO-Offload incompatible with gradient accumulation",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/347",
      "number": 347,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[fix] Fix illegal mem access and possible accuracy lose",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4943",
      "number": 4943,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "fix: return {} for tool arguments when no argument is needed, so that…",
      "url": "https://github.com/vllm-project/vllm/pull/21365",
      "number": 21365,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix EAGLE3 broken logits",
      "url": "https://github.com/vllm-project/vllm/pull/18909",
      "number": 18909,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "vulkan : fix build failure caused by vulkan-shaders-gen install",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14047",
      "number": 14047,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[TPU][TEST] Fix the downloading issue in TPU v1 test 11. ",
      "url": "https://github.com/vllm-project/vllm/pull/21418",
      "number": 21418,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "[SYCL] fallback mmvq",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9088",
      "number": 9088,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix spelling error in function GetMaxTokenLength()",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3482",
      "number": 3482,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Modify small-batched weight only quantization",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2213",
      "number": 2213,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "force quit when get pynvml version issue",
      "url": "https://github.com/vllm-project/vllm/pull/12972",
      "number": 12972,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Correct deepseek-vl2 chat template",
      "url": "https://github.com/vllm-project/vllm/pull/14558",
      "number": 14558,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "convert : fix gemma v1 tokenizer convert",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8248",
      "number": 8248,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Kernel] Fix perf regression caused by PR #12405",
      "url": "https://github.com/vllm-project/vllm/pull/12434",
      "number": 12434,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "convert-*.py: remove add_name from ChatGLMModel class",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8590",
      "number": 8590,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "DataLoader Length Fix",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1718",
      "number": 1718,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix FP8 Marlin MoE and enable for compressed-tensors models",
      "url": "https://github.com/vllm-project/vllm/pull/18026",
      "number": 18026,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CUDA: fix variable name conflict for Windows build",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9382",
      "number": 9382,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Avoid fp32->fp16->fp32 conversion on cdna in ggml_cuda_op_mul_mat_cublas",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11356",
      "number": 11356,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Add support for missing quants in CPY (Metal & CUDA).",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11987",
      "number": 11987,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[None][fix] fix kimi k2 serving and add test for Kimi-K2",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6589",
      "number": 6589,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fixed default MSVS build",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4567",
      "number": 4567,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix build tokenizer in quantize and remove duplicate import",
      "url": "https://github.com/huggingface/text-generation-inference/pull/768",
      "number": 768,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "vulkan: increase timeout for CI",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14574",
      "number": 14574,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "cuda: fix compilation error (#12893)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12894",
      "number": 12894,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Fix torch distributed stateless PG backend init",
      "url": "https://github.com/vllm-project/vllm/pull/14870",
      "number": 14870,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix the issue where llm.generate cannot be called repeatedly after setting GuidedDecodingParams",
      "url": "https://github.com/vllm-project/vllm/pull/16767",
      "number": 16767,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[Infra]Fix l0_sanity_check.yml which also has gb202 and gb203 (#5360)",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5362",
      "number": 5362,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix wrong documentation of `ignore_unused_parameters`",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4418",
      "number": 4418,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml-cpu: sycl: Re-enable exp f16",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14462",
      "number": 14462,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: FP8 kv accuracy",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3675",
      "number": 3675,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Pass] Fix sampling func attachment to not read existing vocab size",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2292",
      "number": 2292,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "OpenAI compatibility fix: valid `top_p` value range is 0.0 to 1.0.",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2228",
      "number": 2228,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] improve regex for hermes tool detection",
      "url": "https://github.com/vllm-project/vllm/pull/20474",
      "number": 20474,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][ROCm][CI/Build] Fix ROCm build regression",
      "url": "https://github.com/vllm-project/vllm/pull/22822",
      "number": 22822,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[None][fix] disagg ctx pp4 + gen pp4 integ test",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6489",
      "number": 6489,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Kernel injection fixes",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/601",
      "number": 601,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[NFC] Typo fix in SP layer.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7152",
      "number": 7152,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llava : fix occasional undefined behavior crash",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9078",
      "number": 9078,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix merge waive list 2",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6505",
      "number": 6505,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix]: Hermes tool parser stream output error  #19056",
      "url": "https://github.com/vllm-project/vllm/pull/19058",
      "number": 19058,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Added error handling for malloc and strdup",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8727",
      "number": 8727,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix erroneous \"model doesn't support compile\" warning",
      "url": "https://github.com/vllm-project/vllm/pull/16486",
      "number": 16486,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Enhance user input handling for llama-run",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11138",
      "number": 11138,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Update barrier and reduce_scatter_base to conform to PyTorch signatures",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2570",
      "number": 2570,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[nvbugs/5401156][fix] Avoid import all models when import trtllm._common",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6266",
      "number": 6266,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[gpt-oss] Fix mxfp4 support",
      "url": "https://github.com/vllm-project/vllm/pull/22700",
      "number": 22700,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Fix index in ChatCompletionChunk",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1648",
      "number": 1648,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Prevent web browser error and chat history not saving",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10711",
      "number": 10711,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "mv 'paged_kv_cache=paged_kv_cache' to 'kv_cache_type=KVCacheType.PAGED'",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2199",
      "number": 2199,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "Minor bug fix and reorganization on ChatModule embed separation",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/488",
      "number": 488,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : add RobertaForSequenceClassification reranker support",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13875",
      "number": 13875,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix memory leak in src/llama.cpp",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8958",
      "number": 8958,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "swift : exclude ggml-metal-embed.metal",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10211",
      "number": 10211,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "gemma : more consistent attention scaling for v2 and v3",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13951",
      "number": 13951,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[https://nvbugs/5178445][fix] Skip blackwell tests for sm120",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3815",
      "number": 3815,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: add SM90 guard for FP8 Blockscale GEMM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3575",
      "number": 3575,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix:  Fix minor issues in test_autotuner.py",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3261",
      "number": 3261,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "server : pad small embedding batches",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13692",
      "number": 13692,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix checkpoint conversion when model layers share weights",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3825",
      "number": 3825,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build] fix flaky test",
      "url": "https://github.com/vllm-project/vllm/pull/3602",
      "number": 3602,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix(launcher): copy current env vars to subprocesses",
      "url": "https://github.com/huggingface/text-generation-inference/pull/70",
      "number": 70,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] [ROCm]: Remove assertion logic when using AITER fused moe in unquantizedMethod to reenable LLama4 BF16",
      "url": "https://github.com/vllm-project/vllm/pull/18205",
      "number": 18205,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[LogitProcessor] Add max thread awareness to logit processing kernels",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1955",
      "number": 1955,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Set `alias` for `max_completion_tokens` in `ChatRequest`",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2932",
      "number": 2932,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Bugfix] Cannot find global function `mlc.llm_chat_create`",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1167",
      "number": 1167,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Revert ROCm Custom Paged Attention Env Flag Check",
      "url": "https://github.com/vllm-project/vllm/pull/17022",
      "number": 17022,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[fix] fix log probs and add for mtp and completion requests",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5620",
      "number": 5620,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix overflow indexing in causal_conv1d kernel",
      "url": "https://github.com/vllm-project/vllm/pull/20938",
      "number": 20938,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[https://nvbugs/5415862][fix] Update cublas as 12.9.1 and cuda memory alignment as 256",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6501",
      "number": 6501,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "gguf : fix strings to not be null-terminated",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2839",
      "number": 2839,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "fix: llama_memory_seq_rm(mem, -1, ...)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/15200",
      "number": 15200,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Server: reorganize some http logic",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5939",
      "number": 5939,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix(server): blacklist local files",
      "url": "https://github.com/huggingface/text-generation-inference/pull/609",
      "number": 609,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "vocab : add special infill tokens for CodeLlama",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11850",
      "number": 11850,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix OOM tests in initialization test",
      "url": "https://github.com/vllm-project/vllm/pull/21921",
      "number": 21921,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Remove bf16 from inference config dtye enum",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3010",
      "number": 3010,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI][Bugfix] Add mistral_tool_use to Ci",
      "url": "https://github.com/vllm-project/vllm/pull/16517",
      "number": 16517,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix eviction cached blocked logic",
      "url": "https://github.com/vllm-project/vllm/pull/21357",
      "number": 21357,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : fix KV shift for qwen2vl",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13870",
      "number": 13870,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix vLLM x torch.compile config caching",
      "url": "https://github.com/vllm-project/vllm/pull/16491",
      "number": 16491,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Change the sparse attention API to be compatible with latest changes of triton",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/902",
      "number": 902,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Model] Use AutoWeightsLoader for mamba2",
      "url": "https://github.com/vllm-project/vllm/pull/18918",
      "number": 18918,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server (webui): Fix Premature Submission During IME Conversion",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11971",
      "number": 11971,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Fix MultiConnector test after HMA changes",
      "url": "https://github.com/vllm-project/vllm/pull/19291",
      "number": 19291,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Update clip.cpp",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9482",
      "number": 9482,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix using torch.half with transformers",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4670",
      "number": 4670,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Doc] Fix wrong code block of mkdocs",
      "url": "https://github.com/vllm-project/vllm/pull/18607",
      "number": 18607,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Serving] Update Llama model with attn kernel",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1169",
      "number": 1169,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Build llama.cpp on Intel with nix",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2795",
      "number": 2795,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Clamp out of range values in K quantizer",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6888",
      "number": 6888,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[ROCm] [Bugfix] [Critical]: Fix mamba compilation bug",
      "url": "https://github.com/vllm-project/vllm/pull/20883",
      "number": 20883,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Remove flashinfer warning, add flashinfer tests to CI",
      "url": "https://github.com/vllm-project/vllm/pull/6351",
      "number": 6351,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Enable dynamic shapes for pipeline parallel engine inputs",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5481",
      "number": 5481,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "musa: workaround for Guilty Lockup in cleaning src0 in #10032",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10042",
      "number": 10042,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BUG] Addreses #3935 and #3683, by making `intial_incremental_detokenization_offset` configurable",
      "url": "https://github.com/vllm-project/vllm/pull/13103",
      "number": 13103,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[None][fix] Remove lock related typo in py_executor",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6319",
      "number": 6319,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix uneven issue & add balance autotp",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4697",
      "number": 4697,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix][AMD][Quantization] Fix torch.compile issue where wvSplitKQ not being called when it should when using quantized FP8 model",
      "url": "https://github.com/vllm-project/vllm/pull/22281",
      "number": 22281,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Doc][Neuron] add note to neuron documentation about resolving triton issue",
      "url": "https://github.com/vllm-project/vllm/pull/9257",
      "number": 9257,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Allow \"quantizing\" to f16 and f32",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1787",
      "number": 1787,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "test: remove perf test l40s/l20 oom test cases and unwaive tests",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4755",
      "number": 4755,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Use tagged version of llguidance that does not break the build",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13413",
      "number": 13413,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Provide default max model length",
      "url": "https://github.com/vllm-project/vllm/pull/1224",
      "number": 1224,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix model loading time through prefetching the file on another thread",
      "url": "https://github.com/ggml-org/llama.cpp/pull/734",
      "number": 734,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Free memory in universal checkpointing tests",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6693",
      "number": 6693,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[Fix] the bug in the trtllm-gen heurisitcf for MLA kernels.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6284",
      "number": 6284,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI/Build] Fix pre-commit errors",
      "url": "https://github.com/vllm-project/vllm/pull/13696",
      "number": 13696,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "convert : identify missing model files",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9397",
      "number": 9397,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Properly invoke CTest",
      "url": "https://github.com/ggml-org/llama.cpp/pull/629",
      "number": 629,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "sync : ggml (im2col, GPU conv, 32-bit arm compat)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4060",
      "number": 4060,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "tests : avoid github urls due to throttling",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13654",
      "number": 13654,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix error :Dictionary expression not allowed in type annotation Pylance",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3708",
      "number": 3708,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "fix(perf/UX): Use num physical cores by default, warn about E/P cores",
      "url": "https://github.com/ggml-org/llama.cpp/pull/934",
      "number": 934,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[bugfix] add seed in torchrun_example.py",
      "url": "https://github.com/vllm-project/vllm/pull/15980",
      "number": 15980,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix [nvbug5256044]: bench hang due to llmapi ipc",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4798",
      "number": 4798,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "scripts: update compare_llama_bench.py [no ci]",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7673",
      "number": 7673,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][VLM] fix llava processor",
      "url": "https://github.com/vllm-project/vllm/pull/15285",
      "number": 15285,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: correct the lowest Maxwell supported by CUDA 12",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11984",
      "number": 11984,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Hotfixes for santacoder/bigcode.",
      "url": "https://github.com/huggingface/text-generation-inference/pull/294",
      "number": 294,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Check all graph nodes when searching for result_embd_pooled (needed for gemma-2)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8981",
      "number": 8981,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Add the endpoints /api/tags and /api/chat",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13659",
      "number": 13659,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Offload all gradients to nvme",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2282",
      "number": 2282,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "ci : fix build CPU arm64",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11472",
      "number": 11472,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Revert \"Add an argument to enable the injection of missing state duri…",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5720",
      "number": 5720,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[TRTLLM-6445] feat: Enable AllReduce-associated fusion patterns in Llama3/4.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6205",
      "number": 6205,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix inadvertently silenced PP tests for `mp`, add DeepSeek V2/V3 model family to PP tests",
      "url": "https://github.com/vllm-project/vllm/pull/20831",
      "number": 20831,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "nix: windows build",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6167",
      "number": 6167,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Update tpu_worker.py 's typo",
      "url": "https://github.com/vllm-project/vllm/pull/17288",
      "number": 17288,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[TRTLLM-5000][feat] NGrams V2",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4569",
      "number": 4569,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Explicitly explain quant method override ordering and ensure all overrides are ordered",
      "url": "https://github.com/vllm-project/vllm/pull/17256",
      "number": 17256,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "sycl: Fix conditional enabling following arch checks for ggml-sycl",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14504",
      "number": 14504,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc]: Add support for goodput on guided benchmarking + TPOT calculation refactor",
      "url": "https://github.com/vllm-project/vllm/pull/13736",
      "number": 13736,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "cherry-pick: [fix: nvbugs/5355493] Correctly clamp max sequence len to max attention window",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5874",
      "number": 5874,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix typo and remove duplicated code in ZeRO stage 1 and 2",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2655",
      "number": 2655,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix incorrect output on OLMo models in Tensor Parallelism",
      "url": "https://github.com/vllm-project/vllm/pull/3869",
      "number": 3869,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix multiple definition while building evoformer",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4556",
      "number": 4556,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix: include add_special_tokens in kserve request",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2859",
      "number": 2859,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix for pytest picking up wrong deepspeed",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2299",
      "number": 2299,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Add kv cache scales to gemma2.py",
      "url": "https://github.com/vllm-project/vllm/pull/11269",
      "number": 11269,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Triton fix",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2995",
      "number": 2995,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix IDEX dependence in xpu accelerator",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5666",
      "number": 5666,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Use amp autocast in ZeRO3 linear",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/990",
      "number": 990,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: [https://nvbugspro.nvidia.com/bug/5286795] Unwaive tests for bug-5286795.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4724",
      "number": 4724,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : disable FA if KV head size do not match",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7982",
      "number": 7982,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "feat: KV events for sliding window attention",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5580",
      "number": 5580,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Model] Automatic conversion of classification and reward models",
      "url": "https://github.com/vllm-project/vllm/pull/11469",
      "number": 11469,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Nested zero.Init() and dynamically defined model class",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2989",
      "number": 2989,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[TPU] Skip creating empty tensor",
      "url": "https://github.com/vllm-project/vllm/pull/7630",
      "number": 7630,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "cmake: exclude libm.lib when GGML_SYCL=OFF but ONEAPI_ROOT is set in windows builds",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9983",
      "number": 9983,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[v1] - Mamba1 Attention Metadata",
      "url": "https://github.com/vllm-project/vllm/pull/21249",
      "number": 21249,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Q8_0: unbreak AVX",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1117",
      "number": 1117,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix copyright check, add copyright replace script",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3141",
      "number": 3141,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "In streaming output mode, the content in delta is missing from the second to last data #11746",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11747",
      "number": 11747,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Fix] Gemma hidden_activation compatibility",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2614",
      "number": 2614,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Remove index_put from MM embeddings merging",
      "url": "https://github.com/vllm-project/vllm/pull/22105",
      "number": 22105,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: [nvbugs/5324229] Fix broken WInt4AFP8FusedMoEMethod since FusedMoE refactor.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4930",
      "number": 4930,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Diffusers 0.15.0 bug fix",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3345",
      "number": 3345,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Add support for non-power-of-two heads with Alibi",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1611",
      "number": 1611,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix : release torch-managed memory as soon as it's not needed",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3579",
      "number": 3579,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix include prompt in stream response when echo=true",
      "url": "https://github.com/vllm-project/vllm/pull/15233",
      "number": 15233,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Pin vllm-flash-attn==2.5.9.post1",
      "url": "https://github.com/vllm-project/vllm/pull/5476",
      "number": 5476,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[WIP] Try to fix numerical issues in embedding models",
      "url": "https://github.com/vllm-project/vllm/pull/22878",
      "number": 22878,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix per token atrributes bits",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7749",
      "number": 7749,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Fix cache block size for flash decoding",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2351",
      "number": 2351,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Incrementally decode output tokens",
      "url": "https://github.com/vllm-project/vllm/pull/121",
      "number": 121,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[https://nvbugspro.nvidia.com/bug/5300080] Fix the bug of setting attention_chunk_size and enable chunked-attention in the generation-phase by default",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4693",
      "number": 4693,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix entrypoints audio test failure",
      "url": "https://github.com/vllm-project/vllm/pull/18111",
      "number": 18111,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix(router): Fix appending to message content",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2395",
      "number": 2395,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Throw better error for when running into k8s service discovery issue",
      "url": "https://github.com/vllm-project/vllm/pull/18209",
      "number": 18209,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "convert-hf : match model part name prefix and suffix",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7687",
      "number": 7687,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Removed <think> on head of reasoning_content for DeepSeek-R1 model",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5181",
      "number": 5181,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix crash with llama 3.2 vision models and guided decoding",
      "url": "https://github.com/vllm-project/vllm/pull/9631",
      "number": 9631,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "ROCm AWQ support",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1514",
      "number": 1514,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[benchmark] add max-concurrency in result table",
      "url": "https://github.com/vllm-project/vllm/pull/21095",
      "number": 21095,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Openfold fix",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4368",
      "number": 4368,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][CI/Build][ROCm] Make sure to use the headers from the build folder on ROCm",
      "url": "https://github.com/vllm-project/vllm/pull/22264",
      "number": 22264,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Added __HIP_PLATFORM_AMD__=1",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4570",
      "number": 4570,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[fix] avoid the overflow issue when supporting 32k sequence length",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1076",
      "number": 1076,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Fix Embedding Models with TP>1",
      "url": "https://github.com/vllm-project/vllm/pull/5075",
      "number": 5075,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: Skip dummy medusa/eagle tests when WORLD_SIZE env variable is missing",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4786",
      "number": 4786,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[https://nvbugs/5437106][fix] Fix llama4 scout TRTLLM attn_backend",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6690",
      "number": 6690,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[TPU] Fix dummy loading OOM",
      "url": "https://github.com/vllm-project/vllm/pull/16372",
      "number": 16372,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix:Fix test_fp4_quantize_gemm_torch",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3551",
      "number": 3551,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Updates needed for NumPy 2+",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5673",
      "number": 5673,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CUDA: fix logic for clearing padding with -ngl 0",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13320",
      "number": 13320,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Utility] Check for isinstance(exc, Exception) before entering pdb",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1095",
      "number": 1095,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bug]: Mistral model loading reporting unsupported operand type(s)",
      "url": "https://github.com/vllm-project/vllm/issues/18573",
      "number": 18573,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[None][fix] xqa precision for fp16/bf16 kv cache",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6573",
      "number": 6573,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[P/D] NixlConnector use cache device index for memory registration",
      "url": "https://github.com/vllm-project/vllm/pull/18969",
      "number": 18969,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix non-contiguous input passed to Marlin kernel",
      "url": "https://github.com/vllm-project/vllm/pull/15319",
      "number": 15319,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix Qwen2-0.5B in convert-hf-to-gguf.py",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6578",
      "number": 6578,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: fix FA out-of-bounds reads",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7479",
      "number": 7479,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][V1] Allow manual FlashAttention for Blackwell",
      "url": "https://github.com/vllm-project/vllm/pull/19492",
      "number": 19492,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Revert incorrect updates to num_computed_tokens",
      "url": "https://github.com/vllm-project/vllm/pull/8950",
      "number": 8950,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Ensure LoRA path from the request can be included in err msg",
      "url": "https://github.com/vllm-project/vllm/pull/13450",
      "number": 13450,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix VLM example typo",
      "url": "https://github.com/vllm-project/vllm/pull/6859",
      "number": 6859,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix output token length check logic",
      "url": "https://github.com/vllm-project/vllm/pull/16419",
      "number": 16419,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Metrics] Add missing fields in `Reset`",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2574",
      "number": 2574,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[TRTLLM-5085][fix] Nemotron H correctness test",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4444",
      "number": 4444,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[TRTLLM-5252][fix] Propagate mapping to intermediate layers (#6611)",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6765",
      "number": 6765,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix build.zig",
      "url": "https://github.com/ggml-org/llama.cpp/pull/814",
      "number": 814,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix potential int8_t overflow in non-SIMD vec_dot",
      "url": "https://github.com/ggml-org/llama.cpp/pull/986",
      "number": 986,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Enforce that TP > 1 is not supported for Mamba2 if Quantization is Enabled.",
      "url": "https://github.com/vllm-project/vllm/pull/14617",
      "number": 14617,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix SmoothQuantGatedMLP ffn_hidden_size bug",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1712",
      "number": 1712,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: Skip rope scaling for local layers in Gemma3 VLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5773",
      "number": 5773,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix custom all reduce nvlink check on multi node",
      "url": "https://github.com/vllm-project/vllm/pull/4903",
      "number": 4903,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[TPU] Call torch._sync(param) during weight loading",
      "url": "https://github.com/vllm-project/vllm/pull/9437",
      "number": 9437,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "musa: fix typo in cc control",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13144",
      "number": 13144,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix wandb.log() call by removing `sync` kwarg",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7383",
      "number": 7383,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix underscores in dict keys passed via CLI",
      "url": "https://github.com/vllm-project/vllm/pull/19030",
      "number": 19030,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix CI issues related to cupy install",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2483",
      "number": 2483,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Use pure Python to determine IP address instead of nonportable `hostname -I`",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/695",
      "number": 695,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix: `RuntimeError` for UCP large DP",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6918",
      "number": 6918,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama-bench : Generate full token count during warm up",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8187",
      "number": 8187,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fixing a config mismatch in unit test.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2447",
      "number": 2447,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[bugfix] fix blackwell deepep installation",
      "url": "https://github.com/vllm-project/vllm/pull/22255",
      "number": 22255,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix the MoE-params gradient-scaling",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4957",
      "number": 4957,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CUDA: use arch list for compatibility check",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11775",
      "number": 11775,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Bug Fix: Sliding Window Attention ",
      "url": "https://github.com/huggingface/text-generation-inference/pull/3112",
      "number": 3112,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[platform] Do not use current_platform in global namespace",
      "url": "https://github.com/vllm-project/vllm/pull/11362",
      "number": 11362,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix FA3 full cuda graph correctness",
      "url": "https://github.com/vllm-project/vllm/pull/19106",
      "number": 19106,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Bump up mistral_common to support v13 tokenizer",
      "url": "https://github.com/vllm-project/vllm/pull/20905",
      "number": 20905,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: draft target README and assertion for logits-based acceptance",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4167",
      "number": 4167,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: nvbug_5398806",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6239",
      "number": 6239,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Support full cuda graph with sliding window attention",
      "url": "https://github.com/vllm-project/vllm/pull/22168",
      "number": 22168,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Create accelerator for apple silicon GPU Acceleration",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3907",
      "number": 3907,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[SLM] Support `q3f16_1` and `q4f32_1`",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1215",
      "number": 1215,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server : fix coredump on std::terminate() #12831",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13088",
      "number": 13088,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix:remove duplicated trust_remote_code knob from trtllm-serve",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5143",
      "number": 5143,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Update quantize.cpp",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5211",
      "number": 5211,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BUGFIX] Fix `UnspecifiedPlatform` package name",
      "url": "https://github.com/vllm-project/vllm/pull/11916",
      "number": 11916,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix async initializer for OpenAI serving",
      "url": "https://github.com/vllm-project/vllm/pull/4321",
      "number": 4321,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Fix] [torch.compile] Improve UUID system for custom passes",
      "url": "https://github.com/vllm-project/vllm/pull/15249",
      "number": 15249,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Add support for Microsoft Phi-4 model ",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10817",
      "number": 10817,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "graph : fix graph reuse reset of params",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14760",
      "number": 14760,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CUDA: fix matrix multiplication logic for tests",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6667",
      "number": 6667,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Type cast AVX512_BF16 data types based on compiler instead of the OS platform",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7851",
      "number": 7851,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix MTP weight loading ",
      "url": "https://github.com/vllm-project/vllm/pull/21941",
      "number": 21941,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Added wrappers for hpu tensors based on dtype",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5771",
      "number": 5771,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[MISC] Make GroupCoordinator compatible with out-of-tree devices",
      "url": "https://github.com/vllm-project/vllm/pull/16464",
      "number": 16464,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "[fix] fix vllm import error",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1386",
      "number": 1386,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Fixed the abnormally high TTFT issue in the PD disaggregation example",
      "url": "https://github.com/vllm-project/vllm/pull/18644",
      "number": 18644,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CUDA: fix crash with partial offloading of MoE",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13439",
      "number": 13439,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[fix]: Revert commit 388b491",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6143",
      "number": 6143,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml : fix memory leaks when loading invalid gguf files",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10094",
      "number": 10094,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix bias shape in weightOnlyGroupwiseQuantMatmulPlugin for TRT workflow",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4348",
      "number": 4348,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] The special_tokens in tokenizer should also be controlled by do_lower_case in encoder_config.",
      "url": "https://github.com/vllm-project/vllm/pull/20750",
      "number": 20750,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix Phi-3v crash when input images are of certain sizes",
      "url": "https://github.com/vllm-project/vllm/pull/7840",
      "number": 7840,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fixing bf16 test ",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3551",
      "number": 3551,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "batch : fix check for empty sequences in memory",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14364",
      "number": 14364,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: Merge PP overlap and non-overlap executor loop",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3878",
      "number": 3878,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "vulkan/cuda: Fix im2col when KW!=KH",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14789",
      "number": 14789,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Add checks to prevent blocks from being invalidly occupied.",
      "url": "https://github.com/vllm-project/vllm/pull/20568",
      "number": 20568,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1][CI] Fix failed v1-test because of min_p",
      "url": "https://github.com/vllm-project/vllm/pull/13316",
      "number": 13316,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Minor improvements in GPT2 tokenizer",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3567",
      "number": 3567,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Metrics] Add bucket for `request_latency`, `time_to_first_token` and `time_per_output_token`",
      "url": "https://github.com/vllm-project/vllm/pull/15202",
      "number": 15202,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "Update the length of decode token in eagle mode",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2834",
      "number": 2834,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[FIX] SyntaxError: invalid syntax",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/320",
      "number": 320,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "add missing methods to MPS_Accelerator",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5134",
      "number": 5134,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server : refactor middleware and /health endpoint",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9056",
      "number": 9056,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1] Update default max_num_batched_tokens for V1 openai server",
      "url": "https://github.com/vllm-project/vllm/pull/16795",
      "number": 16795,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: fix Volta FlashAttention logic",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11615",
      "number": 11615,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "delay imports for replace policies and fix missing req",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1100",
      "number": 1100,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "common : add GLM-4.5 tool calling support",
      "url": "https://github.com/ggml-org/llama.cpp/pull/15186",
      "number": 15186,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix the workspace size calculation for quantization plugins",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2097",
      "number": 2097,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Include pythonpath and local dir in launch env",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/93",
      "number": 93,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] FA2 MLA Accuracy Issue",
      "url": "https://github.com/vllm-project/vllm/pull/18807",
      "number": 18807,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Re-enable support for `ChatGLMForConditionalGeneration`",
      "url": "https://github.com/vllm-project/vllm/pull/16187",
      "number": 16187,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[None] [fix] improve kvcache allocation in PyTorch runtime",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5933",
      "number": 5933,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "common : allow raw byte in SPM vocabs; don't crash if newline token is not found",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5478",
      "number": 5478,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Avoid graph break by removing redundant requires_grad attr change",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7158",
      "number": 7158,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix for stage3 when setting different communication data type",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4540",
      "number": 4540,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][PD] set max_completion_tokens=1 if req has this value",
      "url": "https://github.com/vllm-project/vllm/pull/21841",
      "number": 21841,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Revert \"Use Cache Hinting for fused_moe kernel (#15511)\"",
      "url": "https://github.com/vllm-project/vllm/pull/15645",
      "number": 15645,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix errors when using smoothquant to quantize Qwen2 model",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2370",
      "number": 2370,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : bump max layers from 256 to 512",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8530",
      "number": 8530,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix][P/D] Fix for cases where _recving_transfers can be cleaned up when *all* transfer done",
      "url": "https://github.com/vllm-project/vllm/pull/19874",
      "number": 19874,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix tests in test_scheduler.py that fail with BlockManager V2",
      "url": "https://github.com/vllm-project/vllm/pull/8728",
      "number": 8728,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Make the shell scripts cross-platform",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14341",
      "number": 14341,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc][V1] Fix type in v1 prefix caching",
      "url": "https://github.com/vllm-project/vllm/pull/11151",
      "number": 11151,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[WIP][Model][Kernel][Bugfix] Commits for new MSFT PhiMoE model",
      "url": "https://github.com/vllm-project/vllm/pull/7691",
      "number": 7691,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "fix the script error in MobileVLM README",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9054",
      "number": 9054,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[TPU][V1][Bugfix] Fix chunked prefill with padding",
      "url": "https://github.com/vllm-project/vllm/pull/15037",
      "number": 15037,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Frontend] Improve error message for too many mm items",
      "url": "https://github.com/vllm-project/vllm/pull/22114",
      "number": 22114,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[NVBUG 5301980] Fix fp4 gemm padding.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4662",
      "number": 4662,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml : do not define GGML_USE_CUDA when building with GGML_BACKEND_DL",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11211",
      "number": 11211,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "quantize: fix F16/F32 downcast to q6_K",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5980",
      "number": 5980,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "make : restore build-info.h dependency for several targets",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3205",
      "number": 3205,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[None][fix] Fix assertion errors of quantization when using online EPLB",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6922",
      "number": 6922,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[TRTLLM-3971][feat] low precision all2all",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6047",
      "number": 6047,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Fix path and python alias errors in disagg_prefill exmaples",
      "url": "https://github.com/vllm-project/vllm/pull/18919",
      "number": 18919,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Scheduler: only schedule prefill chunks when entire context fits",
      "url": "https://github.com/vllm-project/vllm/pull/21809",
      "number": 21809,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Fix] Avoid ref capture in prefix cache contruction",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2391",
      "number": 2391,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix convert-lora-to-ggml.py",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2738",
      "number": 2738,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "lookup: fibonacci hashing, fix crashes",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8548",
      "number": 8548,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Convert example docker command in README to use :latest rather than being pegged to :0.9",
      "url": "https://github.com/huggingface/text-generation-inference/pull/600",
      "number": 600,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix: Revert showing control tokens by default for server OpenAI Chat completions",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6860",
      "number": 6860,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix deepseek-vl2 inference with more than 2 images",
      "url": "https://github.com/vllm-project/vllm/pull/13818",
      "number": 13818,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][CI/CD][CPU] Fix CPU CI tests",
      "url": "https://github.com/vllm-project/vllm/pull/20383",
      "number": 20383,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Use past_key_value when provided",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7428",
      "number": 7428,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "opencl: fix for small models",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11950",
      "number": 11950,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix for distributed tests on pytorch>=1.12",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2141",
      "number": 2141,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix][Attention] Fix sliding window attention in V1 giving incorrect results",
      "url": "https://github.com/vllm-project/vllm/pull/17574",
      "number": 17574,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Fix spec decode example",
      "url": "https://github.com/vllm-project/vllm/pull/20296",
      "number": 20296,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix bad path in prometheus example",
      "url": "https://github.com/vllm-project/vllm/pull/12481",
      "number": 12481,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Implement get_num_physical_cores() for Windows",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1278",
      "number": 1278,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix some typo",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3675",
      "number": 3675,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "convert : update phi-2 to latest HF repo",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4903",
      "number": 4903,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Token length type fixing",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3110",
      "number": 3110,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix: Resolve CVEs identified by SDLe CT7 scan",
      "url": "https://github.com/vllm-project/vllm/pull/16558",
      "number": 16558,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI][Intel GPU] update XPU dockerfile and CI script",
      "url": "https://github.com/vllm-project/vllm/pull/15109",
      "number": 15109,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1][Metrics] Fix http metrics middleware",
      "url": "https://github.com/vllm-project/vllm/pull/15894",
      "number": 15894,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix parameter order issue for hybrid memory initialization",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14725",
      "number": 14725,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix unnecessarily retained pointer to rules parameter in llama_grammar_init",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6003",
      "number": 6003,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Move current_platform import to avoid python import cache.",
      "url": "https://github.com/vllm-project/vllm/pull/16601",
      "number": 16601,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Add a Jinja template to support Mistral3 function calling",
      "url": "https://github.com/vllm-project/vllm/pull/17195",
      "number": 17195,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : add support for GPT2, Bloom and CodeShell tied word embeddings",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12456",
      "number": 12456,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Fix flashinfer plan call to use positional arguments for #3165",
      "url": "https://github.com/huggingface/text-generation-inference/pull/3166",
      "number": 3166,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Fp8 Cache condition",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2611",
      "number": 2611,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: Updates to yarn implementation",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5105",
      "number": 5105,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Limit profiling run sequence length by max_model_len",
      "url": "https://github.com/vllm-project/vllm/pull/14785",
      "number": 14785,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix confusing width in simd_load",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4714",
      "number": 4714,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "examples : sprintf -> snprintf",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8434",
      "number": 8434,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix bug in server.cpp for grammar.",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4494",
      "number": 4494,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Clean up MiniCPM-V",
      "url": "https://github.com/vllm-project/vllm/pull/6939",
      "number": 6939,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "fix names and license",
      "url": "https://github.com/vllm-project/vllm/pull/2589",
      "number": 2589,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "fix crash on non-AVX systems dynamically loading GGML CPU backends",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11780",
      "number": 11780,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Move GLM4 f32 attention fix to the correct function",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13750",
      "number": 13750,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "python : bump transformers version",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13351",
      "number": 13351,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[TPU][V1] Fix exponential padding when `max-num-batched-tokens` is not a power of 2",
      "url": "https://github.com/vllm-project/vllm/pull/16596",
      "number": 16596,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Android] Remove var capture in TVM_SOURCE_DIR",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2538",
      "number": 2538,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Fix][Spec] Add the draft model PopN in chain speculative decoding",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2939",
      "number": 2939,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bug][Frontend] Fix structure of transcription's decoder_prompt",
      "url": "https://github.com/vllm-project/vllm/pull/18809",
      "number": 18809,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Upgrading the dependencies in Gaudi backend.",
      "url": "https://github.com/huggingface/text-generation-inference/pull/3170",
      "number": 3170,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Eagle: change config name for fc bias",
      "url": "https://github.com/vllm-project/vllm/pull/9580",
      "number": 9580,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Add missing attributes in mistral tokenizer",
      "url": "https://github.com/vllm-project/vllm/pull/8364",
      "number": 8364,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[AutoDeploy] more robust handling of attention interface and input nodes",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4369",
      "number": 4369,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Update default neuron config for speculation",
      "url": "https://github.com/vllm-project/vllm/pull/18274",
      "number": 18274,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc][LoRA] Ensure Lora Adapter requests return adapter name",
      "url": "https://github.com/vllm-project/vllm/pull/11094",
      "number": 11094,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Update GitHub workflows to work with PyTorch 2.0.0, apply fixes or pin to previous versions.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3039",
      "number": 3039,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix multi-modality apply chat template issue",
      "url": "https://github.com/huggingface/text-generation-inference/pull/3258",
      "number": 3258,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Revert \"refactor: Replace DecoderFinishedEvent with CudaEvent in deco…",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3183",
      "number": 3183,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix non-fp tensor bugs of contiguous activation checkpointing",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1376",
      "number": 1376,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Pass in correct VLLM config in FlashInfer backend (#13207)",
      "url": "https://github.com/vllm-project/vllm/pull/16973",
      "number": 16973,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Kernel] FA3 Fix - RuntimeError: This flash attention build only supports pack_gqa (for build size reasons).",
      "url": "https://github.com/vllm-project/vllm/pull/12405",
      "number": 12405,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "fix makefile and cmake logic for AARCH64",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11246",
      "number": 11246,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Avoid repeatedly creating dummy data during engine startup",
      "url": "https://github.com/vllm-project/vllm/pull/17935",
      "number": 17935,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Model][Misc] Add MistralModel and Embedding API",
      "url": "https://github.com/vllm-project/vllm/pull/3677",
      "number": 3677,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : greatly reduce output buffer memory usage",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6122",
      "number": 6122,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Unit tests setup own venv",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2628",
      "number": 2628,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Support attention_bias on LLaMA architecture",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4283",
      "number": 4283,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix hermes tool parser handling of non-string argument types",
      "url": "https://github.com/vllm-project/vllm/pull/22002",
      "number": 22002,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1][Bugfix] Fix typo in MoE TPU checking",
      "url": "https://github.com/vllm-project/vllm/pull/15927",
      "number": 15927,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix more broken speculative decode tests",
      "url": "https://github.com/vllm-project/vllm/pull/17450",
      "number": 17450,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1] Add VLLM_ALLOW_INSECURE_SERIALIZATION env var",
      "url": "https://github.com/vllm-project/vllm/pull/17490",
      "number": 17490,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Update path name on xpu-max1100.yml, add badge in README",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5386",
      "number": 5386,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix MLA chunked prefill performance",
      "url": "https://github.com/vllm-project/vllm/pull/15688",
      "number": 15688,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Installation] OpenTelemetry version update",
      "url": "https://github.com/vllm-project/vllm/pull/17771",
      "number": 17771,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[MISC] Fix import error for fused_moe",
      "url": "https://github.com/vllm-project/vllm/pull/18641",
      "number": 18641,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Add missing headers for mpiUtils.h to compile with gcc13",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2315",
      "number": 2315,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "Fix Python3.8 compatibility breakage",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1210",
      "number": 1210,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Update zeropp.md",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3821",
      "number": 3821,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[BugFix] Fix extra ChatConfig args crash",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/722",
      "number": 722,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix the max_seq_len limit of 16384 for DeepSeek models",
      "url": "https://github.com/vllm-project/vllm/pull/20322",
      "number": 20322,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Speculative Decoding] Fixing hidden states handling in batch expansion",
      "url": "https://github.com/vllm-project/vllm/pull/7508",
      "number": 7508,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[https://nvbugs/5429636][fix] fix KVC mem leakage by adding kv_transfer_timeout_ms",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6801",
      "number": 6801,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Integrate TensorSchema with shape validation for Phi3VImagePixelInputs",
      "url": "https://github.com/vllm-project/vllm/pull/21232",
      "number": 21232,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[None][fix] Explicitly add tiktoken as required by kimi k2",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6663",
      "number": 6663,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI] use ccache actions properly in release workflow",
      "url": "https://github.com/vllm-project/vllm/pull/4629",
      "number": 4629,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "[CANN]: Fix ggml_backend_cann_buffer_get_tensor",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8871",
      "number": 8871,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: GPT-Next convert failure",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3220",
      "number": 3220,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Change disaggregated prefill example model name reflecting the change on HF",
      "url": "https://github.com/vllm-project/vllm/pull/13573",
      "number": 13573,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix bug in bfloat16 optimizer related to checkpointing",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4434",
      "number": 4434,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix: avoid duplicate bos token",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1594",
      "number": 1594,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : support StableLM 2 1.6B",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5052",
      "number": 5052,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Disable outlines cache by default",
      "url": "https://github.com/vllm-project/vllm/pull/14837",
      "number": 14837,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[ROCm] Fix the Kernels, Core, and Prefix Caching AMD CI groups",
      "url": "https://github.com/vllm-project/vllm/pull/13970",
      "number": 13970,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server : fix LRU check",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14079",
      "number": 14079,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[None][fix] fix ci",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6814",
      "number": 6814,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "gguf : fix a few general keys",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3341",
      "number": 3341,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fixes in _partition_param_sec function",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5613",
      "number": 5613,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Avoid unnecessary Ray import warnings",
      "url": "https://github.com/vllm-project/vllm/pull/6079",
      "number": 6079,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "[Issue #12458] Temporarily Clamp inf Values in ggml-cpu.c to Prevent Garbled Output(or coredump) on RK3588",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12459",
      "number": 12459,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "cuda : fix device sync on buffer clear",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14033",
      "number": 14033,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Update TensorRT-LLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1427",
      "number": 1427,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Support FIPS enabled machines with MD5 hashing",
      "url": "https://github.com/vllm-project/vllm/pull/15299",
      "number": 15299,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: fix the speculative decoding accuracy issue when using cuda graph padded requests (dummy requests) and enabling overlap scheduler",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4301",
      "number": 4301,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fixing the cpu_adam API and add deepspeed_adam flag in config.py",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/365",
      "number": 365,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama-chat : fix multiple system messages for gemma, orion",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14246",
      "number": 14246,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fixing the CPU-Adam compile issue for the AMD architecture",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/796",
      "number": 796,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "cmake : fix VULKAN and ROCm builds",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5525",
      "number": 5525,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix failing transformers dynamic module resolving with spawn multiproc method",
      "url": "https://github.com/vllm-project/vllm/pull/13403",
      "number": 13403,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix/remove some broken model executor tests",
      "url": "https://github.com/vllm-project/vllm/pull/21224",
      "number": 21224,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: Fix broken vanilla moe since FusedMoE refactor.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4897",
      "number": 4897,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "vulkan: fix coopmat2 validation failures",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11284",
      "number": 11284,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[feat] Add support to load fp8 Meta Llama4 weights",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6268",
      "number": 6268,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build] fix pip cache with vllm_nccl & refactor dockerfile to build wheels",
      "url": "https://github.com/vllm-project/vllm/pull/3859",
      "number": 3859,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Doc] Avoid documenting dynamic / internal modules",
      "url": "https://github.com/vllm-project/vllm/pull/18626",
      "number": 18626,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[ci][misc] fix more device count",
      "url": "https://github.com/vllm-project/vllm/pull/6055",
      "number": 6055,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build] LoRA : make add_lora_test safer",
      "url": "https://github.com/vllm-project/vllm/pull/15181",
      "number": 15181,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Vision][Fix] Follow phi3.5-vision prompt format for image",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2928",
      "number": 2928,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix transpose convolution FLOPS profiler (retrieval of out_channels)",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3834",
      "number": 3834,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix sorting of shard optimizer states files for universal checkpoint",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5395",
      "number": 5395,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix cuda illegal mem access with Llama4 TP8 + rms_norm custom op",
      "url": "https://github.com/vllm-project/vllm/pull/22701",
      "number": 22701,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Move current_platform import to avoid python import cache.",
      "url": "https://github.com/vllm-project/vllm/pull/16601",
      "number": 16601,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix bias in InternLM",
      "url": "https://github.com/vllm-project/vllm/pull/1501",
      "number": 1501,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix discard_names bug in safetensors convertion",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1052",
      "number": 1052,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama-chat : Do not throw when tool parsing fails",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14012",
      "number": 14012,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "[zero3] remove debug prints",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/986",
      "number": 986,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "[doc] fix incorrect param name",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/773",
      "number": 773,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI Failure] Fix OOM with test_oot_registration_embedding",
      "url": "https://github.com/vllm-project/vllm/pull/20144",
      "number": 20144,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Fix build with `--features google`",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2566",
      "number": 2566,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[fix] fix typo s/simultanenously /simultaneously",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5359",
      "number": 5359,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "[main] fix infinite generation (-n == -1)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/523",
      "number": 523,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Build] Fix cuda link target of cumem_allocator in CPU env",
      "url": "https://github.com/vllm-project/vllm/pull/12863",
      "number": 12863,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix `_init_vision_model` in NVLM_D model",
      "url": "https://github.com/vllm-project/vllm/pull/9611",
      "number": 9611,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix e2e test failure for RTX6000 Pro ",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6420",
      "number": 6420,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[ROCm][AMD][Bugfix] adding a missing triton autotune config",
      "url": "https://github.com/vllm-project/vllm/pull/4845",
      "number": 4845,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bug] Fix Attention when ignored in by quant_method",
      "url": "https://github.com/vllm-project/vllm/pull/14313",
      "number": 14313,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Docs] Revert the `--no-index` changes in instalation commands",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/3101",
      "number": 3101,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix retrieve_process not ending normally and resources not being released properly",
      "url": "https://github.com/vllm-project/vllm/pull/21502",
      "number": 21502,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix incorrect prompt tokenization in speculative example",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4025",
      "number": 4025,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "vocab : fix ugm tokenizer precision",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13743",
      "number": 13743,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[BUG]bucket.elements is not correctly cleared in ZeRO Stage3",
      "url": "https://github.com/deepspeedai/DeepSpeed/issues/7415",
      "number": 7415,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "more info for preshard ",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2027",
      "number": 2027,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix 'ModuleNotFoundError: No module named 'intel_extension_for_pytorch'' for --tensor-parallel-size more than 1 ",
      "url": "https://github.com/vllm-project/vllm/pull/12546",
      "number": 12546,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1] Delay all xgrammar usage until needed",
      "url": "https://github.com/vllm-project/vllm/pull/14616",
      "number": 14616,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Refactor `projector_type` enum to enum class for type safety and clarity",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10618",
      "number": 10618,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[fix] Fix relaxed acceptance to support enabling it in context phase",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4126",
      "number": 4126,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: Updates to yarn",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4774",
      "number": 4774,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Doc] Fix API doc link in side navigation",
      "url": "https://github.com/vllm-project/vllm/pull/22585",
      "number": 22585,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Fix KVConnector TP worker aggregation",
      "url": "https://github.com/vllm-project/vllm/pull/21473",
      "number": 21473,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Swift: Fix Windows build",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8742",
      "number": 8742,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : use n_vocab to differentiate between mistral 7B and llama3 8B",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7200",
      "number": 7200,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Frontend] Align tool_choice=\"required\" behavior with OpenAI when tools is empty",
      "url": "https://github.com/vllm-project/vllm/pull/21052",
      "number": 21052,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: Fix pipeline parallelism for Llama-style models",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3422",
      "number": 3422,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Update Prometheus datasource configuration to use variable UID",
      "url": "https://github.com/vllm-project/vllm/pull/12659",
      "number": 12659,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Model] use AutoWeightsLoader for granite, grok1, mixtral",
      "url": "https://github.com/vllm-project/vllm/pull/16233",
      "number": 16233,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "vulkan: fix coopmat shader generation when cross-compiling",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12272",
      "number": 12272,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Take 2] Correctly kill vLLM processes after benchmarks",
      "url": "https://github.com/vllm-project/vllm/pull/21646",
      "number": 21646,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix kleidiai build",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12159",
      "number": 12159,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : fix K-shift with quantized K and BLAS backend",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13113",
      "number": 13113,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix device ordinal when initializing spec_decode_sampler under multi-node setup",
      "url": "https://github.com/vllm-project/vllm/pull/13269",
      "number": 13269,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "sampling: make top_n_sigma no-op at <=0 rather than <0",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13345",
      "number": 13345,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "vulkan: initialize devices properly for LLAMA_SPLIT_MODE_NONE",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7552",
      "number": 7552,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Removes spurious \\r in output that causes logging in journalctl to tr…",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10771",
      "number": 10771,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[fix] Use decorator for request cancelation and handle CancelledError",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5559",
      "number": 5559,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: Properly get decoding mode according to same logic as cpp.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4026",
      "number": 4026,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "vulkan: Optimize some mat-vec mul quant shaders",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10296",
      "number": 10296,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Bug fix] Small fix on RestAPIArgs",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/724",
      "number": 724,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CUDA: implement __hmax and __hmax2 for CUDA < 11.7",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7019",
      "number": 7019,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: Skip rope scaling for local layers in Gemma3 VLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5857",
      "number": 5857,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix a typo of global variable in comm.py",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3852",
      "number": 3852,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "None: Make add_special_tokens to false for completions API",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4583",
      "number": 4583,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "kompute: improve backend to pass test_backend_ops",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10542",
      "number": 10542,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build] Add error matching config for mypy",
      "url": "https://github.com/vllm-project/vllm/pull/9512",
      "number": 9512,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Allow all RDNA2 archs to use sdot4 intrinsic",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8629",
      "number": 8629,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "corrected Pydantic warning.",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2095",
      "number": 2095,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Enable tensor fragments for zero 2 & 3",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2727",
      "number": 2727,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Guard against all weights in a super-block being zero",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3010",
      "number": 3010,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[fix] Fix a few issues with EAGLE3 in PyTorch backend",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3686",
      "number": 3686,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Enforce that TP > 1 is not supported for Mamba2 if Quantization is Enabled.",
      "url": "https://github.com/vllm-project/vllm/pull/14617",
      "number": 14617,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix(server): Fix stop sequences",
      "url": "https://github.com/huggingface/text-generation-inference/pull/11",
      "number": 11,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: Set correct draft_token_nums to dummy requests for torch compilation with MTP",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3053",
      "number": 3053,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "`server`: fix format of streamed tool call deltas (diff name, fix id location)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13800",
      "number": 13800,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Update TensorRT-LLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2333",
      "number": 2333,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "nvbugs-5331031; nvbugs-5344203 - address intermittent issues with Mistral Small multimodal for BS=8",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5453",
      "number": 5453,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "remove force-multi and fix None val check in base tuner in autotuning",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1657",
      "number": 1657,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[ROCm][V0][Attention] Revert to th old FA triton kernel",
      "url": "https://github.com/vllm-project/vllm/pull/18225",
      "number": 18225,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Model load fix for qwen2 0.5b and 1.5b",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2303",
      "number": 2303,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "tests : add integration test for lora adapters",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8957",
      "number": 8957,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[zero_to_fp32] adapt to 4-bytes alignment in z2",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1372",
      "number": 1372,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[KVCache] Add max num threads awareness to KVCache kernels",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1822",
      "number": 1822,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "ggml : restore abort() in GGML_ASSERT",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4242",
      "number": 4242,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "fix security issue of logging llm output",
      "url": "https://github.com/vllm-project/vllm/pull/18980",
      "number": 18980,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: fix max_seq_len in executor_config",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3487",
      "number": 3487,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "Fix prefill stats",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/963",
      "number": 963,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[fix] Move kv_cache_free_gpu_mem_fraction arg to benchmark command in tests",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6463",
      "number": 6463,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][Frontend] Strip empty tool calls from incoming chat conversations",
      "url": "https://github.com/vllm-project/vllm/pull/14054",
      "number": 14054,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : add support for EXAONE tied word embeddings",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12451",
      "number": 12451,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Model] Fix Ernie4.5MoE e_score_correction_bias parameter",
      "url": "https://github.com/vllm-project/vllm/pull/21586",
      "number": 21586,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "flake: Restore default package's buildInputs #3261",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3262",
      "number": 3262,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] fix qwen2 reward model loading issue(#9755)",
      "url": "https://github.com/vllm-project/vllm/pull/9756",
      "number": 9756,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[ROCm] Fix the Kernels, Core, and Prefix Caching AMD CI groups",
      "url": "https://github.com/vllm-project/vllm/pull/13970",
      "number": 13970,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Fix] Add seps before first role",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2439",
      "number": 2439,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix: Include special tokens when tokenizing in front-end",
      "url": "https://github.com/huggingface/text-generation-inference/pull/14",
      "number": 14,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Model][3/N] Automatic conversion of CrossEncoding model",
      "url": "https://github.com/vllm-project/vllm/pull/20168",
      "number": 20168,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: Fixing minor typo in allreduce kernel selection",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3912",
      "number": 3912,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "ci : reduce 3b ppl chunks to 1 to avoid timeout",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5771",
      "number": 5771,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix(server): fix past key values logic",
      "url": "https://github.com/huggingface/text-generation-inference/pull/216",
      "number": 216,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][ROCm] Add numba to Dockerfile.rocm",
      "url": "https://github.com/vllm-project/vllm/pull/3962",
      "number": 3962,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[AutoDeploy] disable flaky MoE nvfp4 test",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6302",
      "number": 6302,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix underscores in dict keys passed via CLI",
      "url": "https://github.com/vllm-project/vllm/pull/19030",
      "number": 19030,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Add support for Marlin 2:4 sparsity",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2102",
      "number": 2102,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[ROCm] Temporarily remove GPTQ ROCm support",
      "url": "https://github.com/vllm-project/vllm/pull/2138",
      "number": 2138,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Fix  misleading ROCm warning",
      "url": "https://github.com/vllm-project/vllm/pull/19486",
      "number": 19486,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ARM: Fixes and additions to CPU feature detection",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14049",
      "number": 14049,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "support JIT mha.cu for SPEC_DEC in runtime",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6078",
      "number": 6078,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Fix mistral with length > window_size for long prefills (rotary doesn't create long enough cos, sin).",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1571",
      "number": 1571,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix BNB loader target_modules",
      "url": "https://github.com/vllm-project/vllm/pull/10720",
      "number": 10720,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "unit test for bugfix #1135",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1148",
      "number": 1148,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix PiecewiseCompileInterpreter",
      "url": "https://github.com/vllm-project/vllm/pull/17338",
      "number": 17338,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Add missing `get_language_model` to new MLLMs",
      "url": "https://github.com/vllm-project/vllm/pull/17300",
      "number": 17300,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "convert : fix rwkv bos/eos token",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13844",
      "number": 13844,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build] Fix pre-commit errors",
      "url": "https://github.com/vllm-project/vllm/pull/13696",
      "number": 13696,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix ggml-cuda using a driver symbol in NO_VMM mode",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11188",
      "number": 11188,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[misc] fix typing",
      "url": "https://github.com/vllm-project/vllm/pull/11540",
      "number": 11540,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Frontend] Bad words sampling parameter",
      "url": "https://github.com/vllm-project/vllm/pull/9717",
      "number": 9717,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "convert-hf : save memory with lazy evaluation",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7075",
      "number": 7075,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] VocabParallelEmbedding layer class assertion",
      "url": "https://github.com/vllm-project/vllm/pull/15823",
      "number": 15823,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CUDA: deduplicate FlashAttention code",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7352",
      "number": 7352,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[Fix] fix the building of trtllm with command `TRTLLM_USE_PRECOMPILED=1 pip install -e .`",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6409",
      "number": 6409,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[https://nvbugs/5394392][fix] Enlarge scheduler and slot manager capacity under disagg bs == 1",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6537",
      "number": 6537,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "chore: Fix KV cache block reuse flag name in quickstart_advanced",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3781",
      "number": 3781,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Attention] Add out dtype for QK matmul",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1638",
      "number": 1638,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1] Solve potential deadlock issue in v1 engine core client internally",
      "url": "https://github.com/vllm-project/vllm/pull/19927",
      "number": 19927,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Core] hybrid dtype for Pooling Models: float32 for weights and activation, float16 or bfloat16 for attention. ",
      "url": "https://github.com/vllm-project/vllm/pull/18940",
      "number": 18940,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[bugfix] fix the wrong parser",
      "url": "https://github.com/vllm-project/vllm/pull/17958",
      "number": 17958,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[BUG5374319][fix] WAR for draft-target-model unit tests error",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5958",
      "number": 5958,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Ds inference/fix mp2",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2270",
      "number": 2270,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Add missing kernel for CodeLlama-34B on A/H100 (no tensor parallelism) when using Multi-LoRA.",
      "url": "https://github.com/vllm-project/vllm/pull/3350",
      "number": 3350,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml-quants: Provide ggml_vqtbl1q_u8 for 64bit compatibility ",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5711",
      "number": 5711,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Move GLM4 f32 attention fix to the correct function",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13750",
      "number": 13750,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix partition activations issue when mp=2 and pp=2",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1589",
      "number": 1589,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Fixes and Optimizations for DeepEP + DeepGEMM combination.",
      "url": "https://github.com/vllm-project/vllm/pull/19298",
      "number": 19298,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: fix race condition in MMQ stream-k fixup",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13299",
      "number": 13299,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix CI build when changing only the CUDA sources",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1800",
      "number": 1800,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix the bug in remove_instance_endpoint method handling prefill_insta…",
      "url": "https://github.com/vllm-project/vllm/pull/17761",
      "number": 17761,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix][Minor] Fix full cuda graph bug when max_num_seqs < 512",
      "url": "https://github.com/vllm-project/vllm/pull/19171",
      "number": 19171,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build] Split up models tests",
      "url": "https://github.com/vllm-project/vllm/pull/10069",
      "number": 10069,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "vulkan: Fix newly added tests for permuted mul_mat and 1D im2col",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10226",
      "number": 10226,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Update TensorRT-LLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1315",
      "number": 1315,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "vulkan: Handle src1 batch dimension in non-contiguous mat-vec-mul shader",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13191",
      "number": 13191,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : deci : support ffn-free with attention",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13296",
      "number": 13296,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix crash when running train with CUDA enabled",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1952",
      "number": 1952,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "metal : fix build and swift package",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10279",
      "number": 10279,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[BUGFIX] Reset `bucket.elements` after reduction in ZeRO Stage 3",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7418",
      "number": 7418,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "metal : build metallib + fix embed path",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6015",
      "number": 6015,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "fix: ensure proper cleanup of img_res_v.data in all code paths",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11692",
      "number": 11692,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Docs] Fix readthedocs for tag build",
      "url": "https://github.com/vllm-project/vllm/pull/6158",
      "number": 6158,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Fix idefics_causal_lm.py syntax issues ",
      "url": "https://github.com/huggingface/text-generation-inference/pull/947",
      "number": 947,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1] add error handling",
      "url": "https://github.com/vllm-project/vllm/pull/11420",
      "number": 11420,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "vulkan: fix bug in coopmat1 mul_mat_id",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12316",
      "number": 12316,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "BF16 optimizer: Clear lp grads after updating hp grads in hook",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5328",
      "number": 5328,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix RMSNorm forward in InternViT attention qk_layernorm",
      "url": "https://github.com/vllm-project/vllm/pull/6992",
      "number": 6992,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix Ernie4.5 MoE without shared experts",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14746",
      "number": 14746,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix a couple of Voxtral tests",
      "url": "https://github.com/vllm-project/vllm/pull/21218",
      "number": 21218,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "fix: change GB to GiB in logging close #14979",
      "url": "https://github.com/vllm-project/vllm/pull/15807",
      "number": 15807,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[ROCm] temporary workaround till __double2half support enabled in HIP",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3236",
      "number": 3236,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix build when nvtools is missing",
      "url": "https://github.com/vllm-project/vllm/pull/3698",
      "number": 3698,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "opencl: Noncontiguous `norm`, `rms_norm`, disable `fp16` for some ops",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12217",
      "number": 12217,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Add some minimal optimizations for CDNA",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10498",
      "number": 10498,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix distributed bug again in Qwen2.5-VL & Qwen2.5-Omni",
      "url": "https://github.com/vllm-project/vllm/pull/16974",
      "number": 16974,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[fix] Loosen the thresholds of test_attention_mla",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4074",
      "number": 4074,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix][Intel GPU] Use refactored API for dist_backend in V1 worker",
      "url": "https://github.com/vllm-project/vllm/pull/20596",
      "number": 20596,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI/CD] fix: test metric tag setting",
      "url": "https://github.com/vllm-project/vllm/pull/13717",
      "number": 13717,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Feat/mtp opt 2 lamport allgather",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6500",
      "number": 6500,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix multinode runner to properly append to PDSH_SSH_ARGS_APPEND",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4373",
      "number": 4373,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Fix] Gemma hidden_activation compatibility",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2614",
      "number": 2614,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Revert \"[V1] [Bugfix] eagle bugfix and enable correct lm_head for multimodal (#18034)\"",
      "url": "https://github.com/vllm-project/vllm/pull/18600",
      "number": 18600,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "cam: parallel simple fixes",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3348",
      "number": 3348,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "opencl: remove unnecessary assert for `add`",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13257",
      "number": 13257,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "more fine-grained manifest file for includes/excludes",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/540",
      "number": 540,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "cli : auto activate conversation mode if chat template is available",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11214",
      "number": 11214,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Validate-Params: refactoring-gpt-params-parse",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2454",
      "number": 2454,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Vulkan Optimizations and Fixes",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8959",
      "number": 8959,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Support AWQ quantization with bias",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2117",
      "number": 2117,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "vulkan: fix rms_norm_mul to handle broadcasting dim0",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14817",
      "number": 14817,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: params passed to BuildConfig will propagate to LlmArgs as well",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4913",
      "number": 4913,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix a spelling mistake in llama-sampling.cpp",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9001",
      "number": 9001,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "HuggingFaceM4/Idefics3-8B-Llama3 crash fix",
      "url": "https://github.com/huggingface/text-generation-inference/pull/3267",
      "number": 3267,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Core] Fix an isort error from pre-commit",
      "url": "https://github.com/vllm-project/vllm/pull/12327",
      "number": 12327,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Add local attention for GPT-Neo model architecture",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1114",
      "number": 1114,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: Fix PP for llama.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3449",
      "number": 3449,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix missing docs and out of sync `EngineArgs`",
      "url": "https://github.com/vllm-project/vllm/pull/4219",
      "number": 4219,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix triton import with local TritonPlaceholder",
      "url": "https://github.com/vllm-project/vllm/pull/17446",
      "number": 17446,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix QKVParallelLinearWithShardedLora bias bug",
      "url": "https://github.com/vllm-project/vllm/pull/10844",
      "number": 10844,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix the memory offset of cpu adam when using fp16 param",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1318",
      "number": 1318,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Allow users to specify kv cache memory size",
      "url": "https://github.com/vllm-project/vllm/pull/21489",
      "number": 21489,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix memory bugs in loading code",
      "url": "https://github.com/ggml-org/llama.cpp/pull/651",
      "number": 651,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix RWKV v6 model conversion",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10913",
      "number": 10913,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[core] further polish memory profiling",
      "url": "https://github.com/vllm-project/vllm/pull/12126",
      "number": 12126,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Use runner_type instead of task in GritLM",
      "url": "https://github.com/vllm-project/vllm/pull/11144",
      "number": 11144,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "fix: restore MiniCPM inference after Granite Four changes",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14850",
      "number": 14850,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CUDA: fix --split-mode row for MMQ",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13323",
      "number": 13323,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "FIX: \"inline\" -> \"static inline\" for bytesFromNibbles and packNibbles",
      "url": "https://github.com/ggml-org/llama.cpp/pull/161",
      "number": 161,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix eviction cached blocked logic",
      "url": "https://github.com/vllm-project/vllm/pull/21357",
      "number": 21357,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "tests: Fix lora perf test",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5875",
      "number": 5875,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][1/n] Fix the speculative decoding test by setting the target dtype",
      "url": "https://github.com/vllm-project/vllm/pull/19633",
      "number": 19633,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[https://nvbugspro.nvidia.com/bug/5323820] Fix chunking equation for disabled case.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4964",
      "number": 4964,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix `checkpointable_layers` Logic",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6881",
      "number": 6881,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fallback to outlines for complex json schemas",
      "url": "https://github.com/vllm-project/vllm/pull/10899",
      "number": 10899,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Tokenizer][Fix] Fix SegFault when analyzing tokenizers without tokenizer.json",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2532",
      "number": 2532,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Fix] Use utf-8 encoding in entrypoints/openai/run_batch.py",
      "url": "https://github.com/vllm-project/vllm/pull/5606",
      "number": 5606,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Core][Bugfix] new way for full cudagraph, add support for FA2 and FlashInfer; Two minor bugs fixed",
      "url": "https://github.com/vllm-project/vllm/pull/20050",
      "number": 20050,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: use current_image_tags.properties in rename_docker_images.py",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5846",
      "number": 5846,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "vulkan: optimizations for direct convolution",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14933",
      "number": 14933,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix for Zero3 when MP>1",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2289",
      "number": 2289,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Improve consistency of zero_grad",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6554",
      "number": 6554,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "FIX LLMEngine.seq_id_to_seq_group_size memory leak",
      "url": "https://github.com/vllm-project/vllm/pull/14355",
      "number": 14355,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix]fix modelscope compatible issue",
      "url": "https://github.com/vllm-project/vllm/pull/6730",
      "number": 6730,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix: `RuntimeError` for UCP large DP",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6918",
      "number": 6918,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Ds inference/fix mp2",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2270",
      "number": 2270,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Improve usability of --model-url & related flags",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6930",
      "number": 6930,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: Fix max attn window in TRTLLM Sampler.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6354",
      "number": 6354,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Model] Disable sliding window for Gemma3",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/3176",
      "number": 3176,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: [nvbugs/5324229] Fix broken WInt4AFP8FusedMoEMethod since FusedMoE refactor.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4930",
      "number": 4930,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: nvbugs/5231298: pytorch allreduce issue",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3673",
      "number": 3673,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Server: when no slot is available, defer the task instead of returning \"slot unavailable\"",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5018",
      "number": 5018,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "ggml : fix YARN + add tests + add asserts",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7617",
      "number": 7617,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "convert.py : Update to support 70B HF format model files",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2427",
      "number": 2427,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix reasoning_content for chat_template include <think> tag as input",
      "url": "https://github.com/vllm-project/vllm/pull/14841",
      "number": 14841,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build] bump ruff version, fix linting issues",
      "url": "https://github.com/vllm-project/vllm/pull/6546",
      "number": 6546,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix typo",
      "url": "https://github.com/vllm-project/vllm/pull/1274",
      "number": 1274,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix args in benchmark_serving",
      "url": "https://github.com/vllm-project/vllm/pull/3836",
      "number": 3836,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "compare-llama-bench.py: fix long hexsha args",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6424",
      "number": 6424,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix: default num_ln_in_parallel_attn to one if not supplied",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2364",
      "number": 2364,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server: args for draft model cache types (#11200)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13782",
      "number": 13782,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: fix cublas_scaled_mm",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3600",
      "number": 3600,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Bug fix] Small fix on RestAPIArgs",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/724",
      "number": 724,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Reducing the memory-overhead of creating large-models for multi-GPU run",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1244",
      "number": 1244,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][V1] Fix FlashInfer V1 backend using the wrong VllmConfig",
      "url": "https://github.com/vllm-project/vllm/pull/18086",
      "number": 18086,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "one more CMAKE_CXX_FLAGS fix",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9471",
      "number": 9471,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix memory leak with zero-3",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3903",
      "number": 3903,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Hardware][CPU] Build fix for ARM without BF16",
      "url": "https://github.com/vllm-project/vllm/pull/21848",
      "number": 21848,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][V1] Fix compiled graph hash",
      "url": "https://github.com/vllm-project/vllm/pull/14867",
      "number": 14867,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "model : add hunyuan moe",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14425",
      "number": 14425,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Bug fix for norm calculation in absence of model parallel group",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/551",
      "number": 551,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "run : fix BOS being added to each message",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11302",
      "number": 11302,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Update compatible packaging version",
      "url": "https://github.com/vllm-project/vllm/pull/19308",
      "number": 19308,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Deprecation][2/N] Replace `--task` with `--runner` and `--convert`",
      "url": "https://github.com/vllm-project/vllm/pull/21470",
      "number": 21470,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix fully sharded LoRAs with Mixtral",
      "url": "https://github.com/vllm-project/vllm/pull/11390",
      "number": 11390,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix `MQLLMEngine` hanging",
      "url": "https://github.com/vllm-project/vllm/pull/9973",
      "number": 9973,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix(router): fix a possible deadlock in next_batch",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1731",
      "number": 1731,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][TPU] Fix KV cache size calculation",
      "url": "https://github.com/vllm-project/vllm/pull/5860",
      "number": 5860,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "Update quick_start.rst to fix broken links",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2607",
      "number": 2607,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[Bug fix] WarmupCosineLR issues",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4688",
      "number": 4688,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][v1] fixed llava-hf/llava-1.5-7b-hf is broken on V1",
      "url": "https://github.com/vllm-project/vllm/pull/14554",
      "number": 14554,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Option to exclude frozen weights for checkpoint save",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3953",
      "number": 3953,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "common: fixed not working find argument --n-gpu-layers-draft",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9175",
      "number": 9175,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix attribute error when loading FusedAdamBuilder()",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3527",
      "number": 3527,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix Qwen3-Embedding pre-tokenizer hash",
      "url": "https://github.com/ggml-org/llama.cpp/pull/15030",
      "number": 15030,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix sampling_params passed incorrectly in Phi3v example",
      "url": "https://github.com/vllm-project/vllm/pull/5684",
      "number": 5684,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix loading universal checkpoint for BF16_Optimizer",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5211",
      "number": 5211,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: update test_user_buffers_mm_add_prologue atol (#3711)",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3713",
      "number": 3713,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Codeowner addendum and fix to small model debugging script",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2076",
      "number": 2076,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix: pad DeepEP fp4 recv tensors if empty",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6048",
      "number": 6048,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1] Bugfix: Validate Model Input Length",
      "url": "https://github.com/vllm-project/vllm/pull/12600",
      "number": 12600,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "HIP: force max threads per block to be 1024",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11621",
      "number": 11621,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Do not convert weight scale to e4m3fnuz on CUDA",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2917",
      "number": 2917,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[DRAFT] refactor: PyExecutor uses a list-type for response handling",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5406",
      "number": 5406,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Support tensor parallel",
      "url": "https://github.com/vllm-project/vllm/pull/2",
      "number": 2,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "cmake : use ggml-metal.metal from source dir to build default.metallib",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9325",
      "number": 9325,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": " migrate pydantic from v1 to v2",
      "url": "https://github.com/vllm-project/vllm/pull/2531",
      "number": 2531,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "chore(sever): update requirements",
      "url": "https://github.com/huggingface/text-generation-inference/pull/357",
      "number": 357,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : adjust default context size + print warnings",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10136",
      "number": 10136,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Fix : fix moe regression for sm120",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5823",
      "number": 5823,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix Phi3.5 mini and MoE LoRA inference",
      "url": "https://github.com/vllm-project/vllm/pull/8571",
      "number": 8571,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "Update tokenizers-cpp to latest and fix rust build error",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/762",
      "number": 762,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: creating output of dataset generator in current directory",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3018",
      "number": 3018,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][V1] Fix FlashInfer V1 backend using the wrong VllmConfig",
      "url": "https://github.com/vllm-project/vllm/pull/18086",
      "number": 18086,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : refactor model loader with backend registry",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10026",
      "number": 10026,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Metrics] Add bucket for `request_latency`, `time_to_first_token` and `time_per_output_token`",
      "url": "https://github.com/vllm-project/vllm/pull/15202",
      "number": 15202,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Set `uv` UV_PYTHON_INSTALL_DIR explicitly",
      "url": "https://github.com/huggingface/text-generation-inference/pull/3197",
      "number": 3197,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "fix: Handle unsupported message fields in tool calling",
      "url": "https://github.com/vllm-project/vllm/pull/20973",
      "number": 20973,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build] Add shell script linting using shellcheck",
      "url": "https://github.com/vllm-project/vllm/pull/7925",
      "number": 7925,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] fix pre-commit manual running error",
      "url": "https://github.com/vllm-project/vllm/pull/21898",
      "number": 21898,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "convert_checkpoint qwen1.5 error",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1675",
      "number": 1675,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Fix torch nvsmall through pyexecutor and fix its TP support",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3238",
      "number": 3238,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix wrong passing of offload_optimizer_config to DeepSpeedZeRoOffload",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3420",
      "number": 3420,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] use_existing_torch.py: filter out comments",
      "url": "https://github.com/vllm-project/vllm/pull/12255",
      "number": 12255,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix CodeLlama FIM token checks",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8144",
      "number": 8144,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] fix benchmark moe",
      "url": "https://github.com/vllm-project/vllm/pull/14653",
      "number": 14653,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "`seed_everything` doesn't handle HPU",
      "url": "https://github.com/vllm-project/vllm/pull/9281",
      "number": 9281,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] VLLM_DISABLE_COMPILE_CACHE=1 should disable all reads and writes from the cache",
      "url": "https://github.com/vllm-project/vllm/pull/20942",
      "number": 20942,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix [nvbug/5351244]: address remote mpi session submit",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5664",
      "number": 5664,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "server: Ensure batches are either all embed or all completion (#8076)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8420",
      "number": 8420,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Ignore reuse_dist_env",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6623",
      "number": 6623,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Clear engine reference in AsyncEngineRPCServer",
      "url": "https://github.com/vllm-project/vllm/pull/7618",
      "number": 7618,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix RuntimeError when using ZeRO Stage3 with mpu: #3564",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3565",
      "number": 3565,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "clip: don't throw exceptions from llava functions compiled as extern \"C\"",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8210",
      "number": 8210,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[TRTLLM-6785][feat] BREAKING CHANGE Enable TRTLLM sampler by default",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6216",
      "number": 6216,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix nvcc not found in vllm-openai image",
      "url": "https://github.com/vllm-project/vllm/pull/2781",
      "number": 2781,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fixed bug with hybrid engine generation when inference_tp_size > 1",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4493",
      "number": 4493,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Align structures for 64bit, reorder params and ignore error-warn for Clang 19",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11291",
      "number": 11291,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Add a Jinja template to support Mistral3 function calling",
      "url": "https://github.com/vllm-project/vllm/pull/17195",
      "number": 17195,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Remove Vision FA warning",
      "url": "https://github.com/vllm-project/vllm/pull/18522",
      "number": 18522,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix cuda invalid config error in dequant kernel",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2362",
      "number": 2362,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "[CANN]: Fix ggml_backend_cann_buffer_get_tensor",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8871",
      "number": 8871,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1][Bugfix] Fix oracle for device checking",
      "url": "https://github.com/vllm-project/vllm/pull/15104",
      "number": 15104,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "fix : lookup word in vocab before doing BPE merges",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7193",
      "number": 7193,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "feat(audio): add  flag for Whisper chunking (#19772)",
      "url": "https://github.com/vllm-project/vllm/pull/19961",
      "number": 19961,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build][Bugfix] Fix CUTLASS header-only line",
      "url": "https://github.com/vllm-project/vllm/pull/7034",
      "number": 7034,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1][Spec Decode][Bugfix] Load quantize weights for EAGLE",
      "url": "https://github.com/vllm-project/vllm/pull/18290",
      "number": 18290,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix pop off grad mistakenly",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3064",
      "number": 3064,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix torch.compile x LoRA for PyTorch 2.8",
      "url": "https://github.com/vllm-project/vllm/pull/20755",
      "number": 20755,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "requirements: Remove mamba_ssm from test.txt",
      "url": "https://github.com/vllm-project/vllm/pull/20047",
      "number": 20047,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Improve inference documentation",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1421",
      "number": 1421,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Quantization] add BNB for MixtralForCausalLM",
      "url": "https://github.com/vllm-project/vllm/pull/20893",
      "number": 20893,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix color codes emitting mid-UTF8 code.",
      "url": "https://github.com/ggml-org/llama.cpp/pull/312",
      "number": 312,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "cuda : support Falcon-H1 state size for SSM_SCAN",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14602",
      "number": 14602,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Update TensorRT-LLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1233",
      "number": 1233,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ci : fix arm upload artifacts",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12024",
      "number": 12024,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "sycl : Overcoming workaround for mmap() allocation on Windows",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13482",
      "number": 13482,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix NULL dereferences",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3572",
      "number": 3572,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Bump outlines version to 0.1.11 to include ARM support, FIX #11178",
      "url": "https://github.com/vllm-project/vllm/pull/11180",
      "number": 11180,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Update CMakeLists.txt",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11558",
      "number": 11558,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Fix main trtllm",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2341",
      "number": 2341,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Revert \"refactor: Replace DecoderFinishedEvent with CudaEvent in decoder classes\"",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3181",
      "number": 3181,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix: add patch_rope_scaling after hf override",
      "url": "https://github.com/vllm-project/vllm/pull/20857",
      "number": 20857,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix memory_profiling for speculative decoding",
      "url": "https://github.com/vllm-project/vllm/pull/13221",
      "number": 13221,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[OUTDATED] large block_size fix",
      "url": "https://github.com/vllm-project/vllm/pull/20977",
      "number": 20977,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix Metal backend broken from the allocator changes",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2455",
      "number": 2455,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "[build] make builder smarter and configurable wrt compute capabilities + docs",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/578",
      "number": 578,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ci : add fetch-depth to xcframework upload",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12195",
      "number": 12195,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1] Get vocab size from model config",
      "url": "https://github.com/vllm-project/vllm/pull/14904",
      "number": 14904,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Fix completion_stream_generator return two stops (#2266)",
      "url": "https://github.com/vllm-project/vllm/pull/2267",
      "number": 2267,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Add friendlier error message to fopen errors",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8575",
      "number": 8575,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Add explict defualt ctor for RankData to make it can be built with clang",
      "url": "https://github.com/vllm-project/vllm/pull/14268",
      "number": 14268,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : skip token bounds check when evaluating embeddings",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9437",
      "number": 9437,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Updated mul_mat_f16_f32 metal kernel to allow llama-2-70B on metal",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2459",
      "number": 2459,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Remove unnecessary device synchronization for stage 2",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2500",
      "number": 2500,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[torch.compile] expanding support and fix allgather compilation",
      "url": "https://github.com/vllm-project/vllm/pull/9637",
      "number": 9637,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bug] Fix Compressed Tensor NVFP4 `cutlass_fp4_group_mm` illegal memory access",
      "url": "https://github.com/vllm-project/vllm/pull/21465",
      "number": 21465,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "fix(completion): always include usage",
      "url": "https://github.com/vllm-project/vllm/pull/20983",
      "number": 20983,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Cherry pick \"[NVBUG:5355009] Modify check for fuse_fp4_quant on SM120",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5724",
      "number": 5724,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] model_max_length should consider max_model_len in tokenizer_config",
      "url": "https://github.com/vllm-project/vllm/pull/19201",
      "number": 19201,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[refactor] Unify name of NGram speculative decoding",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5937",
      "number": 5937,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1] Fix CI tests on V1 engine",
      "url": "https://github.com/vllm-project/vllm/pull/10272",
      "number": 10272,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Add --enforce_eager to vision_language_multi_image offline example",
      "url": "https://github.com/vllm-project/vllm/pull/20231",
      "number": 20231,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "allocate c strings in metadata functions",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10424",
      "number": 10424,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bug]: pythonic tool parser only accepts alphabetical tool names",
      "url": "https://github.com/vllm-project/vllm/issues/14470",
      "number": 14470,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI] fix dump_input for str type",
      "url": "https://github.com/vllm-project/vllm/pull/18697",
      "number": 18697,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: add bf16 and i32 to getrows",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14529",
      "number": 14529,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix import errors",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2026",
      "number": 2026,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Lei v0.7.3  Fixed Stream set to True, client stream receiving arguments, concatenated json string, missing curly braces end,",
      "url": "https://github.com/vllm-project/vllm/pull/15847",
      "number": 15847,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[fix] Fix DeepSeek w4a8 weight loading",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6498",
      "number": 6498,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "add /dev/shm size to ds_report",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4015",
      "number": 4015,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "enh: Add script to map tests <-> jenkins stages & vice-versa",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5177",
      "number": 5177,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Serving][Fix] Fix problems in PopenServer",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2032",
      "number": 2032,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Vulkan: Fix mmq int dot float cache size",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12722",
      "number": 12722,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Fix] Disable FlashInfer when sliding window is enabled",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/3026",
      "number": 3026,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Server: handle generation until context is filled ",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10128",
      "number": 10128,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "OpenAI Compatible Frontend",
      "url": "https://github.com/vllm-project/vllm/pull/116",
      "number": 116,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix issue with empty grads for non-fused optimizers",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/83",
      "number": 83,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix incorrectly rendered docs on deepspeed.readthedocs.io",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7342",
      "number": 7342,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix opencl build by wrap #if-else-endif with \\n",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2086",
      "number": 2086,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "fix speculative decoding build on windows",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5874",
      "number": 5874,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Modify LRUCache touch",
      "url": "https://github.com/vllm-project/vllm/pull/16689",
      "number": 16689,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "ci: server: verify deps are coherent with the commit",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6409",
      "number": 6409,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix racing condition in GatheredParameters",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3819",
      "number": 3819,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix params bug in diffusion example",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14993",
      "number": 14993,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[nvbug 5273941] fix: broken cyclic reference detect",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5417",
      "number": 5417,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix checkpoint loading",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1167",
      "number": 1167,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "test: [CI] Add failed cases into waives.txt",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5178",
      "number": 5178,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Use host argument to bind to interface",
      "url": "https://github.com/vllm-project/vllm/pull/9798",
      "number": 9798,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: Fix converting EXAONE when using model_weights_loader",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2872",
      "number": 2872,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "ensure that max_gen_len is set properly in mlc_chat_config",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1249",
      "number": 1249,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Use @autoreleasepool to avoid memory leaks",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5437",
      "number": 5437,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Models] Define sharding strategy when combine_matmul=False",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1758",
      "number": 1758,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1][Minor] Do not print attn backend twice",
      "url": "https://github.com/vllm-project/vllm/pull/13985",
      "number": 13985,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Lower gemma's unloaded_params exception to warning",
      "url": "https://github.com/vllm-project/vllm/pull/7002",
      "number": 7002,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix(launcher): fix issue where launcher does not properly report shard failures",
      "url": "https://github.com/huggingface/text-generation-inference/pull/522",
      "number": 522,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Overhaul async request cancellation",
      "url": "https://github.com/vllm-project/vllm/pull/7111",
      "number": 7111,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix]: Fix moe_unpermute compatibility by aligning function signatures under CUDA < 12.0",
      "url": "https://github.com/vllm-project/vllm/pull/18749",
      "number": 18749,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "model : fix build after merge conflict",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14754",
      "number": 14754,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix][CPU] Fix `TorchSDPABackendImpl` doesn't have `use_irope` ",
      "url": "https://github.com/vllm-project/vllm/pull/21200",
      "number": 21200,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Make the Nix-based Docker container work on non-NixOS",
      "url": "https://github.com/huggingface/text-generation-inference/pull/3109",
      "number": 3109,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1][Bugfix] Standardize quantized kv cache rejection for attention backends",
      "url": "https://github.com/vllm-project/vllm/pull/14221",
      "number": 14221,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Fix port lookup in internal DP LB tests",
      "url": "https://github.com/vllm-project/vllm/pull/22252",
      "number": 22252,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "ggml : fix Arm NEON feature detection",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14278",
      "number": 14278,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Fix 4-GPU RLHF tests",
      "url": "https://github.com/vllm-project/vllm/pull/18007",
      "number": 18007,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Move `ParallelConfig` from `config/__init__.py` to `config/parallel.py`",
      "url": "https://github.com/vllm-project/vllm/pull/22565",
      "number": 22565,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Frontend] Add backend-specific options for guided decoding",
      "url": "https://github.com/vllm-project/vllm/pull/13505",
      "number": 13505,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix auto prefix bug",
      "url": "https://github.com/vllm-project/vllm/pull/3239",
      "number": 3239,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: fix MMQ stream-k rounding if ne00 % 128 != 0",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8311",
      "number": 8311,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Mamba2 remove bugged initial state condition in chunk scan",
      "url": "https://github.com/vllm-project/vllm/pull/22034",
      "number": 22034,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "fix build package for 2025.0",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10480",
      "number": 10480,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[VLM][Bugfix] Pass processor kwargs properly on init",
      "url": "https://github.com/vllm-project/vllm/pull/13516",
      "number": 13516,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Move page cache via mbind to prevent cross-NUMA access",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13731",
      "number": 13731,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][v1] Fix step pooler implementation and step pooling usage in v1",
      "url": "https://github.com/vllm-project/vllm/pull/19956",
      "number": 19956,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[VLM] Avoid unnecessary dummy multimodal data during processing",
      "url": "https://github.com/vllm-project/vllm/pull/16416",
      "number": 16416,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Use params when loading models in llava-cli",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3976",
      "number": 3976,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "WAR: Remove CUDA sources and keys to avoid conflicts",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6358",
      "number": 6358,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix OpenAI server sampling w.r.t. temp and seed",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4668",
      "number": 4668,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[KVCache] Remove \"bdx == 32\" assertion",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1729",
      "number": 1729,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[SLM] Fix impl of get_indices for Mixtral",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1618",
      "number": 1618,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[TRTLLM-7008][fix] fix wideEP weights loading and args",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6789",
      "number": 6789,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[https://nvbugspro.nvidia.com/bug/5247148][fix] Attention DP with overlap scheduler",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3975",
      "number": 3975,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "[SYCL] Add nvidia and amd backends index",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6157",
      "number": 6157,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix for stage3 when setting different communication data type",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4540",
      "number": 4540,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Do not override NCCL_CUMEM_ENABLE if set explicitly",
      "url": "https://github.com/vllm-project/vllm/pull/19105",
      "number": 19105,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix the tensor non-contiguous issue for Flashinfer TRT-LLM backend attention kernel",
      "url": "https://github.com/vllm-project/vllm/pull/21133",
      "number": 21133,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Fix idefics default.",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1614",
      "number": 1614,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llava: avoid chaging the original BakLLaVA model",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5577",
      "number": 5577,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "gguf : fix mismatch between alloc and free functions",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6929",
      "number": 6929,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Fix][Serving] Respect sliding window size in config inference",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2364",
      "number": 2364,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "HIP: Ignore unsupported unroll transformation in fattn-vec",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14931",
      "number": 14931,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fixed bug with hybrid engine generation when inference_tp_size > 1",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4493",
      "number": 4493,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix Doc Error: ZeRO Stage 2 gradient partitioning",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6775",
      "number": 6775,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix missing option in visual studio.",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7574",
      "number": 7574,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][ROCm] Use `chunked_prefill_paged_decode` as fallback for V1 attention on ROCm",
      "url": "https://github.com/vllm-project/vllm/pull/18093",
      "number": 18093,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[Deepseek][fix] Fix Deepseek MTP with moe_backend=TRTLLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4001",
      "number": 4001,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Explictly set device when reusing dist env",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6696",
      "number": 6696,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Quick fix to make Pixtral-HF load correctly again after 39e227c7ae.",
      "url": "https://github.com/vllm-project/vllm/pull/11024",
      "number": 11024,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Preventing single user hugging the server to death by asking",
      "url": "https://github.com/huggingface/text-generation-inference/pull/3016",
      "number": 3016,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: address CUDA stream sync issue in ModelRunnerCPP",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6426",
      "number": 6426,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix cuda driver link issue with driver version less than 12.3",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5025",
      "number": 5025,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Avoid graph break by removing another redundant requires grad false",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7263",
      "number": 7263,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[bugfix] Fix static asymmetric quantization case",
      "url": "https://github.com/vllm-project/vllm/pull/10334",
      "number": 10334,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "avoid python list copy in sequence initialization",
      "url": "https://github.com/vllm-project/vllm/pull/401",
      "number": 401,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "ggml : fix arm enabled features check",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10961",
      "number": 10961,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix Ernie4_5_MoeForCausalLM shared experts",
      "url": "https://github.com/vllm-project/vllm/pull/21717",
      "number": 21717,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Sampler] Fix GPU sampler behavior when batch size is 0",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2234",
      "number": 2234,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1][Spec Dec Bug Fix] Respect Spec Dec Method Specification",
      "url": "https://github.com/vllm-project/vllm/pull/16636",
      "number": 16636,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix(server): fix flash neox rotary embeddings",
      "url": "https://github.com/huggingface/text-generation-inference/pull/150",
      "number": 150,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "convert : fix set_vocab_sentencepiece",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6866",
      "number": 6866,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix memory leak in clip.cpp",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6138",
      "number": 6138,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Mark invariant normalizer in Gemma as non-persistent",
      "url": "https://github.com/vllm-project/vllm/pull/19788",
      "number": 19788,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix(launcher): copy current env vars to subprocesses",
      "url": "https://github.com/huggingface/text-generation-inference/pull/70",
      "number": 70,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[usage]:Cuda out of memory while loading the quantized model",
      "url": "https://github.com/vllm-project/vllm/issues/14432",
      "number": 14432,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Remove executable flag on a few files related to flash_attn and flashinfer",
      "url": "https://github.com/vllm-project/vllm/pull/20377",
      "number": 20377,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix `benchmark_throughput.py --backend=hf`",
      "url": "https://github.com/vllm-project/vllm/pull/16352",
      "number": 16352,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Revert PR #11435: Fix issues in CPU build Dockerfile. Fixes #9182",
      "url": "https://github.com/vllm-project/vllm/pull/12135",
      "number": 12135,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[bugfix] [AMD] add multi-step advance_step to ROCmFlashAttentionMetadata",
      "url": "https://github.com/vllm-project/vllm/pull/8474",
      "number": 8474,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama-mmap: fix missing include",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11796",
      "number": 11796,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "unset torch arch list for JIT mode",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1765",
      "number": 1765,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Fix speculative config repr string",
      "url": "https://github.com/vllm-project/vllm/pull/15860",
      "number": 15860,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "convert : fix Norway problem when parsing YAML",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12114",
      "number": 12114,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Kernel] Add head size check for attention backend selection",
      "url": "https://github.com/vllm-project/vllm/pull/4944",
      "number": 4944,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Use `HFCompatibleLoRA` instead of instance method replacement",
      "url": "https://github.com/vllm-project/vllm/pull/14131",
      "number": 14131,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "anchor transformers version",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7316",
      "number": 7316,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Add some improvements for pipeline module, engine and assertion into ds engine",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1529",
      "number": 1529,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[NVIDIA] Support Cutlass w8a8 FP8 for Blackwell Geforce GPUs (sm120)",
      "url": "https://github.com/vllm-project/vllm/pull/17280",
      "number": 17280,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fixes crashes in vLLM v1 engine when using LMCache KV",
      "url": "https://github.com/vllm-project/vllm/pull/19194",
      "number": 19194,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "[SYCL] Fix DMMV dequantization",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9279",
      "number": 9279,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Detect SSSE3",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2825",
      "number": 2825,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "minor: fix a bug when pip install -e . --no-deps --no-build-isolation…",
      "url": "https://github.com/vllm-project/vllm/pull/20472",
      "number": 20472,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI] Fix merge conflict",
      "url": "https://github.com/vllm-project/vllm/pull/9317",
      "number": 9317,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI] don't skip fixed `test_kv_cache_events()`",
      "url": "https://github.com/vllm-project/vllm/pull/18183",
      "number": 18183,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : fix `--reverse-prompt` crashing issue",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14794",
      "number": 14794,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[nvbugs/5368410][fix] Disable moe allreduce for multi node",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5918",
      "number": 5918,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Add health check, make async Engine more robust",
      "url": "https://github.com/vllm-project/vllm/pull/3015",
      "number": 3015,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama-bench : set locale to utf8",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2832",
      "number": 2832,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Performance][Core] Optimize the performance of evictor v1 and v2 by applying a priority queue and lazy deletion",
      "url": "https://github.com/vllm-project/vllm/pull/7209",
      "number": 7209,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "server : update xxd usage for older versions compatibility",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2649",
      "number": 2649,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix pipline dataloader when batch elements contain tuple",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/565",
      "number": 565,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix OpenCL kernels for the new formats",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1422",
      "number": 1422,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] fix define of RerankDocument",
      "url": "https://github.com/vllm-project/vllm/pull/20877",
      "number": 20877,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Frontend] Fix Issues Under High Load With `zeromq` Frontend",
      "url": "https://github.com/vllm-project/vllm/pull/7394",
      "number": 7394,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix[nvbug-5295425]: [TRTLLM-5385] fix race condition in MoeLoadBalancer",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4573",
      "number": 4573,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Model] Pad BatchDecode input for e4m3 fp8 models",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2931",
      "number": 2931,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fixed cutlass preprocessors typo",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/859",
      "number": 859,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[None][feat] Use Separate QKV Input Layout for Context MLA",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6538",
      "number": 6538,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Update TensorRT-LLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2849",
      "number": 2849,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "supports running on CPU for GGML_USE_CUBLAS=ON build",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3946",
      "number": 3946,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] [ROCm]: Bugfix and handle addition case of input for `rocm_aiter_rms_norm`",
      "url": "https://github.com/vllm-project/vllm/pull/17857",
      "number": 17857,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Attn][Adreno][OpenCL] Fix workgroup size",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1850",
      "number": 1850,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Frontend] Dynamic RoPE scaling",
      "url": "https://github.com/vllm-project/vllm/pull/4638",
      "number": 4638,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Update TensorRT-LLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1492",
      "number": 1492,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix ndarray video color from VideoAsset",
      "url": "https://github.com/vllm-project/vllm/pull/21064",
      "number": 21064,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix a couple PPLX+CUTLASS MoE bugs",
      "url": "https://github.com/vllm-project/vllm/pull/20825",
      "number": 20825,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1] Fix vocab size calculation for structured output",
      "url": "https://github.com/vllm-project/vllm/pull/14826",
      "number": 14826,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml : allocate graphs in a context",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2392",
      "number": 2392,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Empty ZeRO3 partition cache",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3060",
      "number": 3060,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "fix neuron config override",
      "url": "https://github.com/vllm-project/vllm/pull/16045",
      "number": 16045,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix optimizer saving with BF16 + PP + ZeRO-1",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3555",
      "number": 3555,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Update GitHub workflows to work with PyTorch 2.0.0, apply fixes or pin to previous versions.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3039",
      "number": 3039,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[Infra] - Check all steps for test name and also check the test in waives.txt also exists in l0 or qa test list.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6256",
      "number": 6256,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Transformer/fix layer norm",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1404",
      "number": 1404,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Guard for negative counter metrics to prevent crash",
      "url": "https://github.com/vllm-project/vllm/pull/10430",
      "number": 10430,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Fix Server",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1157",
      "number": 1157,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "clip : fix model size display",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13153",
      "number": 13153,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : fix FA when KV cache is not used (i.e. embeddings)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12825",
      "number": 12825,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Frontend] respect provided default guided decoding backend",
      "url": "https://github.com/vllm-project/vllm/pull/15476",
      "number": 15476,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1] Get vocab size from model config",
      "url": "https://github.com/vllm-project/vllm/pull/14904",
      "number": 14904,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Show error message when -f fails",
      "url": "https://github.com/ggml-org/llama.cpp/pull/656",
      "number": 656,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1] Support disable_any_whtespace for guidance backend",
      "url": "https://github.com/vllm-project/vllm/pull/15584",
      "number": 15584,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "cmake: fix issue with library path during cross-compile",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10383",
      "number": 10383,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix OpenVino/Neuron `driver_worker` init",
      "url": "https://github.com/vllm-project/vllm/pull/10779",
      "number": 10779,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] fix top_k: 0 in generation_config.json can't disable top-k sampling",
      "url": "https://github.com/vllm-project/vllm/pull/17691",
      "number": 17691,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama-server : Improve messages bubble shape in RTL",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11220",
      "number": 11220,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bug]: Authorization ignored when root_path is set",
      "url": "https://github.com/vllm-project/vllm/pull/10606",
      "number": 10606,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml-cpu : bug fix related to KleidiAI multithreaded LHS packing",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12568",
      "number": 12568,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Data parallel example will all use same GPUs if the users script initializes torch.cuda",
      "url": "https://github.com/vllm-project/vllm/pull/14598",
      "number": 14598,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Update sharded_moe.py to support top2 gate with Tutel",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6948",
      "number": 6948,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Cherry-pick feat/llama4's changes",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4746",
      "number": 4746,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Fix test_sharded_state_loader.py(#16004)",
      "url": "https://github.com/vllm-project/vllm/pull/16005",
      "number": 16005,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[TRTLLM-7141][infra] Use repo mirrors to avoid intermittent network failures",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6836",
      "number": 6836,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Update TensorRT-LLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1530",
      "number": 1530,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Model][Bugfix] Implicit model flags and reenable Phi-3-Vision",
      "url": "https://github.com/vllm-project/vllm/pull/5896",
      "number": 5896,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ci: server: fix python installation",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6925",
      "number": 6925,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "nix: make `xcrun` visible in Nix sandbox for precompiling Metal shaders",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6118",
      "number": 6118,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] enable prefix caching for AsyncLLMEngine when requesting prompt_logprobs",
      "url": "https://github.com/vllm-project/vllm/pull/6456",
      "number": 6456,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Support jinja extra template kwargs (Qwen3 enable_thinking feature), from command line and from client",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13196",
      "number": 13196,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI] remove flaky v0 test",
      "url": "https://github.com/vllm-project/vllm/pull/22864",
      "number": 22864,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "pipe/_exec_backward_pass: fix immediate grad update",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5605",
      "number": 5605,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[New Model] Support `StableLMAlphaForCausalLM`",
      "url": "https://github.com/vllm-project/vllm/pull/20036",
      "number": 20036,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Issues while enabling MMA support on AIX machines",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12241",
      "number": 12241,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Allow using OpenCV as video IO fallback",
      "url": "https://github.com/vllm-project/vllm/pull/15055",
      "number": 15055,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "tests: fix 5273697",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4685",
      "number": 4685,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix Phi-3 BNB quantization with tensor parallel",
      "url": "https://github.com/vllm-project/vllm/pull/9948",
      "number": 9948,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "nix: fix blas support",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6281",
      "number": 6281,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "fix: don't add space after special tokens in SPM",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7697",
      "number": 7697,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Core] Flashinfer - Remove advance step size restriction",
      "url": "https://github.com/vllm-project/vllm/pull/10282",
      "number": 10282,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: Disaggregate serving with attention DP",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4993",
      "number": 4993,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build] Only build `per_token_group_quant.cu` on CUDA",
      "url": "https://github.com/vllm-project/vllm/pull/21392",
      "number": 21392,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Import ABC from collections.abc for Python 3.10 compatibility.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1851",
      "number": 1851,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Use the correct torch dtype in topk kernel assertion",
      "url": "https://github.com/vllm-project/vllm/pull/19614",
      "number": 19614,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] fix benchmark moe",
      "url": "https://github.com/vllm-project/vllm/pull/14653",
      "number": 14653,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc][Platform] Move use allgather to platform",
      "url": "https://github.com/vllm-project/vllm/pull/14010",
      "number": 14010,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Server enhancements - grammar segfault and helper titles.",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5080",
      "number": 5080,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Add an option to launch cacheflow without ray",
      "url": "https://github.com/vllm-project/vllm/pull/51",
      "number": 51,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CMake: correct order of sycl flags",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9497",
      "number": 9497,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "model : fix hunyuan moe chat template",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14584",
      "number": 14584,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Disable autofunc V2 in PyTorch 2.6+ ",
      "url": "https://github.com/vllm-project/vllm/pull/14704",
      "number": 14704,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "vulkan: fix NaN issue in flash attention shader",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12776",
      "number": 12776,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "gguf-py : fix upload python package workflow",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13020",
      "number": 13020,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Only convert output to weakref for last graph across all compilation units",
      "url": "https://github.com/vllm-project/vllm/pull/22282",
      "number": 22282,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "vulkan: Catch pipeline creation failure and print an error message",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11436",
      "number": 11436,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Rwkv chat template fix",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10001",
      "number": 10001,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix performance when `--generation-config` is not `None`",
      "url": "https://github.com/vllm-project/vllm/pull/14223",
      "number": 14223,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server : fix divide-by-zero in metrics reporting",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11915",
      "number": 11915,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[fix] Correct handling of NVFP4 block scaling factors in preprocessing for MoE",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6069",
      "number": 6069,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "cvector: fix CI + correct help message",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8064",
      "number": 8064,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix Windows build LNK1181 errors for aio.lib and cufile.lib",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7341",
      "number": 7341,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Micro cleanup.",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2555",
      "number": 2555,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[linting] Enable ruff on more files (wave 2/N)",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6162",
      "number": 6162,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "one more CMAKE_CXX_FLAGS fix",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9471",
      "number": 9471,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] fix client socket timeout when serve multi-node model in Ray",
      "url": "https://github.com/vllm-project/vllm/pull/15850",
      "number": 15850,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix issues raised by Coverity scans",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7431",
      "number": 7431,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "cmake : fix use of external ggml",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8787",
      "number": 8787,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Update tpu_worker.py: Fixing Variable Name Spelling Error.",
      "url": "https://github.com/vllm-project/vllm/pull/17296",
      "number": 17296,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][Model] Add SupportsQuant interface to Mixtral",
      "url": "https://github.com/vllm-project/vllm/pull/15552",
      "number": 15552,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix misleading information in the documentation",
      "url": "https://github.com/vllm-project/vllm/pull/18845",
      "number": 18845,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Supporting different hidden dimensions for transformer kernels",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/934",
      "number": 934,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI/Build] Add support for Python 3.13",
      "url": "https://github.com/vllm-project/vllm/pull/13164",
      "number": 13164,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: fix non-cont. inputs for batched mat mul",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13155",
      "number": 13155,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Allow lists",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3042",
      "number": 3042,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Fix flashinfer plan call to use positional arguments for #3165",
      "url": "https://github.com/huggingface/text-generation-inference/pull/3166",
      "number": 3166,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Fix] correct tool_id for kimi-k2 when use tool_choice=required",
      "url": "https://github.com/vllm-project/vllm/pull/21259",
      "number": 21259,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix : paralle build example error",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/923",
      "number": 923,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix output transpose dimension bugs",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3747",
      "number": 3747,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Update ggml_sycl_op_mul_mat_vec_q",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5502",
      "number": 5502,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1][Usage] Refactor speculative decoding configuration and tests",
      "url": "https://github.com/vllm-project/vllm/pull/14434",
      "number": 14434,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Update flops profiler to recurse",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4374",
      "number": 4374,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix `bpe_gpt2_preprocess`",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5446",
      "number": 5446,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: use 1 thread if model is fully offloaded",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2915",
      "number": 2915,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Update ROCM libs and improvements ",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2579",
      "number": 2579,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: Allow context_and_generation request type in disagg overlap",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3489",
      "number": 3489,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml : skip register metal backend on os simulator",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10132",
      "number": 10132,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[XPU]Fix `flash_attn_varlen_func` interface on xpu",
      "url": "https://github.com/vllm-project/vllm/pull/22350",
      "number": 22350,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Windows: reactivate sigint handler after each Ctrl-C",
      "url": "https://github.com/ggml-org/llama.cpp/pull/736",
      "number": 736,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "gguf-py : add Numpy MXFP4 de/quantization support",
      "url": "https://github.com/ggml-org/llama.cpp/pull/15111",
      "number": 15111,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Fix Granite model configuration",
      "url": "https://github.com/vllm-project/vllm/pull/8216",
      "number": 8216,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "OpenCL: fix profiling crash in llama-bench",
      "url": "https://github.com/ggml-org/llama.cpp/pull/15072",
      "number": 15072,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "fix: CUDA error when inferencing with Falcon-40B base model",
      "url": "https://github.com/vllm-project/vllm/pull/992",
      "number": 992,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc][LoRA] Fix LoRA weight mapper",
      "url": "https://github.com/vllm-project/vllm/pull/11495",
      "number": 11495,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "[SYCL] Fix cpy with dims of 3",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5289",
      "number": 5289,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Update sampling_metadata.py",
      "url": "https://github.com/vllm-project/vllm/pull/21937",
      "number": 21937,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: fix broken oob check for FA vec f32 kernel",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7904",
      "number": 7904,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml : fix soft max out-of-bounds access",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4307",
      "number": 4307,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Improve multimodal hasher performance for re-used Image prompts",
      "url": "https://github.com/vllm-project/vllm/pull/22825",
      "number": 22825,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "main: use jinja chat template system prompt by default",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12118",
      "number": 12118,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "force int cast",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1170",
      "number": 1170,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "updates website dependencies",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/475",
      "number": 475,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Use string_view::find() to search for tokenization to speed up",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12706",
      "number": 12706,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "webui: Fix Shift+Enter handling",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11609",
      "number": 11609,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix model name mapping (#2)",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2644",
      "number": 2644,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][V1] Fix flashinfer sampling",
      "url": "https://github.com/vllm-project/vllm/pull/14815",
      "number": 14815,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1] Change return type on get_multimodal_embeddings()",
      "url": "https://github.com/vllm-project/vllm/pull/19446",
      "number": 19446,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[NVBUG 5247699]Fix mixtral fp4 llmapi bug.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3929",
      "number": 3929,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1] Detokenizer: Respect Stop Tokens + `not include_stop_str_in_output`",
      "url": "https://github.com/vllm-project/vllm/pull/14624",
      "number": 14624,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix(dockerfile): fix docker build",
      "url": "https://github.com/huggingface/text-generation-inference/pull/32",
      "number": 32,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[None][fix] Fix perfect router.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6797",
      "number": 6797,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Hotfix: fix of use of unquantized weights in Gemma GQA loading",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2255",
      "number": 2255,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Fix duplicate FusedMoEConfig debug messages",
      "url": "https://github.com/vllm-project/vllm/pull/21455",
      "number": 21455,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "fix broken link in pr template",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7880",
      "number": 7880,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix for when prompt contains an odd num of apostrophes",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4660",
      "number": 4660,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Fix] Fix typo in `resolve_hf_chat_template`",
      "url": "https://github.com/vllm-project/vllm/pull/18259",
      "number": 18259,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Upstream Llama4 Support to Main",
      "url": "https://github.com/vllm-project/vllm/pull/16113",
      "number": 16113,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] fix 3 issues: (1) using metadata for causal-conv1d, (2) indexing overflow in v1 vLLM, and (3) init_states in v0",
      "url": "https://github.com/vllm-project/vllm/pull/20838",
      "number": 20838,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "[CANN] Improve the Inferencing Performance for Ascend NPU Device",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10454",
      "number": 10454,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix using torch.half with transformers",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4670",
      "number": 4670,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix run-time on FreeBSD in get_executable_path()",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10948",
      "number": 10948,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "ZeRO-2: Handle gradients of empty partitions",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/275",
      "number": 275,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix: correctly index into mask when applying grammar",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1618",
      "number": 1618,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] fix KeyError on top logprobs are special tokens",
      "url": "https://github.com/vllm-project/vllm/pull/17637",
      "number": 17637,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "build: add /bigobj to MSVC build",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11407",
      "number": 11407,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[FIXBUG] Correctly Apply Grammar Bitmask in Mixed Batches",
      "url": "https://github.com/vllm-project/vllm/pull/22896",
      "number": 22896,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix hpZ with zero element",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5652",
      "number": 5652,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Return model_id to /generate response",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1219",
      "number": 1219,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix: append DONE message to chat stream",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2221",
      "number": 2221,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix regional compilation",
      "url": "https://github.com/vllm-project/vllm/pull/15882",
      "number": 15882,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Fixing the download strategy for ibm-fms",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1917",
      "number": 1917,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Add JSON format logging support with `loguru`",
      "url": "https://github.com/vllm-project/vllm/pull/13920",
      "number": 13920,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix step in adam",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1823",
      "number": 1823,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[zero] prevent poor configs from running w. zero-offload",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2971",
      "number": 2971,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Use rtol to suppress spurious failure on AMD",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1387",
      "number": 1387,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix unbound local error for `return_val`",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7395",
      "number": 7395,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI][PowerPC] Use a more appropriate way to select testcase in tests/models/language/pooling/test_embedding.py",
      "url": "https://github.com/vllm-project/vllm/pull/19253",
      "number": 19253,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Fix] Use a correct device when creating OptionalCUDAGuard",
      "url": "https://github.com/vllm-project/vllm/pull/2583",
      "number": 2583,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Vulkan-run-test: fix mmq_wg_denoms",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11343",
      "number": 11343,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] validate guided decoding grammar for grammar backend",
      "url": "https://github.com/vllm-project/vllm/pull/17621",
      "number": 17621,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "deepspeed/comm/comm.py: fix typo of warning message",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3636",
      "number": 3636,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix error when tp > 1",
      "url": "https://github.com/vllm-project/vllm/pull/2644",
      "number": 2644,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Fix a tokenizer issue in decoding generated tokens",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1024",
      "number": 1024,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix echo/logprob OpenAI completion bug",
      "url": "https://github.com/vllm-project/vllm/pull/3441",
      "number": 3441,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "Fix the max window length fetching in local mode",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1016",
      "number": 1016,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "add missing kv clear in llama_beam_search",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6664",
      "number": 6664,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "vulkan : add GGML_VK_FORCE_HEAP_INDEX env var",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9734",
      "number": 9734,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Model] Fix Ernie4.5MoE e_score_correction_bias parameter",
      "url": "https://github.com/vllm-project/vllm/pull/21586",
      "number": 21586,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix]: ensure all exceptions are properly catched in openai frontend",
      "url": "https://github.com/vllm-project/vllm/pull/13039",
      "number": 13039,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix result validation in benchmark-q4_0-matmult",
      "url": "https://github.com/ggml-org/llama.cpp/pull/987",
      "number": 987,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Make the _apply_rotary_emb compatible with dynamo",
      "url": "https://github.com/vllm-project/vllm/pull/17435",
      "number": 17435,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[build] relax wheel size limit",
      "url": "https://github.com/vllm-project/vllm/pull/6704",
      "number": 6704,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Docs] Fix Unmocked Imports",
      "url": "https://github.com/vllm-project/vllm/pull/3275",
      "number": 3275,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Update common.py If has_comtext becomes a tensor, it will introduce s…",
      "url": "https://github.com/vllm-project/vllm/pull/19178",
      "number": 19178,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][CPU] Fix InputBatch for pooling models in the CPU v1",
      "url": "https://github.com/vllm-project/vllm/pull/20014",
      "number": 20014,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml : refactor metal library loading to avoid GGMLMetalClass ODR",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12200",
      "number": 12200,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "rollback #6726",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7258",
      "number": 7258,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Upgrading exl2.",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2415",
      "number": 2415,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "common: Include torch package for s390x",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13699",
      "number": 13699,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "radix trie: add assertions",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2491",
      "number": 2491,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[WAR][nvbug/5321947] Add an async sleep to unblock event loop.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5342",
      "number": 5342,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: check for config architectures in model config",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3586",
      "number": 3586,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Fix nightly MLA failure (FA2 + MLA chunked prefill, i.e. V1, producing bad results)",
      "url": "https://github.com/vllm-project/vllm/pull/15492",
      "number": 15492,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : fix non-quantization of expert gating tensors",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5754",
      "number": 5754,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: Fallback to NCCL for various patterns when input size is large.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4009",
      "number": 4009,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Early return for zero size calls to get_tensor.",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5482",
      "number": 5482,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] fix: close issue #16554 to make it real async",
      "url": "https://github.com/vllm-project/vllm/pull/16557",
      "number": 16557,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix failing check",
      "url": "https://github.com/ggml-org/llama.cpp/pull/944",
      "number": 944,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "set the default to use set_to_none for clearing gradients in BF16 optimizer.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5434",
      "number": 5434,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "S3 Tokenizer from model param",
      "url": "https://github.com/vllm-project/vllm/pull/18071",
      "number": 18071,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[ROCm][Bug] Remove a bad fused_moe triton config",
      "url": "https://github.com/vllm-project/vllm/pull/15835",
      "number": 15835,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Model] Modify the gate implementation of glm4_moe",
      "url": "https://github.com/vllm-project/vllm/pull/22832",
      "number": 22832,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "parallel : fix n_junk == 0",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13952",
      "number": 13952,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Kernel] Prevent integer overflow in fp8 dynamic per-token quantize kernel",
      "url": "https://github.com/vllm-project/vllm/pull/9425",
      "number": 9425,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix JSON Schema to Grammar for string regexp with top-level alternation.",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9903",
      "number": 9903,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "`sync`: minja (support QwQ-32B)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12235",
      "number": 12235,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Hotfix][VLM] Fixing max position embeddings for Pixtral",
      "url": "https://github.com/vllm-project/vllm/pull/8399",
      "number": 8399,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Add use_irope to flashinfer v1 backend",
      "url": "https://github.com/vllm-project/vllm/pull/19511",
      "number": 19511,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Accelerator Abstraction: Fix skipped tests:",
      "url": "https://github.com/deepspeedai/DeepSpeed/issues/5280",
      "number": 5280,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix overflows in elu function",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8866",
      "number": 8866,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix server crashes",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2076",
      "number": 2076,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix for LLAMA_WIN_VER default value, fixes #5158",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5298",
      "number": 5298,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : skip loading unused tensors",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12004",
      "number": 12004,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Update compatible packaging version",
      "url": "https://github.com/vllm-project/vllm/pull/19279",
      "number": 19279,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix tensor groups for encoder-decoder models in gguf-dump.py",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8090",
      "number": 8090,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Fix] MLC-LLM website link weight convert not accessible",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1877",
      "number": 1877,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Core] v1 Filter out already scheduled requests during preemptive scheduling.",
      "url": "https://github.com/vllm-project/vllm/pull/21162",
      "number": 21162,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Relax DeepSpeed MoE ZeRO-1 Assertion",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2007",
      "number": 2007,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : fix progress dots",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11730",
      "number": 11730,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "zero-3 dynamic tracing support",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/873",
      "number": 873,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Added support for BUILD_VERSION env variable. Allow one to set build …",
      "url": "https://github.com/vllm-project/vllm/pull/1923",
      "number": 1923,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Vulkan Bugfixes and Improvements",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7084",
      "number": 7084,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[None][feat] CUTLASS MoE FC2+Finalize fusion",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3294",
      "number": 3294,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix duplicated import module",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2182",
      "number": 2182,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix typo in llama.h",
      "url": "https://github.com/ggml-org/llama.cpp/pull/593",
      "number": 593,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Fix chat input string in xgrammar format",
      "url": "https://github.com/vllm-project/vllm/pull/16239",
      "number": 16239,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix spelling error with  docs/index.md",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3443",
      "number": 3443,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[fix] Fix add_dummy_requests for spec decoding cases",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4084",
      "number": 4084,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Modify LRUCache touch",
      "url": "https://github.com/vllm-project/vllm/pull/16689",
      "number": 16689,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1] [Spec Decode] Fix ngram tests",
      "url": "https://github.com/vllm-project/vllm/pull/14878",
      "number": 14878,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Switch to multimap based nfd_map due to compile time issues",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5799",
      "number": 5799,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix f-string messages",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4865",
      "number": 4865,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "Fix typo in token_postproc_method names",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2261",
      "number": 2261,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Align optimizer swap buffer sizes",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1036",
      "number": 1036,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "CVE-2007-4559 Patch",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2605",
      "number": 2605,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Frontend] Cleanup \"fix chat logprobs\"",
      "url": "https://github.com/vllm-project/vllm/pull/5026",
      "number": 5026,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "tts : fix n_ubatch + make WavTokenizer cache-less",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13713",
      "number": 13713,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix attention mask handling in the Hybrid Engine Bloom flow",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5101",
      "number": 5101,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix: avoid setting use_sgmv if no kernels present",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2796",
      "number": 2796,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Mark 'hidden_states' as mutable in moe_forward registration.",
      "url": "https://github.com/vllm-project/vllm/pull/20152",
      "number": 20152,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI] Fix race condition in test_kv_cache_events test",
      "url": "https://github.com/vllm-project/vllm/pull/18169",
      "number": 18169,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix GLM4 model",
      "url": "https://github.com/vllm-project/vllm/pull/16618",
      "number": 16618,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Molmo text-only input bug fix",
      "url": "https://github.com/vllm-project/vllm/pull/9397",
      "number": 9397,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix doubling of BOS token when BOS already exists (e.g. via chat template)",
      "url": "https://github.com/vllm-project/vllm/pull/16976",
      "number": 16976,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "TensorRT-LLM backend bump to latest version + misc fixes",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2791",
      "number": 2791,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix mics run with offload++",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4749",
      "number": 4749,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix Lora Name Parsing",
      "url": "https://github.com/vllm-project/vllm/pull/17196",
      "number": 17196,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[NVIDIA] Support Cutlass w8a8 FP8 for Blackwell Geforce GPUs (sm120)",
      "url": "https://github.com/vllm-project/vllm/pull/17280",
      "number": 17280,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix Mistral3 support on SM100/SM120",
      "url": "https://github.com/vllm-project/vllm/pull/20998",
      "number": 20998,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: Updates to yarn implementation",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5105",
      "number": 5105,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Documented CUDA reproducibility, added warning",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1346",
      "number": 1346,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Inference changes for incorporating meta loading checkpoint",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4692",
      "number": 4692,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Fix logits repetition penalty cuda check",
      "url": "https://github.com/vllm-project/vllm/pull/22592",
      "number": 22592,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "CacheState fix for disabled DP",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5131",
      "number": 5131,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Limit concurrent long partial prefills via max_long_partial_prefills",
      "url": "https://github.com/vllm-project/vllm/pull/21651",
      "number": 21651,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : fix detokenization of non-special added-tokens",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4916",
      "number": 4916,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: [nvbug/5251968] Fix NVLink version decoding.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3996",
      "number": 3996,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Fix multi-node offline data parallel",
      "url": "https://github.com/vllm-project/vllm/pull/19937",
      "number": 19937,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[Model load] Fix llama min-latency model load",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5883",
      "number": 5883,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "[see_memory_usage] fix deprecation",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1234",
      "number": 1234,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix error message on `TORCH_CUDA_ARCH_LIST`",
      "url": "https://github.com/vllm-project/vllm/pull/1239",
      "number": 1239,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix num_kv_heads sharding in autoTP for the new in-repo Falcon-40B",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4654",
      "number": 4654,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Fix DP Coordinator incorrect debug log message",
      "url": "https://github.com/vllm-project/vllm/pull/19624",
      "number": 19624,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Minor Dockerfile update",
      "url": "https://github.com/huggingface/text-generation-inference/pull/261",
      "number": 261,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "chore: Raise error for PP + MTP",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3244",
      "number": 3244,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix crashing for multimodal when image passed with height == 1",
      "url": "https://github.com/vllm-project/vllm/pull/9141",
      "number": 9141,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "hotfix: Fix number of KV heads",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2202",
      "number": 2202,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "fix no file in win rel for sycl",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6314",
      "number": 6314,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CUDA: mul_mat_vec_q tiling, refactor mul mat logic",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5434",
      "number": 5434,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Bump nokogiri from 1.11.4 to 1.12.5 in /docs",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1409",
      "number": 1409,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "musa: extract ggml_cuda_mul_mat_batched_cublas_gemm_batched_ex",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13887",
      "number": 13887,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "bench: server add stop word for PHI-2",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6916",
      "number": 6916,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CUDA: mul_mat_vec_q max. batch size 8 -> 4",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5370",
      "number": 5370,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[nvbugs/5274894] fix: Sort requests for functional correctness and performance",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4608",
      "number": 4608,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix:update the default excluded_modules value for fp8rowwise recipe.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3477",
      "number": 3477,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Lazy import fused_experts in BitsAndBytesMoEMethod to avoid break not-cuda-alike devices ",
      "url": "https://github.com/vllm-project/vllm/pull/20822",
      "number": 20822,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Only use FIM middle token if it exists",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7648",
      "number": 7648,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[None][fix] Refactoring input prep to allow out-of-tree models",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6497",
      "number": 6497,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: Update log query regex in perf integration test to match trtllm-bench reporting",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4070",
      "number": 4070,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Server: use llama_chat_apply_template",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5593",
      "number": 5593,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Hardware] correct method signatures for HPU,ROCm,XPU",
      "url": "https://github.com/vllm-project/vllm/pull/18551",
      "number": 18551,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "fix: check model pointer validity before use",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13631",
      "number": 13631,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "make : do not add linker flags when compiling static llava lib",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3977",
      "number": 3977,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix weight loading for some models in Transformers backend",
      "url": "https://github.com/vllm-project/vllm/pull/15544",
      "number": 15544,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Update TensorRT-LLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2333",
      "number": 2333,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "examples : allow extracting embeddings from decoder contexts",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13797",
      "number": 13797,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix nightly transformers CI failure",
      "url": "https://github.com/vllm-project/vllm/pull/21427",
      "number": 21427,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix Mistral guided generation using xgrammar",
      "url": "https://github.com/vllm-project/vllm/pull/15704",
      "number": 15704,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Update Florence-2 tokenizer to make grounding tasks work",
      "url": "https://github.com/vllm-project/vllm/pull/16734",
      "number": 16734,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server : handle speculative decoding llama_decode failures",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10628",
      "number": 10628,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Multinode hang fix with PP",
      "url": "https://github.com/vllm-project/vllm/pull/6399",
      "number": 6399,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "metal : fix flash attention kernel requirements",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7169",
      "number": 7169,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Handle concurrent grammar requests",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1610",
      "number": 1610,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Change methods to be static",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1038",
      "number": 1038,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[None][fix] Remove lock related typo in py_executor",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6319",
      "number": 6319,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][CPU] Fix InputBatch for pooling models in the CPU v1",
      "url": "https://github.com/vllm-project/vllm/pull/20014",
      "number": 20014,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server : fix draft context not being released",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11354",
      "number": 11354,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "Fix broken link to Python API",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2168",
      "number": 2168,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: check if event is NULL before cudaStreamWaitEvent",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2505",
      "number": 2505,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "musa: fix all warnings, re-enable `-DLLAMA_FATAL_WARNINGS=ON` in ci and update doc",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12611",
      "number": 12611,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "model : do not repack if a GPU device is present",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12498",
      "number": 12498,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: add support for passing calib sequence length, and num samples + fixing use of custom calibration dataset for smoothquant in llama",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2243",
      "number": 2243,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Remove unneeded dict reinit (fix for #4565)",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4702",
      "number": 4702,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fixing a config mismatch in unit test.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2447",
      "number": 2447,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Easy] Eliminate c10::optional usage in vllm/csrc",
      "url": "https://github.com/vllm-project/vllm/pull/17819",
      "number": 17819,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: set allreduce strategy to model config",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5955",
      "number": 5955,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Support `nix build '.#opencl'`",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2337",
      "number": 2337,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "Fix DeviceAPI include error in `logit_processor.cc`",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2486",
      "number": 2486,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[TRTLLM-6771][feat] Support MMMU for multimodal models",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6828",
      "number": 6828,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Limit max_num_batched_tokens by max_num_seqs * max_model_len",
      "url": "https://github.com/vllm-project/vllm/pull/15062",
      "number": 15062,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Checkpointing: Avoid assigning tensor storage with different device",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4836",
      "number": 4836,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": " [Bugfix]: Fix Possible Output Corruption in Cascade Attention Caused by Non-Contiguous LSE Tensor",
      "url": "https://github.com/vllm-project/vllm/pull/22003",
      "number": 22003,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "server: apply grammar before other samplers",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12643",
      "number": 12643,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "anchor transformers version",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7316",
      "number": 7316,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build][Bugfix] Fix CPU CI image clean up",
      "url": "https://github.com/vllm-project/vllm/pull/11836",
      "number": 11836,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix f16_sycl cpy call from Arc",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5411",
      "number": 5411,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix MTP weight loading ",
      "url": "https://github.com/vllm-project/vllm/pull/21941",
      "number": 21941,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "metal : pad n_ctx by 32",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6177",
      "number": 6177,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix glm4.1v video_grid_thw tensor shape scheme",
      "url": "https://github.com/vllm-project/vllm/pull/21744",
      "number": 21744,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[None][fix] fix logprob in trtllm-serve",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6845",
      "number": 6845,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Frontend] Fix typo in tool chat templates for llama3.2 and toolace",
      "url": "https://github.com/vllm-project/vllm/pull/14501",
      "number": 14501,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Model] Fix config detection for Mistral",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2504",
      "number": 2504,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "Fix windows and macOS build",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1609",
      "number": 1609,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Correct per_act_token in CompressedTensorsW8A8Fp8MoECutlassM…",
      "url": "https://github.com/vllm-project/vllm/pull/20937",
      "number": 20937,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[zero] faster flatten/unflatten (cpp version) ",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/910",
      "number": 910,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix cuda_archs_loose_intersection when handling sm_*a",
      "url": "https://github.com/vllm-project/vllm/pull/20207",
      "number": 20207,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Fix incorrect cache allocation with multi-query",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2203",
      "number": 2203,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[None][feat] Add GPT OSS support for AutoDeploy",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6641",
      "number": 6641,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix import error of op_builder",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2687",
      "number": 2687,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix latency benchmark script",
      "url": "https://github.com/vllm-project/vllm/pull/118",
      "number": 118,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1] Correct V1 parallel sampling params",
      "url": "https://github.com/vllm-project/vllm/pull/20580",
      "number": 20580,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix: set MODEL_ID in sagemaker-entrypoint script",
      "url": "https://github.com/huggingface/text-generation-inference/pull/343",
      "number": 343,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Doc][Bugfix] Add missing EOF in k8s deploy doc",
      "url": "https://github.com/vllm-project/vllm/pull/16025",
      "number": 16025,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix: Add V100 (older) GPU Support for Mistral 7b Models",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1279",
      "number": 1279,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Skip test_bias_gelu unit test if torch < 1.12",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2754",
      "number": 2754,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Attention] Fix attn kernel for general GQA group size",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2543",
      "number": 2543,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix Hermes tool call parser with streaming",
      "url": "https://github.com/vllm-project/vllm/pull/18220",
      "number": 18220,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix CI build time increase",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5337",
      "number": 5337,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix WorkerWrapperBase initialization: defer vllm_config setup",
      "url": "https://github.com/vllm-project/vllm/pull/14179",
      "number": 14179,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Fix GPTQ for models which do not have float16 at the default dtype (simpler)",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1953",
      "number": 1953,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[https://nvbugs/5415862][fix] Update cublas as 12.9.1 and cuda memory alignment as 256",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6501",
      "number": 6501,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Core] [Bugfix] Add Input Embeddings",
      "url": "https://github.com/vllm-project/vllm/pull/15428",
      "number": 15428,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Copy grads to cpu in z1-offload",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1679",
      "number": 1679,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Add compute capability 8.9 to default targets",
      "url": "https://github.com/vllm-project/vllm/pull/829",
      "number": 829,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CI Fix: Removes multiple newlines at the end of files that breaks `editorconfig` ",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8258",
      "number": 8258,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Fix][Serving] Respect sliding window size in config inference",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2364",
      "number": 2364,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Avoid transferring cached multi-modal items from P0 to P1",
      "url": "https://github.com/vllm-project/vllm/pull/16273",
      "number": 16273,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Config] Fix `max_batch_size` in dumped config",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1652",
      "number": 1652,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix ffn_down quantization mix for MoE models",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4927",
      "number": 4927,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fixing several bugs in the inference-api and the kernels",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1951",
      "number": 1951,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama-tts : avoid crashes related to bad model file paths",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12482",
      "number": 12482,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "fix: ci-test fail on generate IPv4 address with regex",
      "url": "https://github.com/vllm-project/vllm/pull/3368",
      "number": 3368,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[nvbug/5374773] chore: Add a runtime flag to enable fail fast when attn window is too large to fit at least one sequence in KV cache",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5974",
      "number": 5974,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: Mistral Small vision encoder with BS>1",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4712",
      "number": 4712,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "[deepspeed/autotuner] Bug fix for skipping mbs on gas",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2171",
      "number": 2171,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "[CANN]:Replace aclrtMemsetSync with InplaceZero operator for zero tensor creation",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14002",
      "number": 14002,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ci(docker): fix tags in \"Build and push docker image (tagged)\"",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4603",
      "number": 4603,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Switching attention backend to correctly complete LoRA and Quantized Model Tests",
      "url": "https://github.com/vllm-project/vllm/pull/20829",
      "number": 20829,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : avoid double token-to-piece cache",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7654",
      "number": 7654,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[ModelOpt] Introduce VLLM_MAX_TOKENS_PER_EXPERT_FP4_MOE env var to control blockscale tensor allocation",
      "url": "https://github.com/vllm-project/vllm/pull/18160",
      "number": 18160,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] enable logging in v1 offline inference",
      "url": "https://github.com/vllm-project/vllm/pull/20250",
      "number": 20250,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fixes the misuse/mixuse of time.time()/time.monotonic()",
      "url": "https://github.com/vllm-project/vllm/pull/3220",
      "number": 3220,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "feat: Replace cyclic kv cache with sliding kv cache",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3424",
      "number": 3424,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Hardware][PPC64LE] Enable V1 for ppc64le and ARM",
      "url": "https://github.com/vllm-project/vllm/pull/20554",
      "number": 20554,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Use git-path commit in hook",
      "url": "https://github.com/vllm-project/vllm/pull/17616",
      "number": 17616,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "support JIT mha.cu for SPEC_DEC in runtime",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6078",
      "number": 6078,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fixed RNG seed docs",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9723",
      "number": 9723,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Disable gptq_bitblas for <SM80 to fix GPTQ on V100/T4",
      "url": "https://github.com/vllm-project/vllm/pull/17541",
      "number": 17541,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix num_heads value for simple connector when tp enabled",
      "url": "https://github.com/vllm-project/vllm/pull/12074",
      "number": 12074,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "metal : fix compute pass descriptor autorelease crash",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9718",
      "number": 9718,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama: Add special tokens in hf_converter for RWKV v6",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9428",
      "number": 9428,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Config] Fix `max_batch_size` in dumped config",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1652",
      "number": 1652,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Switch from torch.cuda.amp.custom_fwd to torch.amp.custom_fwd(device=...)",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5684",
      "number": 5684,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[fix] fix correct assertion syntax error in attention utils.",
      "url": "https://github.com/vllm-project/vllm/pull/22154",
      "number": 22154,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[NVBUGs/5301636] ci: Update waives.txt to unskip L0 test for verification",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5430",
      "number": 5430,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix --split-max-size",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6655",
      "number": 6655,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix fmha v2 tests",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4661",
      "number": 4661,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: [5328141] increase tolerance for test_fp8_block_scale_gemm",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5849",
      "number": 5849,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "cmake: exclude libm.lib when GGML_SYCL=OFF but ONEAPI_ROOT is set in windows builds",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9983",
      "number": 9983,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1][Spec Decode] Handle draft tokens beyond max_model_len",
      "url": "https://github.com/vllm-project/vllm/pull/16087",
      "number": 16087,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix Opt injection",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2541",
      "number": 2541,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Bump the patch-update group with 5 updates",
      "url": "https://github.com/vllm-project/vllm/pull/10210",
      "number": 10210,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1] [Bugfix] eagle bugfix and enable correct lm_head for multimodal (2)",
      "url": "https://github.com/vllm-project/vllm/pull/18781",
      "number": 18781,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "gguf-py : do not use internal numpy types",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7472",
      "number": 7472,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Add one level list nesting for embeddings",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8936",
      "number": 8936,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "Fix mismatch of metadata func and global symbol",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2078",
      "number": 2078,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: nvbugs/5075538: fix cross attention mask when decoder input len > 1",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3585",
      "number": 3585,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Serving] Use stop strs and token ids for completions",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2534",
      "number": 2534,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[TRTLLM-5061] chore: add tags to API reference",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6123",
      "number": 6123,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Core]: Ability To Use Prompt Token Ids Inside Logit Processor",
      "url": "https://github.com/vllm-project/vllm/pull/4976",
      "number": 4976,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Update ROCM libs and improvements",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2358",
      "number": 2358,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "broken change: deprecate GGML_TASK_INIT and GGML_TASK_FINALIZE",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1995",
      "number": 1995,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Max concurrency estimation and check_enough_kv_cache_memory for models with sliding window layers",
      "url": "https://github.com/vllm-project/vllm/pull/19029",
      "number": 19029,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Max concurrency estimation and check_enough_kv_cache_memory for models with sliding window layers",
      "url": "https://github.com/vllm-project/vllm/pull/19029",
      "number": 19029,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1][TPU] Change kv cache shape.",
      "url": "https://github.com/vllm-project/vllm/pull/15145",
      "number": 15145,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Fix Llama4 - Index Error When Single Request Near Max Context",
      "url": "https://github.com/vllm-project/vllm/pull/16209",
      "number": 16209,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "vulkan: Find optimal memory type but with fallback",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5381",
      "number": 5381,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix AssertionError: skip_special_tokens=False is not supported for Mistral tokenizers",
      "url": "https://github.com/vllm-project/vllm/pull/16964",
      "number": 16964,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml-backend : only offload from host buffers (fix)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11124",
      "number": 11124,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix tensor gather rank error",
      "url": "https://github.com/vllm-project/vllm/pull/2530",
      "number": 2530,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Work-around incremental detokenization edge case error",
      "url": "https://github.com/vllm-project/vllm/pull/19449",
      "number": 19449,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Remove Yi model definition, please use `LlamaForCausalLM` instead",
      "url": "https://github.com/vllm-project/vllm/pull/2854",
      "number": 2854,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "quantize: fix F16/F32 downcast to q6_K",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5980",
      "number": 5980,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix IDEX dependence in xpu accelerator",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5666",
      "number": 5666,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server : improve error reporting",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13680",
      "number": 13680,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "kv-cache : fix seq_rm with seq_id == -1",
      "url": "https://github.com/ggml-org/llama.cpp/pull/15226",
      "number": 15226,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Clone tensors to avoid torch.save bloat",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3348",
      "number": 3348,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix critical buffer overflow vulnerability in quantize tool",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14283",
      "number": 14283,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix link jump in windows readme.md",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/30",
      "number": 30,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix broken link in README.md for the 1Cycle doc.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/60",
      "number": 60,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Serving] Fix BatchVerify to feed the extra token when fully accepted",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2285",
      "number": 2285,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "chat : fix multiple tool_calls on hermes-2-pro",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14962",
      "number": 14962,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "vulkan: Optimize soft_max",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10301",
      "number": 10301,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "[SYCL] correct cmd name in scirpt",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8877",
      "number": 8877,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "make: fix nvcc optimization flags for host code",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5309",
      "number": 5309,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI] Increase the threshold of the MTEB RERANK tests",
      "url": "https://github.com/vllm-project/vllm/pull/20615",
      "number": 20615,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Fix] Fix llama4 modelopt weight loading error",
      "url": "https://github.com/vllm-project/vllm/pull/22107",
      "number": 22107,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[TRTLLM-5532][feat] store the block of context request into kv cache",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6683",
      "number": 6683,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "`tool-call`: allow `--chat-template chatml` w/ `--jinja`, default to chatml upon parsing issue, avoid double bos",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11616",
      "number": 11616,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Modifications to error handling of multiple vllm api endpoints",
      "url": "https://github.com/vllm-project/vllm/pull/17165",
      "number": 17165,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Fix] Use static_cast for `.size()` for safety",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2369",
      "number": 2369,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[fix] Eagle-2 LLMAPI pybind argument fix.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3967",
      "number": 3967,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc][Bugfix] Update transformers for tokenizer issue",
      "url": "https://github.com/vllm-project/vllm/pull/6364",
      "number": 6364,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "A faster and more memory-efficient implementation of `zero_to_fp32`",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6658",
      "number": 6658,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Update flops profiler to recurse",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4374",
      "number": 4374,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Make `--cuda-graphs` work as expected",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1767",
      "number": 1767,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[ROCm] Enable custom paged attention kernel for Navi3/4",
      "url": "https://github.com/vllm-project/vllm/pull/13843",
      "number": 13843,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "enable multiple platform device in DP init",
      "url": "https://github.com/vllm-project/vllm/pull/17368",
      "number": 17368,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Fix mla cpu - missing 3 required positional arguments",
      "url": "https://github.com/vllm-project/vllm/pull/17494",
      "number": 17494,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix build on OpenBSD",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13541",
      "number": 13541,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "feat:[AutoDeploy] Add support for Phi3/4 Model Family",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3079",
      "number": 3079,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix missing task for speculative decoding",
      "url": "https://github.com/vllm-project/vllm/pull/9524",
      "number": 9524,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix all-gather duplicate params and wrong dtype",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7462",
      "number": 7462,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[https://nvbugs/5385987][fix] Fix Qwen2 quantization issue by pinning transformers version",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6673",
      "number": 6673,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Build] fix Dockerfile shell",
      "url": "https://github.com/vllm-project/vllm/pull/18402",
      "number": 18402,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[LogitProcessor] Use min float value as the mask value",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2451",
      "number": 2451,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix aya-23 conversion script",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7539",
      "number": 7539,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][Logprobs] Fix logprobs op to support more backend",
      "url": "https://github.com/vllm-project/vllm/pull/21591",
      "number": 21591,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[FIX] update openai version",
      "url": "https://github.com/vllm-project/vllm/pull/11287",
      "number": 11287,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][Frontend] Add missing \"type\":\"function\" in tool call streaming responses",
      "url": "https://github.com/vllm-project/vllm/pull/16346",
      "number": 16346,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[torch.compile] limit inductor threads and lazy import quant",
      "url": "https://github.com/vllm-project/vllm/pull/10482",
      "number": 10482,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][1/n] Fix the speculative decoding test by setting the target dtype",
      "url": "https://github.com/vllm-project/vllm/pull/19633",
      "number": 19633,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "gguf : basic type checking in gguf_get_*",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3346",
      "number": 3346,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[https://nvbugs/5394685][fix] using static scheduler 2CTA MLA as WAR for an accuracy issue",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6896",
      "number": 6896,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Mamba V2 Test not Asserting Failures. ",
      "url": "https://github.com/vllm-project/vllm/pull/21379",
      "number": 21379,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "feat: Add support for disaggregation with pp with pytorch backend",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6369",
      "number": 6369,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build] Fix LoRA OOM",
      "url": "https://github.com/vllm-project/vllm/pull/16624",
      "number": 16624,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[Issue 5927][fix] Avoid memory calls during broadcast for single GPU",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6010",
      "number": 6010,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: [nvbug/5368507] Fix test_generate_with_seed CI failure.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5772",
      "number": 5772,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[TPU][Bugfix] fix OOM issue in CI test",
      "url": "https://github.com/vllm-project/vllm/pull/21550",
      "number": 21550,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] set correct lora mapping when compute prompt logprobs",
      "url": "https://github.com/vllm-project/vllm/pull/16694",
      "number": 16694,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Core] add seq_id_to_seq_group clearing to avoid memory leak when s…",
      "url": "https://github.com/vllm-project/vllm/pull/16472",
      "number": 16472,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix: allocate tmp based on sgmv kernel if available",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2345",
      "number": 2345,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "vulkan: predicate max operation in soft_max shaders/soft_max",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10437",
      "number": 10437,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Kernel] introducing paged attention v3",
      "url": "https://github.com/vllm-project/vllm/pull/15353",
      "number": 15353,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[ROCM] Fix native attention function call for navi",
      "url": "https://github.com/vllm-project/vllm/pull/13649",
      "number": 13649,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ci: fix job are cancelling each other",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6781",
      "number": 6781,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] get_num_blocks_to_allocate with null_block",
      "url": "https://github.com/vllm-project/vllm/pull/19031",
      "number": 19031,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Keye-VL compatibility with `tok_kwargs` (#20058)",
      "url": "https://github.com/vllm-project/vllm/pull/20353",
      "number": 20353,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "[SYCL] Updated SYCL device filtering ",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8901",
      "number": 8901,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix broken links in CIFAR",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/69",
      "number": 69,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Fix missing rope scaling option for YaRN",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2078",
      "number": 2078,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[iOS] Enable iOS build flow after FFI refactor",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/3241",
      "number": 3241,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Bugfix] Fix Sharding for fp16",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1010",
      "number": 1010,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Upgrade FlashInfer to v0.2.11",
      "url": "https://github.com/vllm-project/vllm/pull/22613",
      "number": 22613,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Fix MLA + V1 + TP==1 causing reinitialization of cuda context",
      "url": "https://github.com/vllm-project/vllm/pull/14910",
      "number": 14910,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: iteration logging and typing in PyExecutor",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4734",
      "number": 4734,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix top k sampler in server example",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1977",
      "number": 1977,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "cuda : fix CUDA_FLAGS not being applied",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10403",
      "number": 10403,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix for the condition to accept empty encoder inputs for mllama",
      "url": "https://github.com/vllm-project/vllm/pull/17732",
      "number": 17732,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[FalconH1] Fix output dtype in RMSNorm fallback path for Falcon-H1 (e.g. 0.5B)",
      "url": "https://github.com/vllm-project/vllm/pull/18500",
      "number": 18500,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "[SYCL] fix error when set main gpu to non-zero",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5901",
      "number": 5901,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Support for env value for GPTQ_BITS and GPTQ_GROUPSIZE.",
      "url": "https://github.com/huggingface/text-generation-inference/pull/580",
      "number": 580,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "rpc : throw an exception when the RPC endpoint is unreachable",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7934",
      "number": 7934,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[FIX] fix transform params without GPUs",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/219",
      "number": 219,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml : fix IQ3_S AVX implementation",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5834",
      "number": 5834,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] fix _apply_min_tokens_penalty when eos_token_id is None",
      "url": "https://github.com/vllm-project/vllm/pull/4386",
      "number": 4386,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix run-time on FreeBSD in get_executable_path()",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10948",
      "number": 10948,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix: metrics unbounded memory",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2528",
      "number": 2528,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Make async scheduling compatible with DP",
      "url": "https://github.com/vllm-project/vllm/pull/21244",
      "number": 21244,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: fix FlashAttention on Turing",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13415",
      "number": 13415,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix clip build on windows + clang",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6934",
      "number": 6934,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "`TiledFusedLogitsLoss` bug fix",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7459",
      "number": 7459,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Bug Fix 5880",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6378",
      "number": 6378,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "vulkan: fix noncontig check for mat_mul_id splitting",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14683",
      "number": 14683,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fixing the transformer APIs to return tuple as the output (if needed)",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1491",
      "number": 1491,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Get opentelemetry trace id from request headers instead of creating a new trace",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2648",
      "number": 2648,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix marlin moe fallback logic for llama4",
      "url": "https://github.com/vllm-project/vllm/pull/18042",
      "number": 18042,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix multiple definition while building evoformer",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4556",
      "number": 4556,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Handle non-serializable objects when dumping benchmark results",
      "url": "https://github.com/vllm-project/vllm/pull/19114",
      "number": 19114,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[Issue 6193] Fix gemma3vl weight loader",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6233",
      "number": 6233,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : disable warnings for 3rd party sha1 dependency",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10527",
      "number": 10527,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Revert \"test:[nvbug 5415268] add kv_cache_free_gpu_mem_fraction param and llama4 rcca cases\"",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6462",
      "number": 6462,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Android] Fix header after recent tvm runtime refactor",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/3185",
      "number": 3185,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix a Typo Which Causes It Unable to Compile with Old Glibc",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5694",
      "number": 5694,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: Add nested aliases for Llama 4",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3381",
      "number": 3381,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix pagable h2d memcpy",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5301",
      "number": 5301,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix Tensor Parallelism Padding Consistency in Granite Models",
      "url": "https://github.com/vllm-project/vllm/pull/20843",
      "number": 20843,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix(server): fix seeding with multiple shards",
      "url": "https://github.com/huggingface/text-generation-inference/pull/44",
      "number": 44,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : suppress unary minus operator warning",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8448",
      "number": 8448,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama-bench: fixed help output for `-override-tensors` to `-override-tensor`",
      "url": "https://github.com/ggml-org/llama.cpp/pull/15129",
      "number": 15129,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[WIP] Try to fix numerical issues in embedding models",
      "url": "https://github.com/vllm-project/vllm/pull/22878",
      "number": 22878,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix load time calculation error in llama_bench",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9546",
      "number": 9546,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "Auto_target.py Build Fix",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/3160",
      "number": 3160,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[TRTLLM-5633] - Merge current waive list with the TOT waive list",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5198",
      "number": 5198,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Add modules_to_not_convert in quantized model",
      "url": "https://github.com/huggingface/text-generation-inference/pull/3053",
      "number": 3053,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1] Prompt logprobs + APC compatibility; prompt logprobs reqs cannot fill APC",
      "url": "https://github.com/vllm-project/vllm/pull/13949",
      "number": 13949,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Quant] [Bugfix] Fix quantization config matching with `hf_to_vllm_mapper`",
      "url": "https://github.com/vllm-project/vllm/pull/20046",
      "number": 20046,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Avoid error traceback in logs when V1 `LLM` terminates",
      "url": "https://github.com/vllm-project/vllm/pull/13565",
      "number": 13565,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Use partition numel",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2011",
      "number": 2011,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Simulate mxfp4 quark model execution on cdna4 until kernels are integrated",
      "url": "https://github.com/vllm-project/vllm/pull/22355",
      "number": 22355,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix assertion for offloading states",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6855",
      "number": 6855,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] : Fix typo - logger.warn_once -> logger.warning_once",
      "url": "https://github.com/vllm-project/vllm/pull/20852",
      "number": 20852,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "ggml-quants : fix avx2 iq1_s vec_dot when compiled with gcc",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5742",
      "number": 5742,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix: avoid duplicate bos token",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1594",
      "number": 1594,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "[SYCL] Add SYCL Backend registry, device and Event Interfaces",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9705",
      "number": 9705,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CUDA graphs] Enable full cuda graphs with FA3 AoT scheduling",
      "url": "https://github.com/vllm-project/vllm/pull/20301",
      "number": 20301,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[v1] fix compilation cache",
      "url": "https://github.com/vllm-project/vllm/pull/11598",
      "number": 11598,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "fix-bgmv-kernel-640",
      "url": "https://github.com/vllm-project/vllm/pull/4007",
      "number": 4007,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][Kernel] Support partial rotary embedding for MRoPE triton kernel",
      "url": "https://github.com/vllm-project/vllm/pull/22593",
      "number": 22593,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Correct adapter usage for cohere and jamba",
      "url": "https://github.com/vllm-project/vllm/pull/8292",
      "number": 8292,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix typo: change polciies to policies",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4090",
      "number": 4090,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Increase RayDistributedExecutor RAY_CGRAPH_get_timeout",
      "url": "https://github.com/vllm-project/vllm/pull/15301",
      "number": 15301,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "[Bug Fixed] torch.cuda.is_available -> torch.cuda.is_available()",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2661",
      "number": 2661,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Fix test breakages from transformers 4.45 upgrade",
      "url": "https://github.com/vllm-project/vllm/pull/8829",
      "number": 8829,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "graph : make llm_graph_context destructor virtual",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14410",
      "number": 14410,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "nix: fix blas support",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6281",
      "number": 6281,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI/Build] Fix pre-commit failure in docs",
      "url": "https://github.com/vllm-project/vllm/pull/21897",
      "number": 21897,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix the issue of token count in BlockTable",
      "url": "https://github.com/vllm-project/vllm/pull/20421",
      "number": 20421,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BUGFIX][NON_CUDA] Fix failing introduced by #20061 when import bitsandbytes.py",
      "url": "https://github.com/vllm-project/vllm/pull/20849",
      "number": 20849,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "convert : refactor rope_freqs generation",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9396",
      "number": 9396,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[SLM] Enable cuda graph in O3 mode",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1799",
      "number": 1799,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Build/CI] Upgrade CUTLASS to 3.9.2",
      "url": "https://github.com/vllm-project/vllm/pull/17641",
      "number": 17641,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Don't log OpenAI field aliases as ignored",
      "url": "https://github.com/vllm-project/vllm/pull/11378",
      "number": 11378,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix a bug in tying OPT embeddings",
      "url": "https://github.com/vllm-project/vllm/pull/1",
      "number": 1,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "TEST CI DEBUG WORKFLOW",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6469",
      "number": 6469,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Remove misleading message in gemma2 and gemma3",
      "url": "https://github.com/vllm-project/vllm/pull/14850",
      "number": 14850,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Bump tensorflow-gpu from 1.14.0 to 1.15.2",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/39",
      "number": 39,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Remove the random-generator from context during inference",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2228",
      "number": 2228,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Core] Zero-copy asdict for InputMetadata",
      "url": "https://github.com/vllm-project/vllm/pull/3475",
      "number": 3475,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "finetune : keep allocs alive until all allocations are done",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4486",
      "number": 4486,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix replace lmhead issue when ckpt not load",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4522",
      "number": 4522,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Platform] improve platforms getattr",
      "url": "https://github.com/vllm-project/vllm/pull/12264",
      "number": 12264,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Core] cleanup zmq ipc sockets on exit",
      "url": "https://github.com/vllm-project/vllm/pull/11115",
      "number": 11115,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Change granite chat template to keep json list formatting for tool calls",
      "url": "https://github.com/vllm-project/vllm/pull/10452",
      "number": 10452,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "make : fix embdinput library and server examples building on MSYS2",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2235",
      "number": 2235,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "remove sort_keys=True in guided_decoding",
      "url": "https://github.com/vllm-project/vllm/pull/5332",
      "number": 5332,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "[WIP] Add gfx1100 support to AMD pytorch build",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2642",
      "number": 2642,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[easy] Fix logspam on PiecewiseBackend errors",
      "url": "https://github.com/vllm-project/vllm/pull/17138",
      "number": 17138,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "CI: fix TensorRT H200 tests",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5301",
      "number": 5301,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix overflow indexing in causal_conv1d kernel",
      "url": "https://github.com/vllm-project/vllm/pull/20938",
      "number": 20938,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[fix][test] Speedup Nemotron NAS unittests",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5202",
      "number": 5202,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Remove CustomChatCompletionContentPartParam multimodal input type",
      "url": "https://github.com/vllm-project/vllm/pull/10054",
      "number": 10054,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Performance] [Speculative decoding] Support draft model on different tensor-parallel size than target model",
      "url": "https://github.com/vllm-project/vllm/pull/4933",
      "number": 4933,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix chunked prefill for GGUF",
      "url": "https://github.com/vllm-project/vllm/pull/14666",
      "number": 14666,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix loading of aliased weights",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6018",
      "number": 6018,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server: fix reported top tokens for temperature 0",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7203",
      "number": 7203,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix Bloom logits mismatch",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2851",
      "number": 2851,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Prevent `LLM.encode` for non-generation Models ",
      "url": "https://github.com/vllm-project/vllm/pull/5184",
      "number": 5184,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "fix mul_mat_id() for new input, make the ut pass",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6682",
      "number": 6682,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "models/templates: add mistralai/Mistral-Small-3.1-24B-Instruct-2503 template with tool calling support",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14148",
      "number": 14148,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server: maintain chat completion id for streaming responses",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5880",
      "number": 5880,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[manifest] update mainfest to add hpp file in deepspeed.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5533",
      "number": 5533,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Use numba 0.61 for python 3.10+ to support numpy>=2",
      "url": "https://github.com/vllm-project/vllm/pull/15692",
      "number": 15692,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[AMD][Quantization] Add TritonScaledMMLinearKernel since int8 is broken for AMD",
      "url": "https://github.com/vllm-project/vllm/pull/12282",
      "number": 12282,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "cmake: Guard GGML_CPU_ALL_VARIANTS by architecture",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13890",
      "number": 13890,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "set the default to use set_to_none for clearing gradients in BF16 optimizer.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5434",
      "number": 5434,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Added support for SFTTrainer checkpoint models and adapter models containing some non-LoRA weights",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9778",
      "number": 9778,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fixed CUDA runtime version check",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1879",
      "number": 1879,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: wrong argument name `enable_overlap_scheduler`",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4433",
      "number": 4433,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "feat: Enhance AutoTuner inference path and code readability",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4466",
      "number": 4466,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Frontend][Bugfix] support prefill decode disaggregation on deepseek",
      "url": "https://github.com/vllm-project/vllm/pull/14824",
      "number": 14824,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Fix docker build cpu-dev image error",
      "url": "https://github.com/vllm-project/vllm/pull/19394",
      "number": 19394,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "cuda : non-cont concat support",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7610",
      "number": 7610,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[nvbug/5314469][feat] Include the executor's max batch size in CUDA g…",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4843",
      "number": 4843,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] fix top_k: 0 in generation_config.json can't disable top-k sampling",
      "url": "https://github.com/vllm-project/vllm/pull/17691",
      "number": 17691,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Frontend] Support image object in llm.chat",
      "url": "https://github.com/vllm-project/vllm/pull/19635",
      "number": 19635,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[#4403][refactor] Move more transformations to new inf optimizer, Add quantization_source to factory interface",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6759",
      "number": 6759,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fixed rlimit error message.",
      "url": "https://github.com/ggml-org/llama.cpp/pull/888",
      "number": 888,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][Core] Prevent token lengths exceeding `max_model_len` in V0",
      "url": "https://github.com/vllm-project/vllm/pull/19348",
      "number": 19348,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix GLM rotary_dim issue and support v1",
      "url": "https://github.com/vllm-project/vllm/pull/16912",
      "number": 16912,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix buffer size for pipeline parallel and communication schedule",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2862",
      "number": 2862,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: llama4 EP weights loading issue",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3386",
      "number": 3386,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[XPU] Fix OOM issue for data parallel with Ray backend",
      "url": "https://github.com/vllm-project/vllm/pull/22500",
      "number": 22500,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Support MLA for CompressedTensorsWNA16",
      "url": "https://github.com/vllm-project/vllm/pull/13725",
      "number": 13725,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc]Fix incorrect local IP detection in multi-network interface environments",
      "url": "https://github.com/vllm-project/vllm/pull/15071",
      "number": 15071,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Fixing GPTQ exllama kernel usage.",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1101",
      "number": 1101,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Check for local CUDA graphs when enable_cuda_graph=True",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2941",
      "number": 2941,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix GGML not compiling on macOS with GCC",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11185",
      "number": 11185,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] llama4 fa3 fix - RuntimeError: scheduler_metadata must have shape (metadata_size)",
      "url": "https://github.com/vllm-project/vllm/pull/16998",
      "number": 16998,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Support AutoAWQ Models Serialized in Marlin Format",
      "url": "https://github.com/vllm-project/vllm/pull/3751",
      "number": 3751,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Bump gradio from 3.40.1 to 4.19.2 in /examples/qwen",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1640",
      "number": 1640,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Feature][Hardware][AMD] Enable level 3 compilation on rocm",
      "url": "https://github.com/vllm-project/vllm/pull/10836",
      "number": 10836,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Core] Use torch.cuda.memory_stats() to profile peak memory usage",
      "url": "https://github.com/vllm-project/vllm/pull/9352",
      "number": 9352,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "[SYCL] Fix compilation error from GGML_ASSERT",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7303",
      "number": 7303,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Enable prefix caching with full cuda graphs",
      "url": "https://github.com/vllm-project/vllm/pull/19617",
      "number": 19617,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[New Model]: nomic-embed-text-v2-moe",
      "url": "https://github.com/vllm-project/vllm/pull/17785",
      "number": 17785,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] add mm_processor_kwargs to extra_body for Qwen2.5-VL",
      "url": "https://github.com/vllm-project/vllm/pull/13533",
      "number": 13533,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml : fix BLAS with unsupported types",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9775",
      "number": 9775,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Optimize the Qwen3_ReasoningParser extract_reasoning_content",
      "url": "https://github.com/vllm-project/vllm/pull/17515",
      "number": 17515,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Dynamic Spec Decoding] Minor fix for disabling speculative decoding",
      "url": "https://github.com/vllm-project/vllm/pull/5000",
      "number": 5000,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Update test_flash_attn.py",
      "url": "https://github.com/vllm-project/vllm/pull/17102",
      "number": 17102,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Model] Expose Phi3v num_crops as a mm_processor_kwarg",
      "url": "https://github.com/vllm-project/vllm/pull/8658",
      "number": 8658,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : fix Gemma3 SWA KV cache shift",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12373",
      "number": 12373,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix bad import.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2943",
      "number": 2943,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : sanitize tokens in the upper bound",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9359",
      "number": 9359,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "Rename subpackage operator => op",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1521",
      "number": 1521,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Serving][Fix] Pass draft length when constructing draft action",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2291",
      "number": 2291,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : support raw NUL bytes in tokens",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8992",
      "number": 8992,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Fix] Copy tokenizer files before dumping config",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/668",
      "number": 668,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Reorder kv dtype check to avoid nvcc not found error on AMD platform",
      "url": "https://github.com/vllm-project/vllm/pull/3104",
      "number": 3104,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Neuron] Update Dockerfile.neuron to fix build failure",
      "url": "https://github.com/vllm-project/vllm/pull/9822",
      "number": 9822,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Config.__init__() got an unexpected keyword argument 'engine' api_server args",
      "url": "https://github.com/vllm-project/vllm/pull/8556",
      "number": 8556,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "check input length of sonnet samples",
      "url": "https://github.com/vllm-project/vllm/pull/16423",
      "number": 16423,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Streamer] Fix UTF-8 handling in streamer",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2978",
      "number": 2978,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix missing `kv_caches` and `attn_metadata` in `OpenVINOCausalLM`",
      "url": "https://github.com/vllm-project/vllm/pull/14271",
      "number": 14271,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix Maverick correctness by filling zero to cache space in cutlass_moe",
      "url": "https://github.com/vllm-project/vllm/pull/20167",
      "number": 20167,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "fix autoTP checkpoint load for OPT models",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3749",
      "number": 3749,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1][Performance] Implement custom serializaton for MultiModalKwargs [Rebased]",
      "url": "https://github.com/vllm-project/vllm/pull/16432",
      "number": 16432,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Samyamr/largest partitioned params calculation fix",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1150",
      "number": 1150,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix compile wrapper",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5455",
      "number": 5455,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build] Update OpenVINO Dockerfile to Ubuntu 24.04",
      "url": "https://github.com/vllm-project/vllm/pull/11670",
      "number": 11670,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Use consistent logger everywhere",
      "url": "https://github.com/vllm-project/vllm/pull/3738",
      "number": 3738,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[ci] Fix metrics test model path",
      "url": "https://github.com/vllm-project/vllm/pull/13635",
      "number": 13635,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Build][AMD] Fix the hashbang in the hipify script",
      "url": "https://github.com/vllm-project/vllm/pull/12698",
      "number": 12698,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[None][fix] WAR GPT OSS on H20 with Triton MOE",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6721",
      "number": 6721,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[P/D] Support NIXL connector to disconnect during a clean shutdown",
      "url": "https://github.com/vllm-project/vllm/pull/22638",
      "number": 22638,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[doc] update mtp documents",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5387",
      "number": 5387,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : switch KQ multiplication to use F32 precision by default",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10015",
      "number": 10015,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "cuda : add LLAMA_CUDA_NO_PEER_COPY to workaround broken ROCm p2p copy",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6208",
      "number": 6208,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix entrypoint tests for embedding models",
      "url": "https://github.com/vllm-project/vllm/pull/14052",
      "number": 14052,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Neuron][Build] Require setuptools >= 77.0.3 for PEP 639",
      "url": "https://github.com/vllm-project/vllm/pull/17603",
      "number": 17603,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llguidance : init tokenizer slices",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13424",
      "number": 13424,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Doc] Add note about context length in Phi-3-Vision example",
      "url": "https://github.com/vllm-project/vllm/pull/5887",
      "number": 5887,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Add left recursion check: quit early instead of going into an infinite loop",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7083",
      "number": 7083,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "chat : fix yandex chat template",
      "url": "https://github.com/ggml-org/llama.cpp/pull/15116",
      "number": 15116,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix issues XPU tests hit with extra-index-url",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7291",
      "number": 7291,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: draft tokens `TorchSampler` fast path",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5619",
      "number": 5619,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Dockerfile: Upgrade Cuda to 12.1",
      "url": "https://github.com/vllm-project/vllm/pull/1609",
      "number": 1609,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "tokenization: no double BOS tokens",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7107",
      "number": 7107,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "chore : Fix vulkan related compiler warnings, add help text, improve CLI options",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8477",
      "number": 8477,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[bugfix] fix profile impact benchmark results",
      "url": "https://github.com/vllm-project/vllm/pull/21507",
      "number": 21507,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "fix logging msg for block manager",
      "url": "https://github.com/vllm-project/vllm/pull/3701",
      "number": 3701,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[None][fix] fix CUDA graph config for test_llm_api_pytorch.py.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6826",
      "number": 6826,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "update the free_gpu_mem_fraction for H100 qwen3 qa test",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5114",
      "number": 5114,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "fix: llama-mmap: add include for cerrno; fixes issue #11295",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11296",
      "number": 11296,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[TRTLLM-5863][feat] Support Weight-Only-Quantization in PyTorch Workflow",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5850",
      "number": 5850,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[ROCm] Fixes for GPTQ on ROCm",
      "url": "https://github.com/vllm-project/vllm/pull/2180",
      "number": 2180,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Bugfix] Fix the bug of MOSS compilation with fp16",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/44",
      "number": 44,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Reduce warning message introduced in env_override",
      "url": "https://github.com/vllm-project/vllm/pull/19476",
      "number": 19476,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fp32 optimizer state loading",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/289",
      "number": 289,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[BUG5388075][fix] Fix error in post-merge-tests",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5949",
      "number": 5949,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "convert: handle when model's tokenization method relies on Mecab",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13830",
      "number": 13830,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "AMD: parse the architecture as supplied by gcnArchName",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11244",
      "number": 11244,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix #4227 (double free in examples/train-text-from-scratch)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4351",
      "number": 4351,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][V1][Minor] Fix shutting_down flag checking in V1 MultiprocExecutor",
      "url": "https://github.com/vllm-project/vllm/pull/14053",
      "number": 14053,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[None][fix] WAR GPT OSS on H20 with Triton MOE",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6721",
      "number": 6721,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Model][Speculative Decoding] Expand DeepSeek MTP code to support k > n_predict",
      "url": "https://github.com/vllm-project/vllm/pull/13626",
      "number": 13626,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Performance: Placing Metal encoder debug group behind a define",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4873",
      "number": 4873,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] add jsonargment to support --hf-overrides",
      "url": "https://github.com/vllm-project/vllm/pull/17842",
      "number": 17842,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Remove default multiproc executor `collective_rpc` timeout",
      "url": "https://github.com/vllm-project/vllm/pull/17000",
      "number": 17000,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "keep the minimum `min_keep` value to 1 in sampling",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9603",
      "number": 9603,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Build/BugFix] Fix hopper 12.8 build",
      "url": "https://github.com/vllm-project/vllm/pull/14354",
      "number": 14354,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Option to exclude frozen weights for checkpoint save",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3953",
      "number": 3953,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI] Fix model path in runai streamer",
      "url": "https://github.com/vllm-project/vllm/pull/13796",
      "number": 13796,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : better replace_all",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8852",
      "number": 8852,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] [Spec Decode] Remove LlamaForCausalLMEagle3 to fix CI",
      "url": "https://github.com/vllm-project/vllm/pull/22611",
      "number": 22611,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Bug fix for flops profilers output",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1885",
      "number": 1885,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1] Solve potential deadlock issue in v1 engine core client internally",
      "url": "https://github.com/vllm-project/vllm/pull/19927",
      "number": 19927,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BUGFIX][NON_CUDA] Fix failing introduced by #20061 when import bitsandbytes.py",
      "url": "https://github.com/vllm-project/vllm/pull/20849",
      "number": 20849,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "musa: enable VMM support",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9597",
      "number": 9597,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[TRTLLM-7030][fix] BREAKING CHANGE: Mismatch between docs and actual commands",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6780",
      "number": 6780,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix \"text_offset\" when \"--return_tokens_as_token_ids\" is enabled",
      "url": "https://github.com/vllm-project/vllm/pull/19379",
      "number": 19379,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Fix gated docs",
      "url": "https://github.com/huggingface/text-generation-inference/pull/805",
      "number": 805,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "[SYCL] Update SYCL-Rope op and Refactor",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8157",
      "number": 8157,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: mllama e2e pytorch flow fix",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3397",
      "number": 3397,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Fix][Bench] Improve determinism and log dump",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2960",
      "number": 2960,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Qwen3moe router logits allreduce",
      "url": "https://github.com/vllm-project/vllm/pull/21829",
      "number": 21829,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[pipe] prevent deadlock with multiple evals sequence",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1944",
      "number": 1944,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "fix(rpc): Improve input validation and error handling",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13069",
      "number": 13069,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "gguf_convert_endian.py: implement byteswapping for q4_k and q6_k",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11349",
      "number": 11349,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Docs][minor] Fix broken gh-file link in distributed serving docs",
      "url": "https://github.com/vllm-project/vllm/pull/21543",
      "number": 21543,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Frontend] Online Pooling API",
      "url": "https://github.com/vllm-project/vllm/pull/11457",
      "number": 11457,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Add warning for Attention backends that do not support irope yet",
      "url": "https://github.com/vllm-project/vllm/pull/16212",
      "number": 16212,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Empty grad fix",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/291",
      "number": 291,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Skip device and quant Pydantic validation to make plugin device work",
      "url": "https://github.com/vllm-project/vllm/pull/18843",
      "number": 18843,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[feat] Enable chunked context for flashinfer",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4132",
      "number": 4132,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[SLM] Android refactor for SLM",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1494",
      "number": 1494,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Fix case where `collective_rpc` returns `None`",
      "url": "https://github.com/vllm-project/vllm/pull/22006",
      "number": 22006,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Bugfix for MiniCPM model support (undo HF model tensor permute)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5392",
      "number": 5392,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "fix custom cache dir ",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2226",
      "number": 2226,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[None][fix] remove closed bugs",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6576",
      "number": 6576,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[None][fix] Revert commit 48ddc3d & add test for disagg server with different max_num_tokens",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6259",
      "number": 6259,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[xpu] disable cudagraph for xpu platform",
      "url": "https://github.com/vllm-project/vllm/pull/21354",
      "number": 21354,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : fix compatibility with old 2 expert models",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6735",
      "number": 6735,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "opencl: remove unreachable `return`",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14806",
      "number": 14806,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Hotfix] Fix CPU wheels for Python API reference",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/471",
      "number": 471,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Quantization]Fix support for non quantized visual layers in otherwise quantized mllama model, including missing scaling factors",
      "url": "https://github.com/vllm-project/vllm/pull/9800",
      "number": 9800,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix overlap communication of ZeRO stage 1 and 2",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5606",
      "number": 5606,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Avoid race condition with port selection in unit tests",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3975",
      "number": 3975,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[None][infra] Waive failed case in post-merge on main",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6602",
      "number": 6602,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[ROCm][Bugfix] Fixed several bugs related to rccl path and attention selector logic",
      "url": "https://github.com/vllm-project/vllm/pull/3699",
      "number": 3699,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix QKVCrossParallelLinear::sync_weight_attrs for PyTorch compile",
      "url": "https://github.com/vllm-project/vllm/pull/17844",
      "number": 17844,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Update python to python3 calls for image; fix prefix & input calculations.",
      "url": "https://github.com/vllm-project/vllm/pull/21391",
      "number": 21391,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Model] Support Mistral-Nemo",
      "url": "https://github.com/vllm-project/vllm/pull/6548",
      "number": 6548,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[Infra] Revert #3759",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3904",
      "number": 3904,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix Type Mismatch",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6410",
      "number": 6410,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "clip : refactor set input for cgraph + fix qwen2.5vl input",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13136",
      "number": 13136,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "fix: use `vm_allocate` to allocate CPU backend buffer on macOS",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9875",
      "number": 9875,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Update TensorRT-LLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1725",
      "number": 1725,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Fix: ignore nvshmem_src_*.txz from `confidentiality-scan`",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5831",
      "number": 5831,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix PP checkpoint bloat",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1324",
      "number": 1324,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI] Enable test_initialization to run on V1",
      "url": "https://github.com/vllm-project/vllm/pull/16736",
      "number": 16736,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build][Bugfix][Doc][ROCm] CI fix and doc update after ROCm 6.2 upgrade",
      "url": "https://github.com/vllm-project/vllm/pull/8777",
      "number": 8777,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix(server): Fix stop sequences",
      "url": "https://github.com/huggingface/text-generation-inference/pull/11",
      "number": 11,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "musa: fix build warning",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13129",
      "number": 13129,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][V1] Avoid importing PreTrainedModel",
      "url": "https://github.com/vllm-project/vllm/pull/15366",
      "number": 15366,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CUDA: fix pointer incrementation in FA",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14916",
      "number": 14916,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml : fix llamafile sgemm wdata offsets",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6710",
      "number": 6710,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Allow for LoRA modules with different rank dimensions when using HF format",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2366",
      "number": 2366,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Improve progress bar",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10821",
      "number": 10821,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "Fix windows build again",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/902",
      "number": 902,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Fix the error in the tip for the --lora-modules parameter",
      "url": "https://github.com/vllm-project/vllm/pull/12319",
      "number": 12319,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[vlm] Remove vision language config.",
      "url": "https://github.com/vllm-project/vllm/pull/6089",
      "number": 6089,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "feat(ci): add visionOS build workflow",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11103",
      "number": 11103,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "common : try to fix Android CI",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6780",
      "number": 6780,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "metal : fix compute pass descriptor autorelease crash",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9718",
      "number": 9718,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : adds llama-grammar memoization stacks (#4218)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9833",
      "number": 9833,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Add attribute check for language_model when replace last linear module",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6650",
      "number": 6650,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Fix] Remove the type index defined for RadixTree and PrefixCache",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/3161",
      "number": 3161,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama: fix shadowing",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1367",
      "number": 1367,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Frontend] Fix log message to use http vs https",
      "url": "https://github.com/vllm-project/vllm/pull/14774",
      "number": 14774,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Added support for GGML_OP_CLAMP in Metal",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6662",
      "number": 6662,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Avoid -march=native when reproducible build is wanted",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11366",
      "number": 11366,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix multiple zero 3 tracing errors",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1901",
      "number": 1901,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: Remove duplicate tokenization in generation server",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4492",
      "number": 4492,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "docker : fix missing binaries in full-cuda image",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9278",
      "number": 9278,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix dtype-related RuntimeError when computing scaled global norm",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5296",
      "number": 5296,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fixed ppc build due to openssl conflict",
      "url": "https://github.com/vllm-project/vllm/pull/18144",
      "number": 18144,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[iOS] Handle invalid input URL",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/338",
      "number": 338,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[fix] Enable pp tests",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3978",
      "number": 3978,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Revert \"fix: Update num_of_ctx_tokens in iteration stats\"",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3938",
      "number": 3938,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Bump rexml from 3.2.4 to 3.2.5 in /docs",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1025",
      "number": 1025,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "fix missing slash in `fs_get_cache_directory()`",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7503",
      "number": 7503,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "#1447 upgrade rust to fix compilation error",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1448",
      "number": 1448,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Implement scalar sum over all rows in ggml_compute_forward_sum_f32",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1162",
      "number": 1162,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Only call rocblas_initialize for versions < 4 to eliminate unncessary VRAM allocation on some AMD cards",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11080",
      "number": 11080,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Core] Fix abrupt request abort",
      "url": "https://github.com/vllm-project/vllm/pull/18485",
      "number": 18485,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix:  Refactor Deepseek tp_size calculation",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3695",
      "number": 3695,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Make unspecified --host bind to dual stack",
      "url": "https://github.com/vllm-project/vllm/pull/22823",
      "number": 22823,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "json-schema-to-grammar: fix order of props in C++, support non-string const/enum",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6232",
      "number": 6232,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Support multiple attention groups for KV sharing",
      "url": "https://github.com/vllm-project/vllm/pull/22672",
      "number": 22672,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "quantize: skip tensor override when in fallback mode",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14995",
      "number": 14995,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build] force writing version file",
      "url": "https://github.com/vllm-project/vllm/pull/13544",
      "number": 13544,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "bumping DSE commit for pillow security fix",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/312",
      "number": 312,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Frontend] Represent tokens with identifiable strings",
      "url": "https://github.com/vllm-project/vllm/pull/6626",
      "number": 6626,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "Fix symbolic link to tvm_home",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/413",
      "number": 413,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "encoded ds config into command line argument when launching child processes in autotuning",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2524",
      "number": 2524,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Enable ZeRO checkpointing for ZeRO-Offload",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/337",
      "number": 337,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "vulkan: Use push constant offset to handle misaligned descriptors",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10987",
      "number": 10987,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "tests: fix commit info",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5016",
      "number": 5016,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] fix adding bias twice in ipex GPTQ quantization",
      "url": "https://github.com/vllm-project/vllm/pull/18363",
      "number": 18363,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[MISC] correct signature for LoaderFunction",
      "url": "https://github.com/vllm-project/vllm/pull/18670",
      "number": 18670,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix wrong truncate_prompt_tokens type hint",
      "url": "https://github.com/vllm-project/vllm/pull/22761",
      "number": 22761,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "server: fix reported top tokens for temperature 0",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7203",
      "number": 7203,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Revert inspection code in #13743",
      "url": "https://github.com/vllm-project/vllm/pull/13832",
      "number": 13832,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Fix][ROCm] Enforce eager for all encoder-decoder models on ROCm",
      "url": "https://github.com/vllm-project/vllm/pull/18154",
      "number": 18154,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Windows fixes",
      "url": "https://github.com/ggml-org/llama.cpp/pull/31",
      "number": 31,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Remove unused parts in Megatron-LM code and add copyright notice",
      "url": "https://github.com/vllm-project/vllm/pull/110",
      "number": 110,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix token_to_piece implementation in Swift",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4278",
      "number": 4278,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : fix missing quotes",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4937",
      "number": 4937,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "ggml : add SSE 4.2 and x64 base variant for CPUs without AVX",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12871",
      "number": 12871,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "code clean when clean shm",
      "url": "https://github.com/vllm-project/vllm/pull/22516",
      "number": 22516,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "docker : fix CPU ARM build",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11403",
      "number": 11403,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix bugs of running Quark quantized models",
      "url": "https://github.com/vllm-project/vllm/pull/16236",
      "number": 16236,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix: fix autodeploy",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4957",
      "number": 4957,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix `HfExampleModels.find_hf_info`",
      "url": "https://github.com/vllm-project/vllm/pull/12223",
      "number": 12223,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "hotfix: ipex fails since cuda moe kernel is not supported",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2532",
      "number": 2532,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Convert the fp16_params to group of parameters",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1651",
      "number": 1651,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "chat : only remove double bos/eos if added",
      "url": "https://github.com/ggml-org/llama.cpp/pull/15086",
      "number": 15086,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama: fix some return values to match hparams types",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8461",
      "number": 8461,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][V0] Another multi-sequence logprobs streaming edge case",
      "url": "https://github.com/vllm-project/vllm/pull/16805",
      "number": 16805,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix OpenVINO model runner",
      "url": "https://github.com/vllm-project/vllm/pull/12750",
      "number": 12750,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fixes for RTD build errors",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/606",
      "number": 606,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Fix] Memory usage statistics",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1252",
      "number": 1252,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix port handling in make_zmq_path",
      "url": "https://github.com/vllm-project/vllm/pull/19117",
      "number": 19117,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Zero3 Fix allreduce optimization for extra large tensor",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3739",
      "number": 3739,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CI: fix release build (Ubuntu+Mac)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8170",
      "number": 8170,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[https://nvbugs/5449155][fix] Fix DeepSeek R1 weight loading for TP16",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6913",
      "number": 6913,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "fix: using combined imatrix to quantise models triggers an error #14952",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14973",
      "number": 14973,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Model] Fix annotation typos ",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2672",
      "number": 2672,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Revert \"[infra] Unwaive unittests/_torch\"",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4950",
      "number": 4950,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix broken vision language example",
      "url": "https://github.com/vllm-project/vllm/pull/14292",
      "number": 14292,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Revert \"fix a bug of global cuda graph dummy request\"",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4970",
      "number": 4970,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[bug fix] Fix llama4 spec decoding",
      "url": "https://github.com/vllm-project/vllm/pull/22691",
      "number": 22691,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Harmless] Fix hardcoded float16 dtype for model_is_embedding",
      "url": "https://github.com/vllm-project/vllm/pull/7566",
      "number": 7566,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Temp",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6578",
      "number": 6578,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Fix triton compile error in `kernel_unified_attention_2/3d` caused by attention sinks",
      "url": "https://github.com/vllm-project/vllm/pull/22368",
      "number": 22368,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "server : fix assistant prefilling when content is an array",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14360",
      "number": 14360,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "mtmd-helper : bug fix to token batching in mtmd",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13650",
      "number": 13650,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "server : add condition for grammar",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11863",
      "number": 11863,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Example] make lmcache v0 work.",
      "url": "https://github.com/vllm-project/vllm/pull/18051",
      "number": 18051,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: use arch list for compatibility check",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11775",
      "number": 11775,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix a log error in chunked prefill",
      "url": "https://github.com/vllm-project/vllm/pull/6694",
      "number": 6694,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Minor][SLM][AutoLLM] Fix Quantization Parameter Naming",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1179",
      "number": 1179,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix autotuning so that it records Floating Point Operations per second, not microsecond",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2711",
      "number": 2711,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[iOS] Remove unrecognized linker flag `-noall_load`",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/946",
      "number": 946,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1][Minor] Do not print attn backend twice",
      "url": "https://github.com/vllm-project/vllm/pull/13985",
      "number": 13985,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "feat: Add tool_call support with json_schema for LLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3266",
      "number": 3266,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : add early return for empty range",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8327",
      "number": 8327,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] add error message in non linux platform",
      "url": "https://github.com/vllm-project/vllm/pull/3438",
      "number": 3438,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Add an option to cache torch dynamo artifacts on disk to reduce warm start time.",
      "url": "https://github.com/vllm-project/vllm/pull/22658",
      "number": 22658,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml: fix div-by-zero",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9003",
      "number": 9003,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix multiple zero 3 tracing errors",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1901",
      "number": 1901,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Minor] Make ignore_eos effective ",
      "url": "https://github.com/vllm-project/vllm/pull/4468",
      "number": 4468,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Implement approximate GELU kernels",
      "url": "https://github.com/vllm-project/vllm/pull/828",
      "number": 828,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "fix `error C2078: too many initializers` with uint32x4_t for MSVC ARM64",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5391",
      "number": 5391,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix grafana's model_name list showing other values",
      "url": "https://github.com/vllm-project/vllm/pull/20677",
      "number": 20677,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][ROCm] running new process using spawn method for rocm in tests.",
      "url": "https://github.com/vllm-project/vllm/pull/14810",
      "number": 14810,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix .gitignore for windows",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2729",
      "number": 2729,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : fix buffer checks for mamba and rwk",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10111",
      "number": 10111,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[TRTLLM-6656][chore] Validate FP8 support for Gemma3",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6678",
      "number": 6678,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "[SYCL][OPT] Fix reorder optimization for Q4_0",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13003",
      "number": 13003,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Use cmake 3.26.1 instead of 3.26 to avoid build failure",
      "url": "https://github.com/vllm-project/vllm/pull/19019",
      "number": 19019,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] set VLLM_WORKER_MULTIPROC_METHOD=spawn for vllm.entrypoionts.openai.api_server",
      "url": "https://github.com/vllm-project/vllm/pull/15700",
      "number": 15700,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1] Fix json_object support with xgrammar",
      "url": "https://github.com/vllm-project/vllm/pull/15488",
      "number": 15488,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "gguf-py : fix SpecialVocab parsing when post_processor is null",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14330",
      "number": 14330,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[fix] https://nvbugs/5333654 Verify the test on 0.21",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5829",
      "number": 5829,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Fix missing arguments in Galactica's from_pb",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1022",
      "number": 1022,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fixed the Windows build.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5596",
      "number": 5596,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: Skip rope scaling for local layers in Gemma3 VLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5857",
      "number": 5857,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix meta load tensor imcompatible issue",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7073",
      "number": 7073,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[NVIDIA] Auto detect modelopt quant and fix DSR1-FP4 weight loading",
      "url": "https://github.com/vllm-project/vllm/pull/22073",
      "number": 22073,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Use `${CMAKE_RUNTIME_OUTPUT_DIRECTORY}` instead of `bin` for copying the metal file",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4217",
      "number": 4217,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fixed rslora scaling in lora_manager",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1669",
      "number": 1669,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix deep_gemm_utils",
      "url": "https://github.com/vllm-project/vllm/pull/21509",
      "number": 21509,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix a typo of global variable in comm.py",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3852",
      "number": 3852,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1] Allow best_of=1 in sampling params",
      "url": "https://github.com/vllm-project/vllm/pull/14299",
      "number": 14299,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Fix seeded random sampling with encoder-decoder models",
      "url": "https://github.com/vllm-project/vllm/pull/8870",
      "number": 8870,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix runtime bug in developer install mode",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2688",
      "number": 2688,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI/Build] Fix registry tests",
      "url": "https://github.com/vllm-project/vllm/pull/21934",
      "number": 21934,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "fix[Docs]: link anchor is incorrect #20309",
      "url": "https://github.com/vllm-project/vllm/pull/20315",
      "number": 20315,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Remove undeed header file.",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6158",
      "number": 6158,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[Infa] - waive failed cases and fix a typo",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6384",
      "number": 6384,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[V1][BugFix] Fix for mixed top_k batch",
      "url": "https://github.com/vllm-project/vllm/pull/14301",
      "number": 14301,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix fused_qkv print model ValueError",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7109",
      "number": 7109,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix FlexibleArgumentParser replaces _ with - for actual args",
      "url": "https://github.com/vllm-project/vllm/pull/5795",
      "number": 5795,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Fix] Add phi lm head name to is_final_fc, add q4f16_ft to CI",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1849",
      "number": 1849,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix Linux /sys cpu path to guess number of cores",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7064",
      "number": 7064,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix FP8 Marlin MoE and enable for compressed-tensors models",
      "url": "https://github.com/vllm-project/vllm/pull/18026",
      "number": 18026,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Modify inference engine",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1520",
      "number": 1520,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Log user config exactly",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2201",
      "number": 2201,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix autoTP checkpoint load for OPT models",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3749",
      "number": 3749,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fixes for Phi3v and Ultravox Multimodal EmbeddingInputs Support",
      "url": "https://github.com/vllm-project/vllm/pull/8979",
      "number": 8979,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[iOS][Android] Add validation of library file for iOS and Android build",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1993",
      "number": 1993,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "ZeRO3 handling frozen weights",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2653",
      "number": 2653,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix embedding assignment for InternVL-based models",
      "url": "https://github.com/vllm-project/vllm/pull/15086",
      "number": 15086,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "[Zero2] Reduce the unnecessary all-reduce when tensor size is 0.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5868",
      "number": 5868,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "bug fix: handle saving/loading null layers in recurrent memory",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14675",
      "number": 14675,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Fix][Bitmask] Mask dummy padded tokens for grammar",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2651",
      "number": 2651,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml : fix missing reference to std::filesystem #10978",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11320",
      "number": 11320,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[bugfix] Fix profiling for RayDistributedExecutor",
      "url": "https://github.com/vllm-project/vllm/pull/13945",
      "number": 13945,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Quant] Move to `packed_modules_mapping` from class var to instance var",
      "url": "https://github.com/vllm-project/vllm/pull/13304",
      "number": 13304,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "launcher_helper: enable fds passing",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5042",
      "number": 5042,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Remove assumption that padding only occurs on last rank",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6974",
      "number": 6974,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Update TensorRT-LLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1274",
      "number": 1274,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][V1][Spec Dec] Add generator to request even when no seed is provided.",
      "url": "https://github.com/vllm-project/vllm/pull/17509",
      "number": 17509,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] V1 Memory Profiling: V0 Sampler Integration without Rejection Sampler",
      "url": "https://github.com/vllm-project/vllm/pull/13594",
      "number": 13594,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI/Build] fix cpu_extension for apple silicon",
      "url": "https://github.com/vllm-project/vllm/pull/21195",
      "number": 21195,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "lasx_ext16_32: Initialize the vector",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11726",
      "number": 11726,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[ds-inference] fix progress bar",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2286",
      "number": 2286,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Init distributed torch only if needed",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/108",
      "number": 108,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "save-load-state : fix example + add ci test",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3655",
      "number": 3655,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fixed some MobileVLM's inference bugs. Added more tests on different devices.",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6364",
      "number": 6364,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI] Pass local python version explicitly to pre-commit mypy.sh",
      "url": "https://github.com/vllm-project/vllm/pull/12224",
      "number": 12224,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[zero_to_fp32] fix shared param recovery",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3407",
      "number": 3407,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[Infra][TRTLLM-5633] - Fix merge waive list",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6504",
      "number": 6504,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: [nvbugs/5066257] serialization improvments",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3869",
      "number": 3869,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: only set _mpi_session if world_size is > 1",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5253",
      "number": 5253,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[P/D]Provide bucket algorithm rate limiter  for proxy_server",
      "url": "https://github.com/vllm-project/vllm/pull/22643",
      "number": 22643,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Update serial_utils.py",
      "url": "https://github.com/vllm-project/vllm/pull/20379",
      "number": 20379,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix llama meta tensor loading in AutoTP and kernel injected inference",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3608",
      "number": 3608,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama: enable maintaining key ordering in rules for grammar",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10789",
      "number": 10789,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Fix prefix caching V0 MLA",
      "url": "https://github.com/vllm-project/vllm/pull/14255",
      "number": 14255,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fix phi 3 conversion",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8262",
      "number": 8262,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] 1D query fix for MoE models",
      "url": "https://github.com/vllm-project/vllm/pull/3597",
      "number": 3597,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix nightly CI tests",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2493",
      "number": 2493,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "[fp32] fix default dtype",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1023",
      "number": 1023,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix typical acceptance sampler with correct recovered token ids",
      "url": "https://github.com/vllm-project/vllm/pull/8562",
      "number": 8562,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Explictly set device when reusing dist env",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6696",
      "number": 6696,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix disagg config params",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4646",
      "number": 4646,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix permission for local user issues in NGC docker container.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5373",
      "number": 5373,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : better replace_all",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8852",
      "number": 8852,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "convert : fix nomic-bert-moe mask token",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13757",
      "number": 13757,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix pydantic `ValidationError` bug",
      "url": "https://github.com/vllm-project/vllm/pull/18872",
      "number": 18872,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Frontend] add chunking audio for > 30s audio",
      "url": "https://github.com/vllm-project/vllm/pull/19597",
      "number": 19597,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Build/CI] Fix env var typo",
      "url": "https://github.com/vllm-project/vllm/pull/15305",
      "number": 15305,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix InternVL2 vision embeddings process with pipeline parallel",
      "url": "https://github.com/vllm-project/vllm/pull/8299",
      "number": 8299,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[bugfix] torch profiler bug for single gpu with GPUExecutor",
      "url": "https://github.com/vllm-project/vllm/pull/8354",
      "number": 8354,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: update test_user_buffers_mm_add_prologue atol",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3711",
      "number": 3711,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] GPTBigCodeForCausalLM: Remove lm_head from supported_lora_modules.",
      "url": "https://github.com/vllm-project/vllm/pull/6326",
      "number": 6326,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "test: Unwaive Llama 3.1 with torch compile test",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3475",
      "number": 3475,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Update nv-accelerate to latest torch",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5040",
      "number": 5040,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "iq3_xxs: guards for the no-imatrix situation",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5334",
      "number": 5334,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix nv-accelerate and nv-torch-latest-v100.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5035",
      "number": 5035,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix the failing gte embedding test",
      "url": "https://github.com/vllm-project/vllm/pull/18720",
      "number": 18720,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "ci : add fetch-depth to xcframework upload",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12195",
      "number": 12195,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] Stop silent failures on compressed-tensors parsing",
      "url": "https://github.com/vllm-project/vllm/pull/9381",
      "number": 9381,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[misc] Revert # 12833",
      "url": "https://github.com/vllm-project/vllm/pull/12857",
      "number": 12857,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Enforce nccl/rccl alignment of start location of each shard",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1564",
      "number": 1564,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[Fix][Chore][Qwen3] fix bug of using fp4 on sm120",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6065",
      "number": 6065,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: mul_mat_q=true as default for llama_context_params",
      "url": "https://github.com/ggml-org/llama.cpp/pull/2912",
      "number": 2912,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : fix attention layer count sanity check",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6550",
      "number": 6550,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ci : add swift build via xcodebuild",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3482",
      "number": 3482,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Update TensorRT-LLM",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2363",
      "number": 2363,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[https://nvbugspro.nvidia.com/bug/5323820] Fix chunking equation for disabled case.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4964",
      "number": 4964,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Build] Dump debug files only when debug-dump flag set",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/422",
      "number": 422,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "gguf-py: Support identity operation in TensorNameMap",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3095",
      "number": 3095,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Update TensorRT-LLM v0.14.0",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2401",
      "number": 2401,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix weight loading issue by rename variable.",
      "url": "https://github.com/vllm-project/vllm/pull/8293",
      "number": 8293,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "CUDA: fix FTZ in FA for Gemma 3",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13991",
      "number": 13991,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "add --mmap in llama-bench",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5257",
      "number": 5257,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1][TPU] Apply the ragged paged attention kernel fix and remove the padding.",
      "url": "https://github.com/vllm-project/vllm/pull/14846",
      "number": 14846,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Only build CUTLASS MoE kernels on Hopper",
      "url": "https://github.com/vllm-project/vllm/pull/19648",
      "number": 19648,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : sanitize invalid tokens",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9357",
      "number": 9357,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix requires_grad of input must be true for activation checkpoint layer in pipeline train.",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4128",
      "number": 4128,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix unit test typo in tests/unit/ops/transformer/inference",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3697",
      "number": 3697,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Engine side fix for loading llama checkpoint fine-tuned with zero3",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3981",
      "number": 3981,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix][Frontend] Fix `LLM.chat()` tokenization",
      "url": "https://github.com/vllm-project/vllm/pull/16081",
      "number": 16081,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Bugfix] Fix Disco break ",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1022",
      "number": 1022,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][Nixl] Fix Preemption Bug",
      "url": "https://github.com/vllm-project/vllm/pull/18631",
      "number": 18631,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Loosen requirement on packaging dependency",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1758",
      "number": 1758,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] EAGLE output norm bug",
      "url": "https://github.com/vllm-project/vllm/pull/14464",
      "number": 14464,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: Refactor the first token response in PD (#4692)",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4941",
      "number": 4941,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Gpt-j-6B patch kv_scale to k_scale path ",
      "url": "https://github.com/vllm-project/vllm/pull/10063",
      "number": 10063,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "[SYCL] Fix DMMV dequantization",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9279",
      "number": 9279,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix Precision Mismatch in MoE Router of DeepSeek V2/V3 Models and Fused Kernels (BF16 -> FP32)",
      "url": "https://github.com/vllm-project/vllm/pull/14027",
      "number": 14027,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix][TPU] Fix megacore setting for v5e-litepod",
      "url": "https://github.com/vllm-project/vllm/pull/6397",
      "number": 6397,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "fix: refactor adapter weight loading and mapping",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2193",
      "number": 2193,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llama : fix loading models with shared tok_embd and output",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5651",
      "number": 5651,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[VLM] Disallow overflowing `max_model_len` for multimodal models",
      "url": "https://github.com/vllm-project/vllm/pull/7998",
      "number": 7998,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Fix amd build fail caused by #21803",
      "url": "https://github.com/vllm-project/vllm/pull/21405",
      "number": 21405,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Update deepspeed_light.py",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/186",
      "number": 186,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[MODEL] New model support for naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B",
      "url": "https://github.com/vllm-project/vllm/pull/20931",
      "number": 20931,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ci : fix line breaks on windows builds",
      "url": "https://github.com/ggml-org/llama.cpp/pull/11409",
      "number": 11409,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bug] Fix Attention when ignored in by quant_method",
      "url": "https://github.com/vllm-project/vllm/pull/14313",
      "number": 14313,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[moe][quant] add weight name case for offset",
      "url": "https://github.com/vllm-project/vllm/pull/15515",
      "number": 15515,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix profiling dummy data for Pixtral",
      "url": "https://github.com/vllm-project/vllm/pull/18677",
      "number": 18677,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "GPTNeoX: Use static rotary embedding",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1498",
      "number": 1498,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Refactor] Create a function util and cache the results for `has_deepgemm`, `has_deepep`, `has_pplx`",
      "url": "https://github.com/vllm-project/vllm/pull/20187",
      "number": 20187,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[fix] Fix Triton build (#6076)",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6111",
      "number": 6111,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "issue",
      "title": "[Smallfix] Fix wizard math template",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1463",
      "number": 1463,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "bump to 0.3.6 and fix manifest to include reqs",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/561",
      "number": 561,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "threading: support for GGML_SCHED_PRIO_LOW, update thread info on Windows to avoid throttling",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12995",
      "number": 12995,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "export-lora : throw error if lora is quantized",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9002",
      "number": 9002,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "common : fix duplicated file name with hf_repo and hf_file",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10550",
      "number": 10550,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Update build.yml for Windows Vulkan builder to use Vulkan 1.4.304 SDK…",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12301",
      "number": 12301,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Core] Allow full cudagraph with separate attention routines and orthogonal to compilation, add support for FA2 and FlashInfer",
      "url": "https://github.com/vllm-project/vllm/pull/20059",
      "number": 20059,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "GPTQ Env vars: catch correct type of error",
      "url": "https://github.com/huggingface/text-generation-inference/pull/596",
      "number": 596,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix error caused by all_reduce call in domino",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6880",
      "number": 6880,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[FIX]avoid initialize process group when using a single GPU",
      "url": "https://github.com/vllm-project/vllm/pull/2496",
      "number": 2496,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Change methods to be static",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1038",
      "number": 1038,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] DeepSeek Accuracy",
      "url": "https://github.com/vllm-project/vllm/pull/14476",
      "number": 14476,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Fixing llama-android.cpp for error - \"common/common.h not found\"",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8145",
      "number": 8145,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[None][feat] Add support of scheduling attention dp request",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6246",
      "number": 6246,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[bugfix] promote state in bf16_optimizer",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5767",
      "number": 5767,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Inference ops unit test failures/fixes",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/6879",
      "number": 6879,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CUDA: fixed mmvq kernel for bs 2,3,4 and -sm row",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5386",
      "number": 5386,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix][v1] xgrammar structured output supports Enum.",
      "url": "https://github.com/vllm-project/vllm/pull/15594",
      "number": 15594,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Fix for breaking changes in xformers 0.0.21",
      "url": "https://github.com/vllm-project/vllm/pull/834",
      "number": 834,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "middlware to track server load",
      "url": "https://github.com/vllm-project/vllm/pull/18613",
      "number": 18613,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Fix] Improve prefill policy for prefix cache reuse",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2859",
      "number": 2859,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Fix][Tokenizer] Fix failure in decoding tokens for ByteLevel BPE",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/2649",
      "number": 2649,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Add n_key_dim and n_value_dim",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4657",
      "number": 4657,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix HybridEngine infer bug",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3569",
      "number": 3569,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix sparse-attn dependency install with torch-cpu",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/631",
      "number": 631,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[bugfix] small fix logic issue",
      "url": "https://github.com/vllm-project/vllm/pull/18999",
      "number": 18999,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[ROCm] Dockerfile fix for flash-attention build",
      "url": "https://github.com/vllm-project/vllm/pull/2885",
      "number": 2885,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "Avoiding timeout for bloom tests.",
      "url": "https://github.com/huggingface/text-generation-inference/pull/2693",
      "number": 2693,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ggml-alloc : add 10% margin to the buffer sizes",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5149",
      "number": 5149,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Allow \"None\" or \"\" to be passed to CLI for string args that default to None",
      "url": "https://github.com/vllm-project/vllm/pull/4586",
      "number": 4586,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "server : add bad input handling in embeddings",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10866",
      "number": 10866,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Frontend]Reduce vLLM's import time",
      "url": "https://github.com/vllm-project/vllm/pull/15128",
      "number": 15128,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "fixed off by one error when context shifting in main.cpp example",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6921",
      "number": 6921,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[BugFix] fix wrong output when using lora and num_scheduler_steps=8",
      "url": "https://github.com/vllm-project/vllm/pull/11161",
      "number": 11161,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] set correct lora mapping when compute prompt logprobs",
      "url": "https://github.com/vllm-project/vllm/pull/16670",
      "number": 16670,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[benchmark] Sending request strictly follows the random intervals",
      "url": "https://github.com/vllm-project/vllm/pull/21108",
      "number": 21108,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Add LogProbs for Chat Completions in OpenAI",
      "url": "https://github.com/vllm-project/vllm/pull/2918",
      "number": 2918,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Log the reason for falling back to FlexAttention",
      "url": "https://github.com/vllm-project/vllm/pull/20699",
      "number": 20699,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix GLM4 incoherence with fp16 accumulators",
      "url": "https://github.com/ggml-org/llama.cpp/pull/13639",
      "number": 13639,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix heap corruption from wmode out-of-bound writes on windows",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6272",
      "number": 6272,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Core] Ignore infeasible swap requests.",
      "url": "https://github.com/vllm-project/vllm/pull/4557",
      "number": 4557,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "DRY: Fixes clone functionality",
      "url": "https://github.com/ggml-org/llama.cpp/pull/10192",
      "number": 10192,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Support full cuda graph with sliding window attention",
      "url": "https://github.com/vllm-project/vllm/pull/22168",
      "number": 22168,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Fix qwen2.5-vl overflow issue",
      "url": "https://github.com/vllm-project/vllm/pull/13968",
      "number": 13968,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "opencl: ref count `ggml_backend_opencl_context` and refactor profiling",
      "url": "https://github.com/ggml-org/llama.cpp/pull/14254",
      "number": 14254,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Misc] Log time consumption on weight downloading",
      "url": "https://github.com/vllm-project/vllm/pull/12926",
      "number": 12926,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "server : fix crash when using verbose output with input tokens that are not in printable range (#12178)",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12338",
      "number": 12338,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[XPU] dispatch xpu/cuda specific calls in the model runner",
      "url": "https://github.com/vllm-project/vllm/pull/20698",
      "number": 20698,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "llava: fix clip-model-is-vision flag in README.md",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5509",
      "number": 5509,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "server : Add verbose output to OAI compatible chat endpoint.",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12246",
      "number": 12246,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Update deprecated HuggingFace function",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5144",
      "number": 5144,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[V1][Spec Decode][Ngram] 1.35x gain -> 1.95x gain on InstructCoder with prompt fix",
      "url": "https://github.com/vllm-project/vllm/pull/18971",
      "number": 18971,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "metal : use F32 prec in FA kernels",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12688",
      "number": 12688,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "[TRTLLM-6650][fix] Enhance CUDA graph + Beam search to correctly handle padding",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6665",
      "number": 6665,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Use already defined default seed to avoid compiler error C2397",
      "url": "https://github.com/ggml-org/llama.cpp/pull/5855",
      "number": 5855,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Build/CI] Fix CUDA 11.8 build",
      "url": "https://github.com/vllm-project/vllm/pull/17679",
      "number": 17679,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix test Pytorch model engine",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5416",
      "number": 5416,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Worker] Fix input_metadata.selected_token_indices in worker",
      "url": "https://github.com/vllm-project/vllm/pull/1546",
      "number": 1546,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Add the missing `tool_prompt` parameter to Python client",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1825",
      "number": 1825,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "mlc-ai/mlc-llm",
      "type": "pr",
      "title": "[Serve] add allocator in Storage as the upstream change",
      "url": "https://github.com/mlc-ai/mlc-llm/pull/1997",
      "number": 1997,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "[zero++] Synchronize at the end of secondary partitioning and simplify the logic",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/5216",
      "number": 5216,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[ci][bugfix] fix kernel tests",
      "url": "https://github.com/vllm-project/vllm/pull/10431",
      "number": 10431,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Validate `stop_token_ids` contents",
      "url": "https://github.com/vllm-project/vllm/pull/17268",
      "number": 17268,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: fix conflicting test names",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3315",
      "number": 3315,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Revert \"Prevent hangs in CI during parallel run compilation\"",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3817",
      "number": 3817,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[FP8][ROCm][Attention] Enable FP8 KV cache on ROCm for V1",
      "url": "https://github.com/vllm-project/vllm/pull/17870",
      "number": 17870,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "issue",
      "title": "support local model config file",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1058",
      "number": 1058,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "tests: fix commit info",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5016",
      "number": 5016,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "fix: add warmup flag into py_executor to prevent enable profiler during wa…",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3852",
      "number": 3852,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix: change ==NONE to is under deepspeed/",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/3923",
      "number": 3923,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: Fix p-tuning test bug",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3326",
      "number": 3326,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "ci : fix xcodebuild destinations",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3491",
      "number": 3491,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] fix detokenizer shallow copy",
      "url": "https://github.com/vllm-project/vllm/pull/5919",
      "number": 5919,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "test: Fix breaking Phi3 multimodal tests",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3544",
      "number": 3544,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "Add EXAONE 4.0 reasoning parser",
      "url": "https://github.com/vllm-project/vllm/pull/22617",
      "number": 22617,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "python logging refactoring and flake8 suppression refactoring",
      "url": "https://github.com/ggml-org/llama.cpp/pull/7081",
      "number": 7081,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "TensorRT-LLM v0.10 update",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1734",
      "number": 1734,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Fix printing momentum for non-deepspeed optimizer",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/464",
      "number": 464,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "Fix llama2 convert_checkpoint.py --load_model_on_cpu",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1460",
      "number": 1460,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[BugFix] Make PD work with Ray",
      "url": "https://github.com/vllm-project/vllm/pull/21072",
      "number": 21072,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Performance] [Speculative decoding]: Replace scoring spec tokens via batched 1-step generation by n-step prefill",
      "url": "https://github.com/vllm-project/vllm/pull/7255",
      "number": 7255,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "fix(worker): adjust memory requirement calculation for GPU worker",
      "url": "https://github.com/vllm-project/vllm/pull/22237",
      "number": 22237,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Make fp32 default communication data type",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2970",
      "number": 2970,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[TRTLLM-4279] feat: Multistream initial support for torch compile flow",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5847",
      "number": 5847,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[nvbugs/5331013] fix AutoDeploy for PyTorch 25.05 dependency upgrade",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5106",
      "number": 5106,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] Add error handling when server cannot respond any valid tokens",
      "url": "https://github.com/vllm-project/vllm/pull/5895",
      "number": 5895,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: [nvbugs/5351130] Adjust DSV3-Lite tests free_gpu_memory_fraction to 0.75 to prevent OOM on CI.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5896",
      "number": 5896,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "fix: [5328141] increase tolerance for test_fp8_block_scale_gemm",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5849",
      "number": 5849,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix unnecessarily retained pointer to rules parameter in llama_grammar_init",
      "url": "https://github.com/ggml-org/llama.cpp/pull/6003",
      "number": 6003,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Installation] Allow lower versions of FastAPI to maintain Ray 2.9 compatibility",
      "url": "https://github.com/vllm-project/vllm/pull/8764",
      "number": 8764,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "fix unit test failure when merging PP ckpt files",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/1359",
      "number": 1359,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "[TRTLLM-6420][feat] add support for Eclairv2 model - cherry-pick changes and minor fix",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6493",
      "number": 6493,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Fix] Fix gemma CI test failing on main",
      "url": "https://github.com/vllm-project/vllm/pull/20124",
      "number": 20124,
      "classified_symptom": "API/argument parsing Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "Fix trying to strip newline from empty prompt and cfg prompt file content",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3534",
      "number": 3534,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Bugfix] multi-step + flashinfer: ensure cuda graph compatible ",
      "url": "https://github.com/vllm-project/vllm/pull/8427",
      "number": 8427,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "llama : bump max layers from 256 to 512",
      "url": "https://github.com/ggml-org/llama.cpp/pull/8530",
      "number": 8530,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "metal : handle zero-sized allocs",
      "url": "https://github.com/ggml-org/llama.cpp/pull/9466",
      "number": 9466,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "issue",
      "title": "Fix Int4 AWQ quantization model saving",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/929",
      "number": 929,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Bugfix] Fix dual-stack binding when --host is empty",
      "url": "https://github.com/vllm-project/vllm/pull/22854",
      "number": 22854,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "NVIDIA/TensorRT-LLM",
      "type": "pr",
      "title": "test: [CI] Add failed cases into waives.txt",
      "url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6289",
      "number": 6289,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "CLBlast: Add outer loops over src0 for broadcasting in mulmat",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3669",
      "number": 3669,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "issue",
      "title": "adding in include needed",
      "url": "https://github.com/ggml-org/llama.cpp/pull/4002",
      "number": 4002,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[Misc] Remove unnecessary fallback to prefill-decode attention",
      "url": "https://github.com/vllm-project/vllm/pull/19138",
      "number": 19138,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "huggingface/text-generation-inference",
      "type": "pr",
      "title": "Fix incorrect use of bias in awq",
      "url": "https://github.com/huggingface/text-generation-inference/pull/1381",
      "number": 1381,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "Release the requested thread pool resource",
      "url": "https://github.com/ggml-org/llama.cpp/pull/3292",
      "number": 3292,
      "classified_symptom": "Performance/Memory Issue",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "pr",
      "title": "Add `pyproject.toml` with legacy build backend to keep most logic in `setup.py`",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/7033",
      "number": 7033,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "Bump helm/chart-testing-action from 2.6.1 to 2.7.0",
      "url": "https://github.com/vllm-project/vllm/pull/12463",
      "number": 12463,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "add last slot for the invalid_token in greedy rejection sampler, specdec",
      "url": "https://github.com/vllm-project/vllm/pull/14519",
      "number": 14519,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Test] fix swap test for multi gpu",
      "url": "https://github.com/vllm-project/vllm/pull/4689",
      "number": 4689,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "issue",
      "title": "[CI/Build] Reorganize models tests",
      "url": "https://github.com/vllm-project/vllm/pull/17459",
      "number": 17459,
      "classified_symptom": "other",
      "confidence": "low"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Fix a bug in DeepSpeedMLP",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/4389",
      "number": 4389,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Kernel] Add punica dimension for Baichuan-13B",
      "url": "https://github.com/vllm-project/vllm/pull/4053",
      "number": 4053,
      "classified_symptom": "Incorrect/Inaccuracy Output",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "FIX: import TypedDict from typing_extensions for python<3.12",
      "url": "https://github.com/vllm-project/vllm/pull/17967",
      "number": 17967,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[CI/Build] Fix LoRA test",
      "url": "https://github.com/vllm-project/vllm/pull/19350",
      "number": 19350,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[v1] Add `__repr__` to KVCacheBlock to avoid recursive print",
      "url": "https://github.com/vllm-project/vllm/pull/14081",
      "number": 14081,
      "classified_symptom": "Runtime Crash or Error",
      "confidence": "high"
    },
    {
      "repository": "microsoft/DeepSpeed",
      "type": "issue",
      "title": "Graceful exit on failures for multi-node runs",
      "url": "https://github.com/deepspeedai/DeepSpeed/pull/2000",
      "number": 2000,
      "classified_symptom": "Distributed Parallelism/Sharding Bug",
      "confidence": "high"
    },
    {
      "repository": "vllm-project/vllm",
      "type": "pr",
      "title": "[Gemma2] add bitsandbytes support for Gemma2",
      "url": "https://github.com/vllm-project/vllm/pull/8338",
      "number": 8338,
      "classified_symptom": "Model Loading  Error",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "ci: [SYCL] Use main GPU and enable sysman",
      "url": "https://github.com/ggml-org/llama.cpp/pull/12547",
      "number": 12547,
      "classified_symptom": "Hardware/Backend Compatibility Issue",
      "confidence": "high"
    },
    {
      "repository": "ggerganov/llama.cpp",
      "type": "pr",
      "title": "[CI] CLBlast: Fix directory name",
      "url": "https://github.com/ggml-org/llama.cpp/pull/1606",
      "number": 1606,
      "classified_symptom": "Build/Compilation Error",
      "confidence": "high"
    }
  ]
}